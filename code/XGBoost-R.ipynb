{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2aec4297",
      "metadata": {},
      "source": [
        "1 — Imports, configuración y rutas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "565a4b55",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exp: XGB_REDUCED_SMOTENC\n",
            "DATA_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/preproc_datasets/full\n",
            "ARTIF_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_REDUCED_SMOTENC\n"
          ]
        }
      ],
      "source": [
        "import json, os, warnings, time, re, glob\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score, precision_recall_curve, roc_auc_score, roc_curve,\n",
        "    f1_score, recall_score, balanced_accuracy_score, confusion_matrix, precision_score\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Balanceo\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTENC, SMOTE\n",
        "    _HAS_IMBLEARN = True\n",
        "except Exception:\n",
        "    _HAS_IMBLEARN = False\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# === Toggles de experimento ===\n",
        "USE_REDUCED = True                \n",
        "SELECTION_MODE = \"L1\"             \n",
        "USE_BALANCED_TRAIN = True         \n",
        "BALANCE_IN_CV = True              \n",
        "RANDOM_STATE = 42\n",
        "DO_TUNE = True\n",
        "DO_CV_BASELINE = True\n",
        "DO_CV_TUNED = True\n",
        "CV_FOLDS = 5\n",
        "\n",
        "# Para MI\n",
        "MI_TOPK = 30\n",
        "\n",
        "# Hiperparámetros L1 estable\n",
        "L1_C_GRID  = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10]\n",
        "L1_KFOLDS  = 5\n",
        "L1_P_KEEP  = 0.8                \n",
        "L1_GROUP_COHERENCE = False         \n",
        "\n",
        "# Calcular máscara L1\n",
        "FORCE_REFIT_L1 = True\n",
        "\n",
        "# === Nombres y rutas ===\n",
        "ROOT = Path.cwd().parent\n",
        "EXP_NAME = f\"XGB_{'REDUCED' if (USE_REDUCED and SELECTION_MODE!='NONE') else 'FULL'}_{'SMOTENC' if USE_BALANCED_TRAIN else 'IMB'}\"\n",
        "ARTIF_DIR = ROOT / \"artifacts\" / EXP_NAME\n",
        "OUT_RESULTS = ARTIF_DIR / \"results\"\n",
        "OUT_FIGS    = ARTIF_DIR / \"figs\"\n",
        "OUT_PREDS   = ARTIF_DIR / \"preds\"\n",
        "OUT_PARAMS  = ARTIF_DIR / \"best_params\"\n",
        "for p in [OUT_RESULTS, OUT_FIGS, OUT_PREDS, OUT_PARAMS]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Carpeta para artefactos de selección de features\n",
        "SEL_DIR = OUT_PARAMS / \"feature_selection\"\n",
        "SEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Dataset preprocesado\n",
        "DATA_DIR = ROOT / \"preproc_datasets\" / \"full\"\n",
        "\n",
        "print(\"Exp:\", EXP_NAME)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"ARTIF_DIR:\", ARTIF_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cd89f52",
      "metadata": {},
      "source": [
        "2 — Carga de artefactos (X, y, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "38f0a43f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes: (6000, 15) (2000, 15) (2000, 15)\n",
            "y train/val/test: (6000,) (2000,) (2000,)\n",
            "n features: 15\n"
          ]
        }
      ],
      "source": [
        "def load_xy_full(dir_full: Path):\n",
        "    X_train = np.load(dir_full / \"X_train_full.npy\")\n",
        "    X_val   = np.load(dir_full / \"X_val_full.npy\")\n",
        "    X_test  = np.load(dir_full / \"X_test_full.npy\")\n",
        "\n",
        "    y_train = pd.read_parquet(dir_full / \"y_train.parquet\")[\"Exited\"].to_numpy()\n",
        "    y_val   = pd.read_parquet(dir_full / \"y_val.parquet\")[\"Exited\"].to_numpy()\n",
        "    y_test  = pd.read_parquet(dir_full / \"y_test.parquet\")[\"Exited\"].to_numpy()\n",
        "\n",
        "    feat = pd.read_parquet(dir_full / \"feature_names_full.parquet\")[\"feature\"].tolist()\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, feat\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_xy_full(DATA_DIR)\n",
        "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "print(\"y train/val/test:\", y_train.shape, y_val.shape, y_test.shape)\n",
        "print(\"n features:\", len(feature_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e693828",
      "metadata": {},
      "source": [
        "3 — Métricas, threshold y plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "650b59ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pr_auc(y_true, y_proba): \n",
        "    return float(average_precision_score(y_true, y_proba))\n",
        "\n",
        "def roc_auc(y_true, y_proba): \n",
        "    return float(roc_auc_score(y_true, y_proba))\n",
        "\n",
        "def find_best_threshold(y_true, y_proba, metric=\"f1\"):\n",
        "    thr_grid = np.linspace(0.0, 1.0, 1001)\n",
        "    best_thr, best_score = 0.5, -1.0\n",
        "    for thr in thr_grid:\n",
        "        y_pred = (y_proba >= thr).astype(int)\n",
        "        if metric == \"f1\":\n",
        "            score = f1_score(y_true, y_pred, zero_division=0)\n",
        "        elif metric == \"recall\":\n",
        "            score = recall_score(y_true, y_pred, zero_division=0)\n",
        "        else:\n",
        "            raise ValueError(\"metric no soportada\")\n",
        "        if score > best_score:\n",
        "            best_score, best_thr = score, thr\n",
        "    return float(best_thr), float(best_score)\n",
        "\n",
        "def compute_all_metrics(y_true, y_proba, thr):\n",
        "    y_pred = (y_proba >= thr).astype(int)\n",
        "    return {\n",
        "        \"pr_auc\": pr_auc(y_true, y_proba),\n",
        "        \"roc_auc\": roc_auc(y_true, y_proba),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"bal_acc\": balanced_accuracy_score(y_true, y_pred)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7943526",
      "metadata": {},
      "source": [
        "4 — Helpers MI Top-K y balanceo in-memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "95ecc2fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def assert_X_names_aligned(X, names, context=\"\"):\n",
        "    if X.shape[1] != len(names):\n",
        "        raise ValueError(\n",
        "            f\"[{context}] Desalineado: X.shape[1]={X.shape[1]} vs len(feature_names)={len(names)}. \"\n",
        "            f\"Asegura pasar los nombres que correspondan a las columnas actuales de X.\"\n",
        "        )\n",
        "\n",
        "class _BoosterAdapter:\n",
        "    def __init__(self, booster, params, best_iteration, feature_names=None):\n",
        "        self._booster = booster\n",
        "        self._params = dict(params)\n",
        "        self.best_iteration = best_iteration\n",
        "        self._feature_names = feature_names\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        d = xgb.DMatrix(X, feature_names=self._feature_names)\n",
        "        if self.best_iteration is not None:\n",
        "            pred = self._booster.predict(d, iteration_range=(0, int(self.best_iteration) + 1))\n",
        "        else:\n",
        "            pred = self._booster.predict(d)\n",
        "        return np.column_stack([1.0 - pred, pred])\n",
        "\n",
        "    def get_booster(self):\n",
        "        return self._booster\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return dict(self._params)\n",
        "\n",
        "def xgb_fit_with_es(\n",
        "    sk_model, X_tr, y_tr, X_va, y_va,\n",
        "    feature_names=None, rounds=200, verbose=False\n",
        "):\n",
        "    p = sk_model.get_params()\n",
        "    n_estimators = p.pop(\"n_estimators\", 1000)\n",
        "    n_estimators = 1000 if n_estimators is None else int(n_estimators)\n",
        "\n",
        "    seed = p.pop(\"random_state\", p.pop(\"seed\", 42))\n",
        "    nthread = p.pop(\"n_jobs\", None)\n",
        "    if nthread is not None:\n",
        "        p[\"nthread\"] = nthread\n",
        "\n",
        "    p.setdefault(\"seed\", seed)\n",
        "    p.setdefault(\"objective\", \"binary:logistic\")\n",
        "    p.setdefault(\"eval_metric\", \"aucpr\")\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr, feature_names=feature_names)\n",
        "    dvalid = xgb.DMatrix(X_va, label=y_va, feature_names=feature_names)\n",
        "\n",
        "    evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
        "    booster = xgb.train(\n",
        "        params=p,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=n_estimators,\n",
        "        evals=evals,\n",
        "        early_stopping_rounds=rounds,\n",
        "        verbose_eval=verbose\n",
        "    )\n",
        "\n",
        "    best_iter = getattr(booster, \"best_iteration\", None)\n",
        "    adapter = _BoosterAdapter(\n",
        "        booster=booster,\n",
        "        params={**sk_model.get_params(), \"best_iteration\": best_iter},\n",
        "        best_iteration=best_iter,\n",
        "        feature_names=feature_names\n",
        "    )\n",
        "    return adapter, best_iter\n",
        "\n",
        "\n",
        "def apply_keep_idx(X, keep_idx):\n",
        "    return X[:, np.array(keep_idx, dtype=int)]\n",
        "\n",
        "def _group_from_name(feat_name: str) -> str:\n",
        "    if feat_name.startswith(\"num__\"):\n",
        "        return feat_name\n",
        "    if \"_\" in feat_name:\n",
        "        return feat_name.rsplit(\"_\", 1)[0]\n",
        "    return feat_name\n",
        "\n",
        "def _groups_indices(feature_names):\n",
        "    groups = {}\n",
        "    for i, f in enumerate(feature_names):\n",
        "        g = _group_from_name(f)\n",
        "        groups.setdefault(g, []).append(i)\n",
        "    return groups\n",
        "\n",
        "def fit_l1_selector(\n",
        "    X, y, feature_names, C_grid=L1_C_GRID, kfolds=L1_KFOLDS,\n",
        "    p_keep=L1_P_KEEP, group_coherence=L1_GROUP_COHERENCE, seed=42\n",
        "):\n",
        "    assert_X_names_aligned(X, feature_names, context=\"fit_l1_selector(INPUT)\")\n",
        "\n",
        "    kf = StratifiedKFold(n_splits=kfolds, shuffle=True, random_state=seed)\n",
        "    n = X.shape[1]\n",
        "    select_counts = np.zeros(n, dtype=int)\n",
        "    rows = []\n",
        "\n",
        "    for f, (tr_idx, va_idx) in enumerate(kf.split(X, y), 1):\n",
        "        X_tr, X_va = X[tr_idx], X[va_idx]\n",
        "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
        "\n",
        "        best_c, best_ap, best_coef = None, -1.0, None\n",
        "        for C in C_grid:\n",
        "            mdl = LogisticRegression(\n",
        "                penalty=\"l1\", solver=\"liblinear\", class_weight=\"balanced\",\n",
        "                max_iter=5000, C=C, random_state=seed\n",
        "            )\n",
        "            mdl.fit(X_tr, y_tr)\n",
        "            proba = mdl.predict_proba(X_va)[:, 1]\n",
        "            ap = average_precision_score(y_va, proba)\n",
        "            if ap > best_ap:\n",
        "                best_ap, best_c, best_coef = ap, C, mdl.coef_.ravel()\n",
        "\n",
        "        mask = (np.abs(best_coef) > 1e-12)\n",
        "        select_counts += mask.astype(int)\n",
        "        rows.append({\"fold\": f, \"best_C\": best_c, \"ap_val\": best_ap, \"n_selected\": int(mask.sum())})\n",
        "\n",
        "    thr = int(np.ceil(kfolds * p_keep))\n",
        "    keep_idx = np.where(select_counts >= thr)[0].tolist()\n",
        "\n",
        "    if group_coherence and keep_idx:\n",
        "        groups = _groups_indices(feature_names)\n",
        "        keep_set = set(keep_idx)\n",
        "        for g, idxs in groups.items():\n",
        "            if keep_set.intersection(idxs):\n",
        "                keep_set.update(idxs)\n",
        "        keep_idx = sorted(list(keep_set))\n",
        "\n",
        "    if len(keep_idx) == 0:\n",
        "        N = max(1, min(10, max(1, n // 3)))\n",
        "        order = np.argsort(select_counts)[::-1]\n",
        "        prelim = order[:N].tolist()\n",
        "        if select_counts[prelim[0]] == 0:\n",
        "            try:\n",
        "                mi = mutual_info_classif(X, y, random_state=seed)\n",
        "                prelim = np.argsort(mi)[::-1][:N].tolist()\n",
        "            except Exception:\n",
        "                prelim = list(range(N))\n",
        "        keep_idx = sorted(prelim)\n",
        "        if group_coherence and keep_idx:\n",
        "            groups = _groups_indices(feature_names)\n",
        "            keep_set = set(keep_idx)\n",
        "            for g, idxs in groups.items():\n",
        "                if keep_set.intersection(idxs):\n",
        "                    keep_set.update(idxs)\n",
        "            keep_idx = sorted(list(keep_set))\n",
        "\n",
        "    report = pd.DataFrame({\n",
        "        \"feature\": feature_names,\n",
        "        \"selected_in_folds\": select_counts,\n",
        "        \"p_keep\": select_counts / kfolds\n",
        "    }).sort_values([\"p_keep\",\"feature\"], ascending=[False, True])\n",
        "\n",
        "    folds_log = pd.DataFrame(rows)\n",
        "    return keep_idx, report, folds_log\n",
        "\n",
        "def save_selection_artifacts(mode_tag: str, keep_idx: list, feature_names: list, report_df: pd.DataFrame, folds_df: pd.DataFrame):\n",
        "    np.save(SEL_DIR / f\"keep_idx_{mode_tag}.npy\", np.array(keep_idx, dtype=int))\n",
        "    pd.DataFrame({\"feature\": [feature_names[i] for i in keep_idx]}).to_csv(SEL_DIR / f\"kept_features_{mode_tag}.csv\", index=False)\n",
        "    report_df.to_csv(SEL_DIR / f\"{mode_tag}_report_features.csv\", index=False)\n",
        "    folds_df.to_csv(SEL_DIR / f\"{mode_tag}_cv_log.csv\", index=False)\n",
        "\n",
        "def load_keep_idx_if_exists(mode_tag: str):\n",
        "    path = SEL_DIR / f\"keep_idx_{mode_tag}.npy\"\n",
        "    return np.load(path) if path.exists() else None\n",
        "\n",
        "def infer_categorical_indices(feat_names):\n",
        "\n",
        "    return [i for i, n in enumerate(feat_names) if not str(n).startswith(\"num__\")]\n",
        "\n",
        "def maybe_smotenc(X, y, feat_names):\n",
        "\n",
        "    if not _HAS_IMBLEARN:\n",
        "        return X, y\n",
        "    cat_idx = infer_categorical_indices(feat_names)\n",
        "    try:\n",
        "        if len(cat_idx) > 0:\n",
        "            sampler = SMOTENC(categorical_features=cat_idx, random_state=RANDOM_STATE)\n",
        "        else:\n",
        "            sampler = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_res, y_res = sampler.fit_resample(X, y)\n",
        "        return X_res, y_res\n",
        "    except Exception:\n",
        "        # Fallback robusto\n",
        "        try:\n",
        "            sampler = SMOTE(random_state=RANDOM_STATE)\n",
        "            return sampler.fit_resample(X, y)\n",
        "        except Exception:\n",
        "            return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d86df8a",
      "metadata": {},
      "source": [
        "5 — Hiperparámetros persistentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "32cca868",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HP] Cargando mejores hiperparámetros previos: BEST_XGB_REDUCED_SMOTENC.json\n"
          ]
        }
      ],
      "source": [
        "VIEW_TAG = \"REDUCED\" if USE_REDUCED else \"FULL\"\n",
        "BAL_TAG  = \"SMOTENC\" if USE_BALANCED_TRAIN else \"IMB\"\n",
        "BEST_HP_FILE = OUT_PARAMS / f\"BEST_XGB_{VIEW_TAG}_{BAL_TAG}.json\"\n",
        "\n",
        "def get_xgb_defaults(seed=RANDOM_STATE):\n",
        "    mdl = XGBClassifier(\n",
        "        random_state=seed,\n",
        "        n_jobs=-1,\n",
        "        eval_metric=\"aucpr\",\n",
        "        tree_method=\"hist\",\n",
        "        verbosity=0,\n",
        "    )\n",
        "    params = mdl.get_params()\n",
        "    # normalizamos la clave de verbosity\n",
        "    params.pop(\"verbose\", None)\n",
        "    params.setdefault(\"verbosity\", 0)\n",
        "    return params\n",
        "\n",
        "def load_best_or_default():\n",
        "    if BEST_HP_FILE.exists():\n",
        "        try:\n",
        "            best = json.loads(BEST_HP_FILE.read_text())\n",
        "            print(\"[HP] Cargando mejores hiperparámetros previos:\", BEST_HP_FILE.name)\n",
        "            base = get_xgb_defaults()\n",
        "            base.update(best)\n",
        "            return base, True\n",
        "        except Exception as e:\n",
        "            print(\"[HP] Aviso: no se pudo leer BEST (uso defaults).\", e)\n",
        "    print(\"[HP] Usando hiperparámetros DEFAULT de XGB.\")\n",
        "    return get_xgb_defaults(), False\n",
        "\n",
        "seed_params, loaded_best_flag = load_best_or_default()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfed62da",
      "metadata": {},
      "source": [
        "6 — Entrenamiento BASELINE + umbral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "ea25cb27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SELECCIÓN L1] Kept=12 | Dropped=3\n",
            "[Kept]: ['num__CreditScore', 'num__Age', 'num__Tenure', 'num__Balance', 'num__EstimatedSalary', 'Geography_1', 'Gender_1', 'HasCrCard_1', 'IsActiveMember_1', 'NumOfProducts_1', 'NumOfProducts_2', 'NumOfProducts_3']\n",
            "[Dropped]: ['Geography_0', 'Geography_2', 'NumOfProducts_0']\n",
            "\n",
            "[SELECCIÓN L1] Top por p_keep:\n",
            "             feature  selected_in_folds  p_keep\n",
            "            Gender_1                  5     1.0\n",
            "         Geography_1                  5     1.0\n",
            "         HasCrCard_1                  5     1.0\n",
            "    IsActiveMember_1                  5     1.0\n",
            "     NumOfProducts_1                  5     1.0\n",
            "     NumOfProducts_2                  5     1.0\n",
            "     NumOfProducts_3                  5     1.0\n",
            "            num__Age                  5     1.0\n",
            "        num__Balance                  5     1.0\n",
            "    num__CreditScore                  5     1.0\n",
            "num__EstimatedSalary                  5     1.0\n",
            "         num__Tenure                  5     1.0\n",
            "         Geography_2                  3     0.6\n",
            "         Geography_0                  2     0.4\n",
            "     NumOfProducts_0                  0     0.0\n",
            "[BASELINE] best_iteration: 1182\n",
            "[BASELINE] Mejor umbral (val) por F1: 0.452 | F1(val)=0.6388\n",
            "[BASELINE] Métricas val: {'pr_auc': 0.6926, 'roc_auc': 0.8582, 'precision': 0.6224, 'f1': 0.6388, 'recall': 0.656, 'bal_acc': 0.7772}\n",
            "[DEBUG] X_train shape: (6000, 15)\n",
            "\n",
            "num__CreditScore: numérica (nunique=417)\n",
            "count    6000.000\n",
            "mean       -0.005\n",
            "std         0.723\n",
            "min        -1.654\n",
            "5%         -1.226\n",
            "25%        -0.511\n",
            "50%         0.000\n",
            "75%         0.489\n",
            "95%         1.203\n",
            "max         1.489\n",
            "\n",
            "num__Age: numérica (nunique=52)\n",
            "count    6000.000\n",
            "mean        0.161\n",
            "std         0.858\n",
            "min        -1.333\n",
            "5%         -1.000\n",
            "25%        -0.417\n",
            "50%         0.000\n",
            "75%         0.583\n",
            "95%         1.917\n",
            "max         2.917\n",
            "\n",
            "num__Tenure: numérica (nunique=11)\n",
            "count    6000.000\n",
            "mean        0.006\n",
            "std         0.582\n",
            "min        -1.000\n",
            "5%         -0.800\n",
            "25%        -0.400\n",
            "50%         0.000\n",
            "75%         0.600\n",
            "95%         0.800\n",
            "max         1.000\n",
            "\n",
            "num__Balance: numérica (nunique=3809)\n",
            "count    6000.000\n",
            "mean       -0.161\n",
            "std         0.484\n",
            "min        -0.762\n",
            "5%         -0.762\n",
            "25%        -0.762\n",
            "50%         0.000\n",
            "75%         0.238\n",
            "95%         0.506\n",
            "max         0.687\n",
            "\n",
            "num__EstimatedSalary: numérica (nunique=5882)\n",
            "count    6000.000\n",
            "mean        0.004\n",
            "std         0.589\n",
            "min        -0.999\n",
            "5%         -0.921\n",
            "25%        -0.498\n",
            "50%         0.000\n",
            "75%         0.502\n",
            "95%         0.934\n",
            "max         1.018\n",
            "\n",
            "Geography_0: binaria/low-card (nunique=2)\n",
            "Geography_0\n",
            "0.0    3020\n",
            "1.0    2980\n",
            "  -> tasa de Exited cuando Geography_0=1: 0.1634\n",
            "\n",
            "Geography_1: binaria/low-card (nunique=2)\n",
            "Geography_1\n",
            "0.0    4496\n",
            "1.0    1504\n",
            "  -> tasa de Exited cuando Geography_1=1: 0.3251\n",
            "\n",
            "Geography_2: binaria/low-card (nunique=2)\n",
            "Geography_2\n",
            "0.0    4484\n",
            "1.0    1516\n",
            "  -> tasa de Exited cuando Geography_2=1: 0.1629\n",
            "\n",
            "Gender_1: binaria/low-card (nunique=2)\n",
            "Gender_1\n",
            "0.0    2724\n",
            "1.0    3276\n",
            "  -> tasa de Exited cuando Gender_1=1: 0.1618\n",
            "\n",
            "HasCrCard_1: binaria/low-card (nunique=2)\n",
            "HasCrCard_1\n",
            "0.0    1742\n",
            "1.0    4258\n",
            "  -> tasa de Exited cuando HasCrCard_1=1: 0.1984\n",
            "\n",
            "IsActiveMember_1: binaria/low-card (nunique=2)\n",
            "IsActiveMember_1\n",
            "0.0    2907\n",
            "1.0    3093\n",
            "  -> tasa de Exited cuando IsActiveMember_1=1: 0.1416\n",
            "\n",
            "NumOfProducts_0: binaria/low-card (nunique=2)\n",
            "NumOfProducts_0\n",
            "0.0    2963\n",
            "1.0    3037\n",
            "  -> tasa de Exited cuando NumOfProducts_0=1: 0.2786\n",
            "\n",
            "NumOfProducts_1: binaria/low-card (nunique=2)\n",
            "NumOfProducts_1\n",
            "0.0    3238\n",
            "1.0    2762\n",
            "  -> tasa de Exited cuando NumOfProducts_1=1: 0.0731\n",
            "\n",
            "NumOfProducts_2: binaria/low-card (nunique=2)\n",
            "NumOfProducts_2\n",
            "0.0    5838\n",
            "1.0     162\n",
            "  -> tasa de Exited cuando NumOfProducts_2=1: 0.8395\n",
            "\n",
            "NumOfProducts_3: binaria/low-card (nunique=2)\n",
            "NumOfProducts_3\n",
            "0.0    5961\n",
            "1.0      39\n",
            "  -> tasa de Exited cuando NumOfProducts_3=1: 1.0000\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "seed_params = dict(seed_params)\n",
        "seed_params.setdefault(\"random_state\", RANDOM_STATE)\n",
        "seed_params.setdefault(\"n_jobs\", -1)\n",
        "seed_params.setdefault(\"eval_metric\", \"aucpr\")\n",
        "seed_params.setdefault(\"tree_method\", \"hist\")\n",
        "seed_params.setdefault(\"verbosity\", 0)\n",
        "seed_params.pop(\"verbose\", None)\n",
        "seed_params[\"n_estimators\"] = seed_params.get(\"n_estimators\") or 1000\n",
        "if seed_params.get(\"n_estimators\") is None:\n",
        "    seed_params.pop(\"n_estimators\", None)\n",
        "\n",
        "# --- Selección global ---\n",
        "keep_idx_global = None\n",
        "feature_names_used = feature_names\n",
        "X_train_fit, X_val_fit, X_test_fit = X_train, X_val, X_test\n",
        "\n",
        "if USE_REDUCED and SELECTION_MODE != \"NONE\":\n",
        "    if SELECTION_MODE == \"MI\":\n",
        "        keep_idx_global, _mi = fit_mi_selector(X_train, y_train, topk=MI_TOPK, seed=RANDOM_STATE)\n",
        "        mode_tag = f\"MI_top{MI_TOPK}\"\n",
        "        save_selection_artifacts(mode_tag, keep_idx_global, feature_names,\n",
        "                                 pd.DataFrame({\"_\":\"MI\"}), pd.DataFrame({\"_\":\"MI\"}))\n",
        "    elif SELECTION_MODE == \"L1\":\n",
        "        mode_tag = \"L1\"\n",
        "        prev = None if FORCE_REFIT_L1 else load_keep_idx_if_exists(mode_tag)\n",
        "        if prev is not None and prev.size > 0:\n",
        "            keep_idx_global = prev.astype(int).tolist()\n",
        "            l1_report = pd.DataFrame({\"note\":[\"loaded_existing_mask\"]})\n",
        "            l1_folds  = pd.DataFrame({\"note\":[\"loaded_existing_mask\"]})\n",
        "        else:\n",
        "            keep_idx_global, l1_report, l1_folds = fit_l1_selector(\n",
        "                X_train, y_train, feature_names, seed=RANDOM_STATE\n",
        "            )\n",
        "            save_selection_artifacts(mode_tag, keep_idx_global, feature_names, l1_report, l1_folds)\n",
        "        # Debug útil:\n",
        "        kept_tmp = [feature_names[i] for i in keep_idx_global]\n",
        "        dropped_tmp = [feature_names[i] for i in sorted(set(range(len(feature_names))) - set(keep_idx_global))]\n",
        "        print(f\"[SELECCIÓN {SELECTION_MODE}] Kept={len(kept_tmp)} | Dropped={len(dropped_tmp)}\")\n",
        "        print(\"[Kept]:\", kept_tmp)\n",
        "        print(\"[Dropped]:\", dropped_tmp)\n",
        "        try:\n",
        "            print(\"\\n[SELECCIÓN L1] Top por p_keep:\")\n",
        "            print(l1_report.sort_values(\"p_keep\", ascending=False).head(15).to_string(index=False))\n",
        "        except Exception:\n",
        "            pass\n",
        "    else:\n",
        "        raise ValueError(\"SELECTION_MODE inválido\")\n",
        "\n",
        "    X_train_fit = apply_keep_idx(X_train, keep_idx_global)\n",
        "    X_val_fit   = apply_keep_idx(X_val,   keep_idx_global)\n",
        "    X_test_fit  = apply_keep_idx(X_test,  keep_idx_global)\n",
        "    feature_names_used = [feature_names[i] for i in keep_idx_global]\n",
        "\n",
        "# Checks de alineación\n",
        "assert_X_names_aligned(X_train_fit, feature_names_used, \"BASELINE(train)\")\n",
        "assert_X_names_aligned(X_val_fit,   feature_names_used, \"BASELINE(val)\")\n",
        "assert_X_names_aligned(X_test_fit,  feature_names_used, \"BASELINE(test)\")\n",
        "\n",
        "# --- Balanceo con SMOTENC ---\n",
        "X_train_final, y_train_final = X_train_fit, y_train\n",
        "if USE_BALANCED_TRAIN:\n",
        "    X_train_final, y_train_final = maybe_smotenc(X_train_fit, y_train, feature_names_used)\n",
        "\n",
        "# --- Entrenamiento con early stopping ---\n",
        "model = XGBClassifier(**seed_params)\n",
        "model, best_iter = xgb_fit_with_es(\n",
        "    model,\n",
        "    X_train_final, y_train_final,\n",
        "    X_val_fit, y_val,\n",
        "    feature_names=feature_names_used,\n",
        "    rounds=200,\n",
        "    verbose=False\n",
        ")\n",
        "print(f\"[BASELINE] best_iteration: {best_iter}\")\n",
        "\n",
        "# --- Umbral óptimo por F1 en val ---\n",
        "proba_val = model.predict_proba(X_val_fit)[:, 1]\n",
        "thr_val, best_f1_val = find_best_threshold(y_val, proba_val, metric=\"f1\")\n",
        "print(f\"[BASELINE] Mejor umbral (val) por F1: {thr_val:.3f} | F1(val)={best_f1_val:.4f}\")\n",
        "\n",
        "val_metrics = compute_all_metrics(y_val, proba_val, thr_val)\n",
        "print(\"[BASELINE] Métricas val:\", {k: (round(v,4) if isinstance(v,float) else v) for k,v in val_metrics.items()})\n",
        "\n",
        "baseline = model\n",
        "base_best_it = best_iter\n",
        "tuned_model = None\n",
        "\n",
        "# DEBUG — Resumen simple por feature\n",
        "df_tr = pd.DataFrame(X_train, columns=feature_names)\n",
        "y_ser = pd.Series(y_train, name=\"Exited\")\n",
        "\n",
        "print(f\"[DEBUG] X_train shape: {df_tr.shape}\")\n",
        "for col in df_tr.columns:\n",
        "    vc = df_tr[col].nunique()\n",
        "    if vc <= 3 or set(np.unique(df_tr[col])).issubset({0,1}):\n",
        "        counts = df_tr[col].value_counts(dropna=False).sort_index()\n",
        "        pos_rate = y_ser[df_tr[col] == 1].mean() if 1 in df_tr[col].unique() else np.nan\n",
        "        print(f\"\\n{col}: binaria/low-card (nunique={vc})\")\n",
        "        print(counts.to_string())\n",
        "        print(f\"  -> tasa de Exited cuando {col}=1: {pos_rate:.4f}\" if not np.isnan(pos_rate) else \"  -> sin 1s\")\n",
        "    else:\n",
        "        print(f\"\\n{col}: numérica (nunique={vc})\")\n",
        "        desc = df_tr[col].describe(percentiles=[.05,.25,.5,.75,.95]).round(3)\n",
        "        print(desc.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d893d5",
      "metadata": {},
      "source": [
        "7 — Optimización incremental (Optuna)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "03c63f35",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-09 19:38:48,161] A new study created in memory with name: XGB_REDUCED_SMOTENC_AP\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OPTUNA] Enqueuing previous BEST as a trial seed.\n",
            "[OPTUNA] Iniciando estudio 'XGB_REDUCED_SMOTENC_AP' con 40 pruebas...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-09 19:38:50,824] Trial 0 finished with value: 0.6925914819343173 and parameters: {'learning_rate': 0.019853113568201724, 'n_estimators': 3000, 'max_depth': 4, 'min_child_weight': 0.7335742765363596, 'subsample': 0.7518096793246529, 'colsample_bytree': 0.6322009411676793, 'gamma': 0.0004329271635598477, 'reg_alpha': 1.4272322165807652e-05, 'reg_lambda': 0.0005801398193153851}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:38:54,080] Trial 1 finished with value: 0.6782713853508637 and parameters: {'learning_rate': 0.008468008575248327, 'n_estimators': 2900, 'max_depth': 8, 'min_child_weight': 4.550475813202184, 'subsample': 0.6624074561769746, 'colsample_bytree': 0.662397808134481, 'gamma': 3.200866785899844e-08, 'reg_alpha': 1.156732719914599, 'reg_lambda': 0.016136341713591334}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:38:55,109] Trial 2 finished with value: 0.6680379232385867 and parameters: {'learning_rate': 0.05675206026988748, 'n_estimators': 800, 'max_depth': 10, 'min_child_weight': 10.779361932748845, 'subsample': 0.6849356442713105, 'colsample_bytree': 0.6727299868828402, 'gamma': 3.939402261362697e-07, 'reg_alpha': 0.000134801802908908, 'reg_lambda': 0.004712973756110786}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:38:57,697] Trial 3 finished with value: 0.682415283498957 and parameters: {'learning_rate': 0.01174843954800703, 'n_estimators': 1450, 'max_depth': 7, 'min_child_weight': 0.8364645453054501, 'subsample': 0.7168578594140873, 'colsample_bytree': 0.7465447373174767, 'gamma': 9.275538076980542e-05, 'reg_alpha': 0.3134958021096912, 'reg_lambda': 2.498713568466947e-05}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:00,793] Trial 4 finished with value: 0.6688136182489869 and parameters: {'learning_rate': 0.018785426399210624, 'n_estimators': 2100, 'max_depth': 3, 'min_child_weight': 4.702115628087815, 'subsample': 0.6682096494749166, 'colsample_bytree': 0.6260206371941118, 'gamma': 1.7960847528705854, 'reg_alpha': 5.746775499181867, 'reg_lambda': 0.45580746840273345}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:03,924] Trial 5 finished with value: 0.6712951401381769 and parameters: {'learning_rate': 0.0056828375585122656, 'n_estimators': 1000, 'max_depth': 8, 'min_child_weight': 2.5358333235759627, 'subsample': 0.6488152939379115, 'colsample_bytree': 0.798070764044508, 'gamma': 1.9913367728263115e-08, 'reg_alpha': 2.318690670290199, 'reg_lambda': 6.478282331897332e-05}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:05,180] Trial 6 finished with value: 0.669406627980317 and parameters: {'learning_rate': 0.043767126303409544, 'n_estimators': 1500, 'max_depth': 7, 'min_child_weight': 3.7569262495760847, 'subsample': 0.6739417822102108, 'colsample_bytree': 0.9878338511058234, 'gamma': 0.05531681668096113, 'reg_alpha': 3.7713131110779936, 'reg_lambda': 1.8356566544355097}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:07,050] Trial 7 finished with value: 0.6907732624238612 and parameters: {'learning_rate': 0.03027406546455652, 'n_estimators': 2850, 'max_depth': 3, 'min_child_weight': 1.0302587393796305, 'subsample': 0.6180909155642152, 'colsample_bytree': 0.7301321323053057, 'gamma': 2.4048726561760165e-05, 'reg_alpha': 7.933105363733024e-05, 'reg_lambda': 0.6326486185661588}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:10,211] Trial 8 finished with value: 0.682287055356692 and parameters: {'learning_rate': 0.0076510536667541975, 'n_estimators': 1400, 'max_depth': 7, 'min_child_weight': 0.8408897660399112, 'subsample': 0.9208787923016158, 'colsample_bytree': 0.6298202574719083, 'gamma': 3.845031120156871, 'reg_alpha': 0.2545150013091294, 'reg_lambda': 2.4604229580184192e-05}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:18,119] Trial 9 finished with value: 0.6636323606577654 and parameters: {'learning_rate': 0.0010319982330247674, 'n_estimators': 2600, 'max_depth': 8, 'min_child_weight': 7.360091638366141, 'subsample': 0.9085081386743783, 'colsample_bytree': 0.6296178606936361, 'gamma': 1.3130541002425655e-05, 'reg_alpha': 6.472669269538641e-06, 'reg_lambda': 1.100839441018132}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:22,816] Trial 10 finished with value: 0.6905897887483655 and parameters: {'learning_rate': 0.00903491931619217, 'n_estimators': 2800, 'max_depth': 3, 'min_child_weight': 0.9598405377197452, 'subsample': 0.8062105254850243, 'colsample_bytree': 0.6196336134552293, 'gamma': 1.0653411478393772e-05, 'reg_alpha': 3.091645714914166e-06, 'reg_lambda': 1.2298019895276126e-06}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:26,466] Trial 11 finished with value: 0.6873002669708018 and parameters: {'learning_rate': 0.016956996325753846, 'n_estimators': 2600, 'max_depth': 3, 'min_child_weight': 2.7238715685124095, 'subsample': 0.6793460058873315, 'colsample_bytree': 0.7136591296334357, 'gamma': 4.057149495900247e-05, 'reg_alpha': 9.544410558118765e-05, 'reg_lambda': 1.5115019717177225}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:27,156] Trial 12 finished with value: 0.6875593043818974 and parameters: {'learning_rate': 0.11832923067174253, 'n_estimators': 2550, 'max_depth': 4, 'min_child_weight': 0.7313514516554619, 'subsample': 0.622220682064514, 'colsample_bytree': 0.8682308826240874, 'gamma': 0.002276561803323421, 'reg_alpha': 3.2341711760915015e-06, 'reg_lambda': 0.08524052764366954}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:28,595] Trial 13 finished with value: 0.6880757258973104 and parameters: {'learning_rate': 0.039037767686772265, 'n_estimators': 2500, 'max_depth': 4, 'min_child_weight': 0.5741715649694946, 'subsample': 0.7957031453610759, 'colsample_bytree': 0.6188561610598536, 'gamma': 0.019349885088733233, 'reg_alpha': 0.00022275632510078771, 'reg_lambda': 0.011573396176259734}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:32,791] Trial 14 finished with value: 0.6890792179255381 and parameters: {'learning_rate': 0.0071643082220628, 'n_estimators': 3000, 'max_depth': 6, 'min_child_weight': 1.9752739703134867, 'subsample': 0.8320875411340752, 'colsample_bytree': 0.7274931474344796, 'gamma': 1.5818568023798875e-05, 'reg_alpha': 1.9789268358404546e-05, 'reg_lambda': 0.005354136134697766}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:34,021] Trial 15 finished with value: 0.6847595816207911 and parameters: {'learning_rate': 0.034092125997602606, 'n_estimators': 2550, 'max_depth': 5, 'min_child_weight': 1.4745605692842014, 'subsample': 0.600719153741045, 'colsample_bytree': 0.7777533923003356, 'gamma': 6.678810771736047e-08, 'reg_alpha': 4.664738509204719e-05, 'reg_lambda': 0.00028753573830333227}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:38,724] Trial 16 finished with value: 0.6619498708984642 and parameters: {'learning_rate': 0.001510948255683071, 'n_estimators': 2150, 'max_depth': 5, 'min_child_weight': 0.7258215832325928, 'subsample': 0.7128231776771397, 'colsample_bytree': 0.6078773222970812, 'gamma': 0.00021369781346320532, 'reg_alpha': 7.274137997101101e-05, 'reg_lambda': 0.01747883047610246}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:39,306] Trial 17 finished with value: 0.6853012408586381 and parameters: {'learning_rate': 0.19494708648503298, 'n_estimators': 2250, 'max_depth': 3, 'min_child_weight': 0.6548214831456479, 'subsample': 0.6910798033911245, 'colsample_bytree': 0.6276117181175338, 'gamma': 2.8509296002601986e-07, 'reg_alpha': 0.0014156376779926058, 'reg_lambda': 0.010416174258859389}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:44,424] Trial 18 finished with value: 0.6735674975838462 and parameters: {'learning_rate': 0.0024902671444602094, 'n_estimators': 2550, 'max_depth': 4, 'min_child_weight': 1.7935538468242487, 'subsample': 0.614519528428578, 'colsample_bytree': 0.8347887311380949, 'gamma': 0.0005935883270605885, 'reg_alpha': 0.008254656323390527, 'reg_lambda': 0.00585494062698771}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:45,578] Trial 19 finished with value: 0.6772692825359474 and parameters: {'learning_rate': 0.02726813149614949, 'n_estimators': 3000, 'max_depth': 7, 'min_child_weight': 4.684605383560489, 'subsample': 0.7504111877102846, 'colsample_bytree': 0.646992349685928, 'gamma': 0.339958497426925, 'reg_alpha': 7.848203760611611e-06, 'reg_lambda': 0.00012998185839652022}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:46,854] Trial 20 finished with value: 0.6765764273134626 and parameters: {'learning_rate': 0.0312152615452766, 'n_estimators': 1900, 'max_depth': 8, 'min_child_weight': 0.9194704755954144, 'subsample': 0.7521721369892855, 'colsample_bytree': 0.6463597519432296, 'gamma': 2.068069577026522e-06, 'reg_alpha': 6.956524183869327e-05, 'reg_lambda': 0.00432774519861477}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:51,942] Trial 21 finished with value: 0.6911090958556646 and parameters: {'learning_rate': 0.008715723401768155, 'n_estimators': 3000, 'max_depth': 3, 'min_child_weight': 0.734091354405257, 'subsample': 0.8012478220352799, 'colsample_bytree': 0.6460462037148427, 'gamma': 1.2372845212252448e-06, 'reg_alpha': 1.6649782702269965e-06, 'reg_lambda': 2.8009332685953842e-06}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:54,510] Trial 22 finished with value: 0.6880882885244097 and parameters: {'learning_rate': 0.014169231943470277, 'n_estimators': 2650, 'max_depth': 4, 'min_child_weight': 1.2978662370335659, 'subsample': 0.7027458002564575, 'colsample_bytree': 0.7235029551418588, 'gamma': 0.016964142401543005, 'reg_alpha': 0.0001491822397615048, 'reg_lambda': 4.212140827613417e-05}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:39:58,873] Trial 23 finished with value: 0.6588214246738806 and parameters: {'learning_rate': 0.002390034552022777, 'n_estimators': 2500, 'max_depth': 3, 'min_child_weight': 0.9769283334216291, 'subsample': 0.8278961696976045, 'colsample_bytree': 0.6962087919097204, 'gamma': 3.017412340973072e-08, 'reg_alpha': 0.0014385499795194323, 'reg_lambda': 0.00011928868433078734}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:40:02,934] Trial 24 finished with value: 0.6916402782828001 and parameters: {'learning_rate': 0.012473160108224801, 'n_estimators': 3000, 'max_depth': 3, 'min_child_weight': 0.5236364481776052, 'subsample': 0.7363823813371374, 'colsample_bytree': 0.748254987241785, 'gamma': 6.100945615592799e-05, 'reg_alpha': 6.781801984372354e-06, 'reg_lambda': 0.00013180795480928405}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:40:05,126] Trial 25 finished with value: 0.6922459502266902 and parameters: {'learning_rate': 0.02966464702694188, 'n_estimators': 2700, 'max_depth': 3, 'min_child_weight': 1.7596109026257878, 'subsample': 0.8575558833133788, 'colsample_bytree': 0.8067302187580827, 'gamma': 5.3202383332690595e-08, 'reg_alpha': 2.176791406370892e-06, 'reg_lambda': 1.95126993361744e-05}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:40:08,137] Trial 26 finished with value: 0.686892518246228 and parameters: {'learning_rate': 0.012250546597119792, 'n_estimators': 2350, 'max_depth': 4, 'min_child_weight': 3.3981029856092992, 'subsample': 0.8075697836688828, 'colsample_bytree': 0.9232032467583694, 'gamma': 9.114975535446854e-08, 'reg_alpha': 5.791507768750103e-06, 'reg_lambda': 5.952985862428686e-05}. Best is trial 0 with value: 0.6925914819343173.\n",
            "[I 2025-12-09 19:40:10,247] Trial 27 finished with value: 0.6938172368868212 and parameters: {'learning_rate': 0.03306272205342358, 'n_estimators': 1750, 'max_depth': 3, 'min_child_weight': 0.8056681762980753, 'subsample': 0.8493794437375066, 'colsample_bytree': 0.8266138222376377, 'gamma': 1.1473724321013295e-06, 'reg_alpha': 5.6264972100784e-06, 'reg_lambda': 5.631463295646268e-06}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:10,940] Trial 28 finished with value: 0.6851078798651953 and parameters: {'learning_rate': 0.16532463425240462, 'n_estimators': 1300, 'max_depth': 3, 'min_child_weight': 0.7013014773011735, 'subsample': 0.7369612614177076, 'colsample_bytree': 0.8906193942585215, 'gamma': 1.3497889337514818e-07, 'reg_alpha': 4.781582244741876e-06, 'reg_lambda': 2.6259330238447713e-05}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:14,114] Trial 29 finished with value: 0.6856988065477285 and parameters: {'learning_rate': 0.009104143163405687, 'n_estimators': 1550, 'max_depth': 5, 'min_child_weight': 0.8416886242589904, 'subsample': 0.9427205748232085, 'colsample_bytree': 0.8544988896408794, 'gamma': 0.00011587867056448902, 'reg_alpha': 4.194787806944011e-05, 'reg_lambda': 4.452689660906414e-06}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:14,960] Trial 30 finished with value: 0.6874497921224879 and parameters: {'learning_rate': 0.05599261841568074, 'n_estimators': 2200, 'max_depth': 4, 'min_child_weight': 0.7335600190452506, 'subsample': 0.833822639478028, 'colsample_bytree': 0.8259898497862733, 'gamma': 5.125763302897694e-07, 'reg_alpha': 7.914272196554482e-05, 'reg_lambda': 6.9913788045906355e-06}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:15,527] Trial 31 finished with value: 0.6863353687988217 and parameters: {'learning_rate': 0.12001073407581518, 'n_estimators': 2950, 'max_depth': 5, 'min_child_weight': 2.9079787445911216, 'subsample': 0.8576456645118731, 'colsample_bytree': 0.7089483829790117, 'gamma': 5.28959162991884e-08, 'reg_alpha': 4.0708097623396784e-05, 'reg_lambda': 2.085024952496601e-05}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:17,413] Trial 32 finished with value: 0.686204143896056 and parameters: {'learning_rate': 0.03220802756436938, 'n_estimators': 2650, 'max_depth': 3, 'min_child_weight': 5.986274339302021, 'subsample': 0.9378295856327699, 'colsample_bytree': 0.8129663942561416, 'gamma': 2.535499359812425e-07, 'reg_alpha': 1.753990553921539e-06, 'reg_lambda': 0.00013416864896139213}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:18,750] Trial 33 finished with value: 0.6821734499699118 and parameters: {'learning_rate': 0.03209923000690139, 'n_estimators': 2900, 'max_depth': 6, 'min_child_weight': 0.7079380080873449, 'subsample': 0.7657429905223416, 'colsample_bytree': 0.61739360181458, 'gamma': 0.0016328655314180457, 'reg_alpha': 4.969199126397144e-05, 'reg_lambda': 0.00014179060382522524}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:19,718] Trial 34 finished with value: 0.6904220153472843 and parameters: {'learning_rate': 0.07429729001173255, 'n_estimators': 2850, 'max_depth': 3, 'min_child_weight': 0.8251374873583932, 'subsample': 0.7727497399878447, 'colsample_bytree': 0.8139065854626136, 'gamma': 3.673941982496439e-07, 'reg_alpha': 1.0486595664008491e-06, 'reg_lambda': 1.6662011807230646e-05}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:23,094] Trial 35 finished with value: 0.6900079147368802 and parameters: {'learning_rate': 0.012251253961908464, 'n_estimators': 2050, 'max_depth': 3, 'min_child_weight': 0.5427535879965788, 'subsample': 0.7751155341318606, 'colsample_bytree': 0.7305286541292589, 'gamma': 3.667705473618903e-05, 'reg_alpha': 1.405159816485398e-06, 'reg_lambda': 0.00010362510077793947}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:24,983] Trial 36 finished with value: 0.6902927936469099 and parameters: {'learning_rate': 0.029789354600945082, 'n_estimators': 1950, 'max_depth': 3, 'min_child_weight': 1.415688988708026, 'subsample': 0.8743672290939549, 'colsample_bytree': 0.8925553175480285, 'gamma': 6.683905042347003e-07, 'reg_alpha': 1.0996749181441457e-05, 'reg_lambda': 1.4701833738966052e-06}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:29,157] Trial 37 finished with value: 0.6821452034014097 and parameters: {'learning_rate': 0.0056412867078480524, 'n_estimators': 2750, 'max_depth': 6, 'min_child_weight': 0.8653225644922691, 'subsample': 0.9011230801827345, 'colsample_bytree': 0.8406349317340488, 'gamma': 1.0637871573624596e-08, 'reg_alpha': 2.212069992568369e-06, 'reg_lambda': 7.2984512730511306e-06}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:32,104] Trial 38 finished with value: 0.6916047741782065 and parameters: {'learning_rate': 0.012037928931264203, 'n_estimators': 3000, 'max_depth': 4, 'min_child_weight': 0.6424309319474214, 'subsample': 0.8325414478762598, 'colsample_bytree': 0.6507164804932346, 'gamma': 1.30162358455608e-05, 'reg_alpha': 1.368254235856834e-05, 'reg_lambda': 0.029127755442199813}. Best is trial 27 with value: 0.6938172368868212.\n",
            "[I 2025-12-09 19:40:34,407] Trial 39 finished with value: 0.6928187224051332 and parameters: {'learning_rate': 0.028993567163246366, 'n_estimators': 2750, 'max_depth': 3, 'min_child_weight': 0.5650591617082448, 'subsample': 0.7699313255801324, 'colsample_bytree': 0.7518216264154434, 'gamma': 0.00011425414125672156, 'reg_alpha': 8.31258322220754e-05, 'reg_lambda': 0.0015671608104375254}. Best is trial 27 with value: 0.6938172368868212.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OPTUNA] Mejor AP(val): 0.693817\n",
            "[OPTUNA] Params ganadores: {'learning_rate': 0.03306272205342358, 'n_estimators': 1750, 'max_depth': 3, 'min_child_weight': 0.8056681762980753, 'subsample': 0.8493794437375066, 'colsample_bytree': 0.8266138222376377, 'gamma': 1.1473724321013295e-06, 'reg_alpha': 5.6264972100784e-06, 'reg_lambda': 5.631463295646268e-06}\n",
            "[OPTUNA] best_iteration (del trial): 1081\n",
            "[OPTUNA] Guardado BEST en: BEST_XGB_REDUCED_SMOTENC.json\n",
            "[OPTUNA] Reentreno final completado. best_iteration = 1081\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "tuned_model = None\n",
        "N_TRIALS = 40\n",
        "STUDY_NAME = f\"XGB_{VIEW_TAG}_{BAL_TAG}_AP\"\n",
        "SAMPLER = TPESampler(seed=RANDOM_STATE, multivariate=True, group=False)\n",
        "study = optuna.create_study(direction=\"maximize\", study_name=STUDY_NAME, sampler=SAMPLER)\n",
        "\n",
        "SEARCH_KEYS = [\n",
        "    \"learning_rate\", \"n_estimators\", \"max_depth\", \"min_child_weight\",\n",
        "    \"subsample\", \"colsample_bytree\", \"gamma\", \"reg_alpha\", \"reg_lambda\"\n",
        "]\n",
        "\n",
        "def suggest_xgb_params(trial):\n",
        "    p = {}\n",
        "    p[\"learning_rate\"]    = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
        "    p[\"n_estimators\"]     = trial.suggest_int(\"n_estimators\", 800, 3000, step=50)\n",
        "    p[\"max_depth\"]        = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    p[\"min_child_weight\"] = trial.suggest_float(\"min_child_weight\", 0.5, 20.0, log=True)\n",
        "    p[\"subsample\"]        = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
        "    p[\"colsample_bytree\"] = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
        "    p[\"gamma\"]            = trial.suggest_float(\"gamma\", 1e-8, 5.0, log=True)\n",
        "    p[\"reg_alpha\"]        = trial.suggest_float(\"reg_alpha\", 1e-6, 10.0, log=True)\n",
        "    p[\"reg_lambda\"]       = trial.suggest_float(\"reg_lambda\", 1e-6, 10.0, log=True)\n",
        "    p[\"random_state\"]     = RANDOM_STATE\n",
        "    p[\"n_jobs\"]           = -1\n",
        "    p[\"eval_metric\"]      = \"aucpr\"\n",
        "    p[\"tree_method\"]      = \"hist\"\n",
        "    p[\"verbosity\"]        = 0\n",
        "    return p\n",
        "\n",
        "# Warm-start\n",
        "if BEST_HP_FILE.exists():\n",
        "    try:\n",
        "        prev = json.loads(BEST_HP_FILE.read_text())\n",
        "        warm = {k: prev[k] for k in SEARCH_KEYS if k in prev}\n",
        "        if warm:\n",
        "            print(\"[OPTUNA] Enqueuing previous BEST as a trial seed.\")\n",
        "            study.enqueue_trial(warm)\n",
        "    except Exception as e:\n",
        "        print(\"[OPTUNA] Aviso: no se pudo usar BEST para warm-start:\", e)\n",
        "\n",
        "def objective(trial):\n",
        "    hp = suggest_xgb_params(trial)\n",
        "    mdl = XGBClassifier(**{**seed_params, **hp})\n",
        "    mdl, best_it = xgb_fit_with_es(\n",
        "        mdl,\n",
        "        X_train_final, y_train_final,\n",
        "        X_val_fit, y_val,\n",
        "        feature_names=feature_names_used,\n",
        "        rounds=200,\n",
        "        verbose=False\n",
        "    )\n",
        "    proba_val_t = mdl.predict_proba(X_val_fit)[:, 1]\n",
        "    ap = average_precision_score(y_val, proba_val_t)\n",
        "    trial.set_user_attr(\"best_iteration\", best_it)\n",
        "    return ap\n",
        "\n",
        "print(f\"[OPTUNA] Iniciando estudio '{STUDY_NAME}' con {N_TRIALS} pruebas...\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
        "\n",
        "best = study.best_trial\n",
        "print(f\"[OPTUNA] Mejor AP(val): {best.value:.6f}\")\n",
        "print(f\"[OPTUNA] Params ganadores:\", best.params)\n",
        "print(f\"[OPTUNA] best_iteration (del trial):\", best.user_attrs.get(\"best_iteration\"))\n",
        "\n",
        "best_params = dict(best.params)\n",
        "best_params.update({\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "    \"n_jobs\": -1,\n",
        "    \"eval_metric\": \"aucpr\",\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"verbosity\": 0\n",
        "})\n",
        "with open(BEST_HP_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
        "print(\"[OPTUNA] Guardado BEST en:\", BEST_HP_FILE.name)\n",
        "\n",
        "tuned_model = XGBClassifier(**best_params)\n",
        "tuned_model, best_it = xgb_fit_with_es(\n",
        "    tuned_model,\n",
        "    X_train_final, y_train_final,\n",
        "    X_val_fit, y_val,\n",
        "    feature_names=feature_names_used,\n",
        "    rounds=200,\n",
        "    verbose=False\n",
        ")\n",
        "print(\"[OPTUNA] Reentreno final completado. best_iteration =\", best_it)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8797a95",
      "metadata": {},
      "source": [
        "8 — Cross-Validation (OOF) para baseline y tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "efb9478a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV-BASELINE] Guardados: cv_summary_XGB_REDUCED_SMOTENC_BASELINE_CV5.csv | oof_XGB_REDUCED_SMOTENC_BASELINE_CV5.parquet\n",
            "[CV-TUNED] Guardados: cv_summary_XGB_REDUCED_SMOTENC_TUNED_CV5.csv | oof_XGB_REDUCED_SMOTENC_TUNED_CV5.parquet\n"
          ]
        }
      ],
      "source": [
        "def run_oof_cv_xgb(model_params, X, y, feature_names_in, k_folds=CV_FOLDS, seed=RANDOM_STATE, exp_suffix=\"BASELINE\"):\n",
        "    assert_X_names_aligned(X, feature_names_in, f\"CV(INPUT)-{exp_suffix}\")\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
        "    oof_proba = np.zeros_like(y, dtype=float)\n",
        "    fold_rows = []\n",
        "\n",
        "    base = dict(model_params)\n",
        "    base.pop(\"verbose\", None)\n",
        "    base.setdefault(\"verbosity\", 0)\n",
        "    base.setdefault(\"eval_metric\", \"aucpr\")\n",
        "    base.setdefault(\"tree_method\", \"hist\")\n",
        "    base.setdefault(\"random_state\", seed)\n",
        "    base.setdefault(\"n_jobs\", -1)\n",
        "    base[\"n_estimators\"] = base.get(\"n_estimators\") or 1000\n",
        "\n",
        "    for f, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        X_tr0, X_va0 = X[tr_idx], X[va_idx]\n",
        "        y_tr0, y_va0 = y[tr_idx], y[va_idx]\n",
        "\n",
        "        feat_names_fold = list(feature_names_in)\n",
        "\n",
        "        # --- Selección por fold ---\n",
        "        if USE_REDUCED and SELECTION_MODE != \"NONE\":\n",
        "            if SELECTION_MODE == \"MI\":\n",
        "                keep_idx, _ = fit_mi_selector(X_tr0, y_tr0, topk=MI_TOPK, seed=seed)\n",
        "            elif SELECTION_MODE == \"L1\":\n",
        "                keep_idx, _rep, _log = fit_l1_selector(X_tr0, y_tr0, feat_names_fold, seed=seed)\n",
        "            else:\n",
        "                raise ValueError(\"SELECTION_MODE inválido\")\n",
        "\n",
        "            X_tr0 = apply_keep_idx(X_tr0, keep_idx)\n",
        "            X_va0 = apply_keep_idx(X_va0, keep_idx)\n",
        "            feat_names_fold = [feat_names_fold[i] for i in keep_idx]\n",
        "\n",
        "        # --- Balanceo por fold con SMOTENC ---\n",
        "        if BALANCE_IN_CV and USE_BALANCED_TRAIN:\n",
        "            X_tr, y_tr = maybe_smotenc(X_tr0, y_tr0, feat_names_fold)\n",
        "        else:\n",
        "            X_tr, y_tr = X_tr0, y_tr0\n",
        "\n",
        "        # Sanity checks\n",
        "        assert_X_names_aligned(X_tr, feat_names_fold, f\"CV(fold={f})-train\")\n",
        "        assert_X_names_aligned(X_va0, feat_names_fold, f\"CV(fold={f})-val\")\n",
        "\n",
        "        mdl = XGBClassifier(**base)\n",
        "        adapter, best_it = xgb_fit_with_es(\n",
        "            mdl, X_tr, y_tr, X_va0, y_va0,\n",
        "            feature_names=feat_names_fold,\n",
        "            rounds=200, verbose=False\n",
        "        )\n",
        "\n",
        "        proba_va = adapter.predict_proba(X_va0)[:, 1]\n",
        "        oof_proba[va_idx] = proba_va\n",
        "\n",
        "        fold_rows.append({\n",
        "            \"fold\": f,\n",
        "            \"pr_auc\": average_precision_score(y_va0, proba_va),\n",
        "            \"roc_auc\": roc_auc_score(y_va0, proba_va),\n",
        "            \"best_iteration\": best_it if best_it is not None else np.nan\n",
        "        })\n",
        "\n",
        "    oof_pr = average_precision_score(y, oof_proba)\n",
        "    oof_roc = roc_auc_score(y, oof_proba)\n",
        "    thr_oof, _ = find_best_threshold(y, oof_proba, metric=\"f1\")\n",
        "    y_oof_pred = (oof_proba >= thr_oof).astype(int)\n",
        "    oof_f1  = f1_score(y, y_oof_pred, zero_division=0)\n",
        "    oof_rec = recall_score(y, y_oof_pred, zero_division=0)\n",
        "    oof_bal = balanced_accuracy_score(y, y_oof_pred)\n",
        "\n",
        "    cv_tag = f\"{EXP_NAME}_{exp_suffix}_CV{CV_FOLDS}\"\n",
        "    cv_csv = OUT_RESULTS / f\"cv_summary_{cv_tag}.csv\"\n",
        "    folds_df = pd.DataFrame(fold_rows)\n",
        "    agg_row = pd.DataFrame([{\n",
        "        \"fold\": \"OOF\", \"pr_auc\": oof_pr, \"roc_auc\": oof_roc,\n",
        "        \"thr\": thr_oof, \"f1\": oof_f1, \"recall\": oof_rec, \"bal_acc\": oof_bal\n",
        "    }])\n",
        "    pd.concat([folds_df, agg_row], ignore_index=True).to_csv(cv_csv, index=False)\n",
        "\n",
        "    oof_path = OUT_PREDS / f\"oof_{cv_tag}.parquet\"\n",
        "    pd.DataFrame({\"oof_proba\": oof_proba, \"y_true\": y}).to_parquet(oof_path, index=False)\n",
        "\n",
        "    print(f\"[CV-{exp_suffix}] Guardados: {cv_csv.name} | {oof_path.name}\")\n",
        "    return {\"oof_pr_auc\": oof_pr, \"oof_roc_auc\": oof_roc, \"thr\": thr_oof,\n",
        "            \"oof_f1\": oof_f1, \"oof_recall\": oof_rec, \"oof_bal_acc\": oof_bal}\n",
        "\n",
        "cv_baseline = None\n",
        "cv_tuned = None\n",
        "\n",
        "if DO_CV_BASELINE:\n",
        "    cv_baseline = run_oof_cv_xgb(seed_params, X_train_fit, y_train, feature_names_used, exp_suffix=\"BASELINE\")\n",
        "\n",
        "if DO_CV_TUNED and tuned_model is not None:\n",
        "    cv_tuned = run_oof_cv_xgb(best_params, X_train_fit, y_train, feature_names_used, exp_suffix=\"TUNED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15dae995",
      "metadata": {},
      "source": [
        "9 — Evaluación en test + guardados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "983f541f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK][BASE] Guardados: \n",
            "  - Seed HPs   : XGB_REDUCED_SMOTENC_BASE_seed_params.json \n",
            "  - Fitted HPs : XGB_REDUCED_SMOTENC_BASE_fitted_params.json \n",
            "  - Importancias: XGB_REDUCED_SMOTENC_feature_importances.csv \n",
            "  - Preds test  : preds_test_XGB_REDUCED_SMOTENC.parquet \n",
            "  - Baselines   : baselines.csv\n",
            "[OK][TUNED] Guardados: \n",
            "  - Fitted HPs : XGB_REDUCED_SMOTENC_TUNED_fitted_params.json \n",
            "  - Importancias: XGB_REDUCED_SMOTENC_TUNED_feature_importances.csv \n",
            "  - Preds test  : preds_test_XGB_REDUCED_SMOTENC_TUNED.parquet \n",
            "  - Baselines   : baselines.csv\n"
          ]
        }
      ],
      "source": [
        "def plot_pr_curve(y_true, y_proba, title, out_path):\n",
        "    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n",
        "    ap = average_precision_score(y_true, y_proba)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.step(rec, prec, where='post')\n",
        "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
        "    plt.title(f'{title} (AP={ap:.4f})')\n",
        "    plt.grid(True, linestyle='--', alpha=.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150); plt.close()\n",
        "\n",
        "def plot_roc_curve(y_true, y_proba, title, out_path):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    auc = roc_auc_score(y_true, y_proba)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(fpr, tpr, lw=2)\n",
        "    plt.plot([0,1],[0,1], 'k--', lw=1)\n",
        "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{title} (AUC={auc:.4f})')\n",
        "    plt.grid(True, linestyle='--', alpha=.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150); plt.close()\n",
        "\n",
        "def plot_confusion(y_true, y_pred, title, out_path, normalize=False):\n",
        "    norm = 'true' if normalize else None\n",
        "    cm = confusion_matrix(y_true, y_pred, normalize=norm)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    im = plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    ticks = np.arange(2)\n",
        "    plt.xticks(ticks, ['0','1']); plt.yticks(ticks, ['0','1'])\n",
        "    thresh = cm.max()/2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            txt = f'{cm[i,j]:.2f}' if normalize else str(cm[i,j])\n",
        "            plt.text(j, i, txt, ha='center', va='center',\n",
        "                     color='white' if cm[i,j] > thresh else 'black')\n",
        "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150); plt.close()\n",
        "\n",
        "def xgb_gain_importances(booster, feature_names):\n",
        "    \"\"\"Devuelve array de importancias 'gain' alineado a feature_names.\"\"\"\n",
        "    gain_dict = booster.get_score(importance_type=\"gain\")\n",
        "    name_to_idx = {n: i for i, n in enumerate(feature_names)}\n",
        "    imp_gain = np.zeros(len(feature_names), dtype=float)\n",
        "    for k, v in gain_dict.items():\n",
        "        # k puede ser \"f12\" o el nombre real de la feature\n",
        "        if k.startswith(\"f\") and k[1:].isdigit():\n",
        "            idx = int(k[1:])\n",
        "        else:\n",
        "            idx = name_to_idx.get(k, None)\n",
        "        if idx is not None and 0 <= idx < len(imp_gain):\n",
        "            imp_gain[idx] = v\n",
        "    return imp_gain\n",
        "\n",
        "# ——— Evaluación y guardados ———\n",
        "base = EXP_NAME\n",
        "\n",
        "# BASELINE\n",
        "proba_test = model.predict_proba(X_test_fit)[:, 1]\n",
        "y_pred_test = (proba_test >= thr_val).astype(int)\n",
        "test_metrics = compute_all_metrics(y_test, proba_test, thr_val)\n",
        "\n",
        "# Guardar HP baseline (seed y fitted)\n",
        "params_seed_path = OUT_PARAMS / f\"{base}_BASE_seed_params.json\"\n",
        "with open(params_seed_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(seed_params, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "params_fitted_path = OUT_PARAMS / f\"{base}_BASE_fitted_params.json\"\n",
        "with open(params_fitted_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(model.get_params(), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Figuras baseline\n",
        "plot_pr_curve(y_val,  proba_val,  f\"{base} — PR (val)\",  OUT_FIGS / f\"{base}_pr_val.png\")\n",
        "plot_pr_curve(y_test, proba_test, f\"{base} — PR (test)\", OUT_FIGS / f\"{base}_pr_test.png\")\n",
        "plot_roc_curve(y_val,  proba_val,  f\"{base} — ROC (val)\",  OUT_FIGS / f\"{base}_roc_val.png\")\n",
        "plot_roc_curve(y_test, proba_test, f\"{base} — ROC (test)\", OUT_FIGS / f\"{base}_roc_test.png\")\n",
        "plot_confusion(y_test, y_pred_test, f\"{base} — Confusion (test @thr={thr_val:.3f})\", OUT_FIGS / f\"{base}_cm_test.png\")\n",
        "\n",
        "# Importancias baseline\n",
        "try:\n",
        "    booster = model.get_booster()\n",
        "    imp_gain = xgb_gain_importances(booster, feature_names_used)\n",
        "except Exception:\n",
        "    imp_gain = np.zeros(len(feature_names_used))\n",
        "\n",
        "imp_df = pd.DataFrame({\n",
        "    \"feature\": feature_names_used[:len(imp_gain)],\n",
        "    \"importance_gain\": imp_gain\n",
        "}).sort_values(\"importance_gain\", ascending=False)\n",
        "imp_path = OUT_RESULTS / f\"{base}_feature_importances.csv\"\n",
        "imp_df.to_csv(imp_path, index=False)\n",
        "\n",
        "# Preds test baseline\n",
        "preds_path = OUT_PREDS / f\"preds_test_{base}.parquet\"\n",
        "pd.DataFrame({\"proba\": proba_test, \"y_true\": y_test}).to_parquet(preds_path, index=False)\n",
        "\n",
        "best_iter_base = getattr(model, \"best_iteration\", getattr(model, \"best_ntree_limit\", None))\n",
        "row_base = {\n",
        "    \"model\": base,\n",
        "    \"thr_val\": thr_val,\n",
        "    \"val_pr_auc\": val_metrics[\"pr_auc\"],\n",
        "    \"val_roc_auc\": val_metrics[\"roc_auc\"],\n",
        "    \"val_precision\": val_metrics[\"precision\"],\n",
        "    \"val_f1\": val_metrics[\"f1\"],\n",
        "    \"val_recall\": val_metrics[\"recall\"],\n",
        "    \"val_bal_acc\": val_metrics[\"bal_acc\"],\n",
        "    \"test_pr_auc\": test_metrics[\"pr_auc\"],\n",
        "    \"test_roc_auc\": test_metrics[\"roc_auc\"],\n",
        "    \"test_precision\": test_metrics[\"precision\"],\n",
        "    \"test_f1\": test_metrics[\"f1\"],\n",
        "    \"test_recall\": test_metrics[\"recall\"],\n",
        "    \"test_bal_acc\": test_metrics[\"bal_acc\"],\n",
        "    \"best_iteration\": best_iter_base if best_iter_base is not None else np.nan\n",
        "}\n",
        "res_csv = OUT_RESULTS / \"baselines.csv\"\n",
        "pd.DataFrame([row_base]).to_csv(res_csv, mode=(\"a\" if res_csv.exists() else \"w\"),\n",
        "                                index=False, header=not res_csv.exists())\n",
        "\n",
        "print(\"[OK][BASE] Guardados:\",\n",
        "      \"\\n  - Seed HPs   :\", params_seed_path.name,\n",
        "      \"\\n  - Fitted HPs :\", params_fitted_path.name,\n",
        "      \"\\n  - Importancias:\", imp_path.name,\n",
        "      \"\\n  - Preds test  :\", preds_path.name,\n",
        "      \"\\n  - Baselines   :\", res_csv.name)\n",
        "\n",
        "# TUNED\n",
        "if tuned_model is not None:\n",
        "    proba_val_tuned = tuned_model.predict_proba(X_val_fit)[:, 1]\n",
        "    thr_val_tuned, _ = find_best_threshold(y_val, proba_val_tuned, metric=\"f1\")\n",
        "    val_metrics_tuned = compute_all_metrics(y_val, proba_val_tuned, thr_val_tuned)\n",
        "\n",
        "    proba_test_tuned = tuned_model.predict_proba(X_test_fit)[:, 1]\n",
        "    y_pred_test_tuned = (proba_test_tuned >= thr_val_tuned).astype(int)\n",
        "    test_metrics_tuned = compute_all_metrics(y_test, proba_test_tuned, thr_val_tuned)\n",
        "\n",
        "    tuned_fitted_path = OUT_PARAMS / f\"{base}_TUNED_fitted_params.json\"\n",
        "    with open(tuned_fitted_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(tuned_model.get_params(), f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    base_t = base + \"_TUNED\"\n",
        "    plot_pr_curve(y_val,  proba_val_tuned,  f\"{base_t} — PR (val)\",  OUT_FIGS / f\"{base_t}_pr_val.png\")\n",
        "    plot_pr_curve(y_test, proba_test_tuned, f\"{base_t} — PR (test)\", OUT_FIGS / f\"{base_t}_pr_test.png\")\n",
        "    plot_roc_curve(y_val,  proba_val_tuned,  f\"{base_t} — ROC (val)\",  OUT_FIGS / f\"{base_t}_roc_val.png\")\n",
        "    plot_roc_curve(y_test, proba_test_tuned, f\"{base_t} — ROC (test)\", OUT_FIGS / f\"{base_t}_roc_test.png\")\n",
        "    plot_confusion(y_test, y_pred_test_tuned, f\"{base_t} — Confusion (test @thr={thr_val_tuned:.3f})\", OUT_FIGS / f\"{base_t}_cm_test.png\")\n",
        "\n",
        "    try:\n",
        "        booster_t = tuned_model.get_booster()\n",
        "        imp_gain_t = xgb_gain_importances(booster_t, feature_names_used)\n",
        "    except Exception:\n",
        "        imp_gain_t = np.zeros(len(feature_names_used))\n",
        "\n",
        "    imp_t_path = OUT_RESULTS / f\"{base_t}_feature_importances.csv\"\n",
        "    pd.DataFrame({\n",
        "        \"feature\": feature_names_used[:len(imp_gain_t)],\n",
        "        \"importance_gain\": imp_gain_t\n",
        "    }).sort_values(\"importance_gain\", ascending=False).to_csv(imp_t_path, index=False)\n",
        "    preds_t_path = OUT_PREDS / f\"preds_test_{base_t}.parquet\"\n",
        "    pd.DataFrame({\"proba\": proba_test_tuned, \"y_true\": y_test}).to_parquet(preds_t_path, index=False)\n",
        "\n",
        "    best_iter_tuned = getattr(tuned_model, \"best_iteration\", getattr(tuned_model, \"best_ntree_limit\", None))\n",
        "    row_t = {\n",
        "        \"model\": base_t,\n",
        "        \"thr_val\": thr_val_tuned,\n",
        "        \"val_pr_auc\": val_metrics_tuned[\"pr_auc\"],\n",
        "        \"val_roc_auc\": val_metrics_tuned[\"roc_auc\"],\n",
        "        \"val_precision\": val_metrics_tuned[\"precision\"],\n",
        "        \"val_f1\": val_metrics_tuned[\"f1\"],\n",
        "        \"val_recall\": val_metrics_tuned[\"recall\"],\n",
        "        \"val_bal_acc\": val_metrics_tuned[\"bal_acc\"],\n",
        "        \"test_pr_auc\": test_metrics_tuned[\"pr_auc\"],\n",
        "        \"test_roc_auc\": test_metrics_tuned[\"roc_auc\"],\n",
        "        \"test_precision\": test_metrics_tuned[\"precision\"],\n",
        "        \"test_f1\": test_metrics_tuned[\"f1\"],\n",
        "        \"test_recall\": test_metrics_tuned[\"recall\"],\n",
        "        \"test_bal_acc\": test_metrics_tuned[\"bal_acc\"],\n",
        "        \"best_iteration\": best_iter_tuned\n",
        "    }\n",
        "    pd.DataFrame([row_t]).to_csv(res_csv, mode=\"a\", index=False, header=False)\n",
        "\n",
        "    print(\"[OK][TUNED] Guardados:\",\n",
        "          \"\\n  - Fitted HPs :\", tuned_fitted_path.name,\n",
        "          \"\\n  - Importancias:\", imp_t_path.name,\n",
        "          \"\\n  - Preds test  :\", preds_t_path.name,\n",
        "          \"\\n  - Baselines   :\", res_csv.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "928335ce",
      "metadata": {},
      "source": [
        "10 — Mejores resultados + resumen CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a837bea0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MEJORES EN TEST (por métrica) ===\n",
            "- test_pr_auc: XGB_REDUCED_SMOTENC_TUNED | PR-AUC=0.7090 | ROC-AUC=0.8597 | F1=0.6111 | Recall=0.6216 | Precision=0.6010 | thr(val)=0.481 | best_iter=1081\n",
            "- test_roc_auc: XGB_REDUCED_SMOTENC | PR-AUC=0.7069 | ROC-AUC=0.8604 | F1=0.6145 | Recall=0.6462 | Precision=0.5857 | thr(val)=0.452 | best_iter=1182\n",
            "- test_recall: XGB_REDUCED_SMOTENC | PR-AUC=0.7069 | ROC-AUC=0.8604 | F1=0.6145 | Recall=0.6462 | Precision=0.5857 | thr(val)=0.452 | best_iter=1182\n",
            "- test_f1: XGB_REDUCED_SMOTENC | PR-AUC=0.7069 | ROC-AUC=0.8604 | F1=0.6145 | Recall=0.6462 | Precision=0.5857 | thr(val)=0.452 | best_iter=1182\n",
            "- test_precision: XGB_REDUCED_SMOTENC_TUNED | PR-AUC=0.7090 | ROC-AUC=0.8597 | F1=0.6111 | Recall=0.6216 | Precision=0.6010 | thr(val)=0.481 | best_iter=1081\n",
            "=== RESUMEN CV-OOF (por experimento) ===\n",
            "                             tag   pr_auc  roc_auc       f1   recall  bal_acc   thr\n",
            "   XGB_REDUCED_SMOTENC_TUNED_CV5 0.686903 0.857152 0.614623 0.659853 0.767544 0.458\n",
            "XGB_REDUCED_SMOTENC_BASELINE_CV5 0.684376 0.854981 0.611903 0.596893 0.753125 0.527\n",
            "=== COMPARACIÓN SOTA XGBOOST vs. MEJOR TEST ===\n",
            "Paper XGBoost: AUC=0.8512 | Recall=N/R | Precision=N/R\n",
            "Tu mejor   : AUC=0.8604 | Recall=0.6462 | Precision=0.5857\n",
            "Deltas     : ΔAUC=+0.0092\n",
            "Fuente SOTA: Shukla (2021), ICSCC — Kaggle Bank Churn (10k)\n",
            "[OK] Normalizado. Backup: baselines_legacy_backup.csv\n"
          ]
        }
      ],
      "source": [
        "AGGREGATE_ALL_RUNS = False\n",
        "\n",
        "def safe(v, fmt=\".4f\"):\n",
        "    try:\n",
        "        return f\"{float(v):{fmt}}\"\n",
        "    except Exception:\n",
        "        return \"NA\"\n",
        "\n",
        "base_csv = OUT_RESULTS / \"baselines.csv\"\n",
        "if not base_csv.exists():\n",
        "    raise FileNotFoundError(f\"No existe {base_csv}\")\n",
        "\n",
        "df = pd.read_csv(base_csv)\n",
        "\n",
        "needed = [\n",
        "    \"model\",\"thr_val\",\n",
        "    \"val_pr_auc\",\"val_roc_auc\",\"val_precision\",\"val_f1\",\"val_recall\",\"val_bal_acc\",\n",
        "    \"test_pr_auc\",\"test_roc_auc\",\"test_precision\",\"test_f1\",\"test_recall\",\"test_bal_acc\",\n",
        "    \"best_iteration\"\n",
        "]\n",
        "for c in needed:\n",
        "    if c not in df.columns:\n",
        "        df[c] = pd.NA \n",
        "\n",
        "\n",
        "df = df[needed].copy()\n",
        "\n",
        "\n",
        "num_cols = [c for c in needed if c not in (\"model\",)]\n",
        "for c in num_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "\n",
        "if AGGREGATE_ALL_RUNS:\n",
        "    root_art = ARTIF_DIR.parent\n",
        "    for p in (root_art).glob(\"XGB_*/results/baselines.csv\"):\n",
        "        if p == base_csv:\n",
        "            continue\n",
        "        try:\n",
        "            d2 = pd.read_csv(p)\n",
        "            for c in needed:\n",
        "                if c not in d2.columns:\n",
        "                    d2[c] = pd.NA\n",
        "            d2 = d2[needed]\n",
        "            for c in num_cols:\n",
        "                d2[c] = pd.to_numeric(d2[c], errors=\"coerce\")\n",
        "            df = pd.concat([df, d2], ignore_index=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"El dataframe de resultados está vacío.\")\n",
        "\n",
        "df = df.drop_duplicates(subset=[\"model\"], keep=\"last\").copy()\n",
        "\n",
        "def best_by(metric):\n",
        "    if metric not in df.columns or df[metric].dropna().empty:\n",
        "        return None\n",
        "    r = df.loc[df[metric].idxmax()]\n",
        "    print(\n",
        "        f\"- {metric}: {r['model']} | \"\n",
        "        f\"PR-AUC={safe(r['test_pr_auc'])} | \"\n",
        "        f\"ROC-AUC={safe(r['test_roc_auc'])} | \"\n",
        "        f\"F1={safe(r['test_f1'])} | \"\n",
        "        f\"Recall={safe(r['test_recall'])} | \"\n",
        "        f\"Precision={safe(r['test_precision'])} | \"\n",
        "        f\"thr(val)={safe(r['thr_val'], '.3f')} | \"\n",
        "        f\"best_iter={int(r['best_iteration']) if pd.notna(r['best_iteration']) else 'NA'}\"\n",
        "    )\n",
        "    return r\n",
        "\n",
        "print(\"=== MEJORES EN TEST (por métrica) ===\")\n",
        "winners = {}\n",
        "for m in [\"test_pr_auc\",\"test_roc_auc\",\"test_recall\",\"test_f1\",\"test_precision\"]:\n",
        "    w = best_by(m)\n",
        "    if w is not None:\n",
        "        winners[m] = w\n",
        "\n",
        "\n",
        "cv_files = list(OUT_RESULTS.glob(\"cv_summary_*_CV*.csv\"))\n",
        "if cv_files:\n",
        "    print(\"=== RESUMEN CV-OOF (por experimento) ===\")\n",
        "    rows = []\n",
        "    for f in cv_files:\n",
        "        tag = re.sub(r\"^cv_summary_|\\.csv$\", \"\", f.name)\n",
        "        cv = pd.read_csv(f)\n",
        "        oof = cv.loc[cv[\"fold\"] == \"OOF\"]\n",
        "        if not oof.empty:\n",
        "            r = oof.iloc[0]\n",
        "            rows.append({\n",
        "                \"tag\": tag,\n",
        "                \"pr_auc\": r.get(\"pr_auc\"),\n",
        "                \"roc_auc\": r.get(\"roc_auc\"),\n",
        "                \"f1\": r.get(\"f1\"),\n",
        "                \"recall\": r.get(\"recall\"),\n",
        "                \"bal_acc\": r.get(\"bal_acc\"),\n",
        "                \"thr\": r.get(\"thr\"),\n",
        "            })\n",
        "    if rows:\n",
        "        print(pd.DataFrame(rows).sort_values([\"pr_auc\",\"roc_auc\"], ascending=False).to_string(index=False))\n",
        "else:\n",
        "    print(\"(No se hallaron archivos de CV para este experimento)\")\n",
        "\n",
        "SOTA_XGB = {\n",
        "    \"AUC\": 0.8512,        \n",
        "    \"Recall\": None,       \n",
        "    \"Precision\": None,    \n",
        "    \"source\": \"Shukla (2021), ICSCC — Kaggle Bank Churn (10k)\"\n",
        "}\n",
        "\n",
        "if \"test_roc_auc\" in winners and winners[\"test_roc_auc\"] is not None:\n",
        "    bt = winners[\"test_roc_auc\"]\n",
        "    d_auc = float(bt[\"test_roc_auc\"]) - SOTA_XGB[\"AUC\"]\n",
        "    print(\"=== COMPARACIÓN SOTA XGBOOST vs. MEJOR TEST ===\")\n",
        "    print(f\"Paper XGBoost: AUC={SOTA_XGB['AUC']:.4f} | Recall={SOTA_XGB['Recall'] or 'N/R'} | Precision={SOTA_XGB['Precision'] or 'N/R'}\")\n",
        "    print(f\"Tu mejor   : AUC={safe(bt['test_roc_auc'])} | Recall={safe(bt['test_recall'])} | Precision={safe(bt['test_precision'])}\")\n",
        "    print(f\"Deltas     : ΔAUC={d_auc:+.4f}\")\n",
        "    print(f\"Fuente SOTA: {SOTA_XGB['source']}\")\n",
        "else:\n",
        "    print(\"No se pudo localizar el ganador por ROC-AUC para comparar contra SOTA.\")\n",
        "\n",
        "backup = OUT_RESULTS / \"baselines_legacy_backup.csv\"\n",
        "base_csv.replace(backup)\n",
        "df.to_csv(base_csv, index=False)\n",
        "print(\"[OK] Normalizado. Backup:\", backup.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a63e3987",
      "metadata": {},
      "source": [
        "11 - Mejores resultados + resumen CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "97790cc8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== REPORTE FINAL DE SELECCIÓN DE FEATURES ===\n",
            "[L1] Total features originales: 15\n",
            "[L1] Features conservadas: 12\n",
            "[L1] Features eliminadas: 3\n",
            "\n",
            "Conservadas:\n",
            "['num__CreditScore', 'num__Age', 'num__Tenure', 'num__Balance', 'num__EstimatedSalary', 'Geography_1', 'Gender_1', 'HasCrCard_1', 'IsActiveMember_1', 'NumOfProducts_1', 'NumOfProducts_2', 'NumOfProducts_3']\n",
            "\n",
            "Eliminadas:\n",
            "['Geography_0', 'Geography_2', 'NumOfProducts_0']\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== REPORTE FINAL DE SELECCIÓN DE FEATURES ===\")\n",
        "if not USE_REDUCED or SELECTION_MODE == \"NONE\":\n",
        "    print(\"Selección: DESACTIVADA (se usaron todas las features).\")\n",
        "else:\n",
        "    mode_tag = (\"MI_top\" + str(MI_TOPK)) if SELECTION_MODE==\"MI\" else \"L1\"\n",
        "    mask_path = SEL_DIR / f\"keep_idx_{mode_tag}.npy\"\n",
        "    if not mask_path.exists():\n",
        "        print(f\"Aviso: no se encontró máscara en {mask_path}. ¿Ejecutaste la sección de selección?\")\n",
        "    else:\n",
        "        keep_idx = np.load(mask_path).astype(int).tolist()\n",
        "        kept_features = [feature_names[i] for i in keep_idx]\n",
        "        dropped_idx = sorted(set(range(len(feature_names))) - set(keep_idx))\n",
        "        dropped_features = [feature_names[i] for i in dropped_idx]\n",
        "\n",
        "        # Guardados informativos\n",
        "        pd.DataFrame({\"feature\": kept_features}).to_csv(SEL_DIR / f\"final_kept_{mode_tag}.csv\", index=False)\n",
        "        pd.DataFrame({\"feature\": dropped_features}).to_csv(SEL_DIR / f\"final_dropped_{mode_tag}.csv\", index=False)\n",
        "\n",
        "        print(f\"[{mode_tag}] Total features originales: {len(feature_names)}\")\n",
        "        print(f\"[{mode_tag}] Features conservadas: {len(kept_features)}\")\n",
        "        print(f\"[{mode_tag}] Features eliminadas: {len(dropped_features)}\")\n",
        "        print(\"\\nConservadas:\")\n",
        "        print(kept_features)\n",
        "        print(\"\\nEliminadas:\")\n",
        "        print(dropped_features)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
