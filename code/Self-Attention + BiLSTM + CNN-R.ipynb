{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "sec1",
      "metadata": {},
      "source": [
        "1 — Imports, configuración y rutas (DL híbrido con reducción L1: Self-Attention + BiLSTM + CNN-R)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "sec1code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exp: DL_REDUCED_SMOTENC\n",
            "DATA_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/preproc_datasets/full\n",
            "ARTIF_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/DL_REDUCED_SMOTENC\n",
            "DEVICE: cpu\n"
          ]
        }
      ],
      "source": [
        "import json, os, warnings, time, re, glob, math, random\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score, precision_recall_curve, roc_auc_score, roc_curve,\n",
        "    f1_score, recall_score, balanced_accuracy_score, confusion_matrix, precision_score\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# Balanceo\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTENC\n",
        "    _HAS_IMBLEARN = True\n",
        "except Exception:\n",
        "    _HAS_IMBLEARN = False\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Toggles de experimento ===\n",
        "USE_REDUCED = True               \n",
        "USE_BALANCED_TRAIN = True       \n",
        "BALANCE_IN_CV = True              \n",
        "RANDOM_STATE = 42\n",
        "DO_TUNE = True\n",
        "DO_CV_BASELINE = True\n",
        "DO_CV_TUNED = True\n",
        "CV_FOLDS = 5\n",
        "\n",
        "# === Nombres y rutas ===\n",
        "ROOT = Path.cwd().parent\n",
        "EXP_NAME = f\"DL_{'REDUCED' if USE_REDUCED else 'FULL'}_{'SMOTENC' if USE_BALANCED_TRAIN else 'IMB'}\"\n",
        "ARTIF_DIR = ROOT / \"artifacts\" / EXP_NAME\n",
        "OUT_RESULTS = ARTIF_DIR / \"results\"\n",
        "OUT_FIGS    = ARTIF_DIR / \"figs\"\n",
        "OUT_PREDS   = ARTIF_DIR / \"preds\"\n",
        "OUT_PARAMS  = ARTIF_DIR / \"best_params\"\n",
        "for p in [OUT_RESULTS, OUT_FIGS, OUT_PREDS, OUT_PARAMS]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Dataset preprocesado\n",
        "DATA_DIR = ROOT / \"preproc_datasets\" / \"full\"\n",
        "\n",
        "print(\"Exp:\", EXP_NAME)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"ARTIF_DIR:\", ARTIF_DIR)\n",
        "\n",
        "# Seeds globales\n",
        "def set_seeds(seed=RANDOM_STATE):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seeds(RANDOM_STATE)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"DEVICE:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec2",
      "metadata": {},
      "source": [
        "2 — Carga de artefactos (X, y, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "sec2code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes (FULL): (6000, 15) (2000, 15) (2000, 15)\n",
            "y train/val/test: (6000,) (2000,) (2000,)\n",
            "n features (FULL): 15\n"
          ]
        }
      ],
      "source": [
        "def load_xy_full(dir_full: Path):\n",
        "    X_train = np.load(dir_full / \"X_train_full.npy\")\n",
        "    X_val   = np.load(dir_full / \"X_val_full.npy\")\n",
        "    X_test  = np.load(dir_full / \"X_test_full.npy\")\n",
        "\n",
        "    y_train = pd.read_parquet(dir_full / \"y_train.parquet\")[\"Exited\"].to_numpy()\n",
        "    y_val   = pd.read_parquet(dir_full / \"y_val.parquet\")[\"Exited\"].to_numpy()\n",
        "    y_test  = pd.read_parquet(dir_full / \"y_test.parquet\")[\"Exited\"].to_numpy()\n",
        "\n",
        "    feat = pd.read_parquet(dir_full / \"feature_names_full.parquet\")[\"feature\"].tolist()\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, feat\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_xy_full(DATA_DIR)\n",
        "print(\"Shapes (FULL):\", X_train.shape, X_val.shape, X_test.shape)\n",
        "print(\"y train/val/test:\", y_train.shape, y_val.shape, y_test.shape)\n",
        "print(\"n features (FULL):\", len(feature_names))\n",
        "\n",
        "# Tipos consistentes\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_val   = X_val.astype(np.float32)\n",
        "X_test  = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_val   = y_val.astype(np.int64)\n",
        "y_test  = y_test.astype(np.int64)\n",
        "\n",
        "PREPROC_META = DATA_DIR / \"preprocessor_meta.json\"\n",
        "cat_cols_order = None\n",
        "if PREPROC_META.exists():\n",
        "    try:\n",
        "        with open(PREPROC_META, \"r\", encoding=\"utf-8\") as f:\n",
        "            _meta = json.load(f)\n",
        "        cat_cols_order = _meta.get(\"cat_cols\", None)\n",
        "    except Exception:\n",
        "        cat_cols_order = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec2b",
      "metadata": {},
      "source": [
        "2b — Reducción de características vía L1 (Logistic Regression) + metadata SMOTENC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "sec2bcode",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[L1] Iniciando selección L1 con C=0.01, threshold=median...\n",
            "[L1] Features seleccionadas: 15 de 15\n",
            "[L1] Primeras features seleccionadas: ['num__CreditScore', 'num__Age', 'num__Tenure', 'num__Balance', 'num__EstimatedSalary', 'Geography_0', 'Geography_1', 'Geography_2', 'Gender_1', 'HasCrCard_1']\n",
            "Shapes (REDUCED): (6000, 15) (2000, 15) (2000, 15)\n",
            "[SMOTENC] 5 grupos OHE; 10 dims categóricas; 5 numéricas\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# === Config L1 ===\n",
        "L1_C = 0.01            \n",
        "L1_THRESHOLD = \"median\"  \n",
        "L1_MAX_ITER = 4000\n",
        "\n",
        "def build_smote_metadata(feature_names, cat_cols_order):\n",
        "    \"\"\"Reconstruye índices numéricos y OHE para SMOTENC a partir de feature_names actuales.\"\"\"\n",
        "    num_idx = [i for i, n in enumerate(feature_names) if str(n).startswith(\"num__\")]\n",
        "    ohe_idx = [i for i in range(len(feature_names)) if i not in num_idx]\n",
        "    base_to_idx = defaultdict(list)\n",
        "    for i in ohe_idx:\n",
        "        base = str(feature_names[i]).split(\"_\", 1)[0]  # <col>_<cat> -> <col>\n",
        "        base_to_idx[base].append(i)\n",
        "    for k in base_to_idx:\n",
        "        base_to_idx[k] = sorted(base_to_idx[k])\n",
        "    if cat_cols_order:\n",
        "        ONEHOT_GROUPS = [base_to_idx[c] for c in cat_cols_order if c in base_to_idx]\n",
        "    else:\n",
        "        ONEHOT_GROUPS = [base_to_idx[k] for k in sorted(base_to_idx.keys(), key=lambda k: min(base_to_idx[k]))]\n",
        "    CAT_IDX = sorted([j for grp in ONEHOT_GROUPS for j in grp])\n",
        "    print(f\"[SMOTENC] {len(ONEHOT_GROUPS)} grupos OHE; {len(CAT_IDX)} dims categóricas; {len(num_idx)} numéricas\")\n",
        "    return num_idx, ONEHOT_GROUPS, CAT_IDX\n",
        "\n",
        "if USE_REDUCED:\n",
        "    print(f\"[L1] Iniciando selección L1 con C={L1_C}, threshold={L1_THRESHOLD}...\")\n",
        "    scaler_l1 = StandardScaler()\n",
        "    X_train_l1 = scaler_l1.fit_transform(X_train)\n",
        "    X_val_l1   = scaler_l1.transform(X_val)\n",
        "    X_test_l1  = scaler_l1.transform(X_test)\n",
        "\n",
        "    base_l1 = LogisticRegression(\n",
        "        penalty=\"l1\",\n",
        "        C=L1_C,\n",
        "        solver=\"saga\",\n",
        "        max_iter=L1_MAX_ITER,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    selector = SelectFromModel(base_l1, threshold=L1_THRESHOLD)\n",
        "    selector.fit(X_train_l1, y_train)\n",
        "    mask = selector.get_support()\n",
        "    idx_keep = np.where(mask)[0]\n",
        "\n",
        "    if len(idx_keep) == 0:\n",
        "        print(\"[L1] ADVERTENCIA: no se seleccionó ninguna feature, se mantiene el conjunto FULL.\")\n",
        "    else:\n",
        "        print(f\"[L1] Features seleccionadas: {len(idx_keep)} de {len(mask)}\")\n",
        "        X_train = X_train[:, idx_keep].astype(np.float32)\n",
        "        X_val   = X_val[:, idx_keep].astype(np.float32)\n",
        "        X_test  = X_test[:, idx_keep].astype(np.float32)\n",
        "        feature_names = [feature_names[i] for i in idx_keep]\n",
        "        print(\"[L1] Primeras features seleccionadas:\", feature_names[:10])\n",
        "        print(\"Shapes (REDUCED):\", X_train.shape, X_val.shape, X_test.shape)\n",
        "else:\n",
        "    print(\"[L1] USE_REDUCED=False -> no se aplica selección L1 (se mantiene FULL).\")\n",
        "\n",
        "# Construir metadata SMOTENC\n",
        "num_idx, ONEHOT_GROUPS, CAT_IDX = build_smote_metadata(feature_names, cat_cols_order)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec3",
      "metadata": {},
      "source": [
        "3 — Métricas, threshold y plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "sec3code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pr_auc(y_true, y_proba):\n",
        "    return float(average_precision_score(y_true, y_proba))\n",
        "\n",
        "def roc_auc(y_true, y_proba):\n",
        "    return float(roc_auc_score(y_true, y_proba))\n",
        "\n",
        "def find_best_threshold(y_true, y_proba, metric=\"f1\"):\n",
        "    thr_grid = np.linspace(0.0, 1.0, 1001)\n",
        "    best_thr, best_score = 0.5, -1.0\n",
        "    for thr in thr_grid:\n",
        "        y_pred = (y_proba >= thr).astype(int)\n",
        "        if metric == \"f1\":\n",
        "            score = f1_score(y_true, y_pred, zero_division=0)\n",
        "        elif metric == \"recall\":\n",
        "            score = recall_score(y_true, y_pred, zero_division=0)\n",
        "        else:\n",
        "            raise ValueError(\"metric no soportada\")\n",
        "        if score > best_score:\n",
        "            best_score, best_thr = score, thr\n",
        "    return float(best_thr), float(best_score)\n",
        "\n",
        "def compute_all_metrics(y_true, y_proba, thr):\n",
        "    y_pred = (y_proba >= thr).astype(int)\n",
        "    return {\n",
        "        \"pr_auc\": pr_auc(y_true, y_proba),\n",
        "        \"roc_auc\": roc_auc(y_true, y_proba),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"bal_acc\": balanced_accuracy_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "def plot_pr_curve(y_true, y_proba, title, out_path):\n",
        "    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n",
        "    ap = average_precision_score(y_true, y_proba)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.step(rec, prec, where='post')\n",
        "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
        "    plt.title(f'{title} (AP={ap:.4f})')\n",
        "    plt.grid(True, linestyle='--', alpha=.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150); plt.close()\n",
        "\n",
        "def plot_roc_curve(y_true, y_proba, title, out_path):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    auc = roc_auc_score(y_true, y_proba)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(fpr, tpr, lw=2)\n",
        "    plt.plot([0,1],[0,1], 'k--', lw=1)\n",
        "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{title} (AUC={auc:.4f})')\n",
        "    plt.grid(True, linestyle='--', alpha=.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150); plt.close()\n",
        "\n",
        "def plot_confusion(y_true, y_pred, title, out_path, normalize=False):\n",
        "    norm = 'true' if normalize else None\n",
        "    cm = confusion_matrix(y_true, y_pred, normalize=norm)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    im = plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    ticks = np.arange(2)\n",
        "    plt.xticks(ticks, ['0','1']); plt.yticks(ticks, ['0','1'])\n",
        "    thresh = cm.max()/2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            txt = f'{cm[i,j]:.2f}' if normalize else str(cm[i,j])\n",
        "            plt.text(j, i, txt, ha='center', va='center',\n",
        "                     color='white' if cm[i,j] > thresh else 'black')\n",
        "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150); plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec4",
      "metadata": {},
      "source": [
        "4 — Helpers: SMOTE, Dataset y utilidades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "sec4code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _repair_onehot_blocks(X, groups):\n",
        "    X = X.copy()\n",
        "    if not groups:\n",
        "        return X.astype(np.float32)\n",
        "    rows = np.arange(X.shape[0])\n",
        "    for grp in groups:\n",
        "        if len(grp) == 1:\n",
        "            c = grp[0]\n",
        "            X[:, c] = (X[:, c] >= 0.5).astype(np.float32)\n",
        "        else:\n",
        "            block = X[:, grp]\n",
        "            winners = np.argmax(block, axis=1)\n",
        "            X[:, grp] = 0.0\n",
        "            X[rows, np.array(grp)[winners]] = 1.0\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "def maybe_resample(X, y, seed=RANDOM_STATE):\n",
        "\n",
        "    if not _HAS_IMBLEARN or X.shape[0] != y.shape[0] or len(CAT_IDX) == 0:\n",
        "        return X, y\n",
        "    try:\n",
        "        sm = SMOTENC(categorical_features=CAT_IDX, random_state=seed)\n",
        "        Xb, yb = sm.fit_resample(X, y)\n",
        "        Xb = _repair_onehot_blocks(Xb, ONEHOT_GROUPS)\n",
        "        return Xb.astype(np.float32), yb.astype(np.int64)\n",
        "    except Exception as e:\n",
        "        print(\"[SMOTENC] Aviso: se usará dataset original por error:\", e)\n",
        "        return X, y\n",
        "\n",
        "class TabDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.X[idx]\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def class_pos_weight(y):\n",
        "    # pos_weight = N_neg / N_pos\n",
        "    y = np.asarray(y)\n",
        "    n_pos = (y == 1).sum()\n",
        "    n_neg = (y == 0).sum()\n",
        "    if n_pos == 0:\n",
        "        return 1.0\n",
        "    return float(n_neg / max(1, n_pos))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec5",
      "metadata": {},
      "source": [
        "5 — Modelo DL híbrido (Self-Attention + BiLSTM + CNN) y entrenamiento con early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "sec5code",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScalarFeatureTokenizer(nn.Module):\n",
        "    def __init__(self, n_features, d_model):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(n_features, d_model) * 0.02)\n",
        "        self.bias   = nn.Parameter(torch.zeros(n_features, d_model))\n",
        "    def forward(self, x):  # x: (B, F)\n",
        "        return x.unsqueeze(-1) * self.weight + self.bias  # (B, F, d_model)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout, batch_first=True)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ff  = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):  # (B, F, d)\n",
        "        attn_out, _ = self.mha(x, x, x, need_weights=False)\n",
        "        x = self.ln1(x + self.dropout(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "class SELayer1D(nn.Module):\n",
        "    def __init__(self, channels, reduction=8):\n",
        "        super().__init__()\n",
        "        hidden = max(1, channels // reduction)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, hidden), nn.ReLU(inplace=True), nn.Linear(hidden, channels), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):  \n",
        "        s = x.mean(dim=-1)  \n",
        "        w = self.fc(s)      \n",
        "        return x * w.unsqueeze(-1)\n",
        "\n",
        "class CCPNetLite(nn.Module):\n",
        "    def __init__(self, n_features, d_model=48, n_heads=4, n_layers=2, lstm_hidden=64,\n",
        "                 cnn_channels=64, kernel_size=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.n_features = n_features\n",
        "        self.tokenizer = ScalarFeatureTokenizer(n_features, d_model)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model=d_model, n_heads=n_heads, d_ff=4*d_model, dropout=dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.bi_lstm = nn.LSTM(input_size=d_model, hidden_size=lstm_hidden, batch_first=True, bidirectional=True)\n",
        "        conv_in = lstm_hidden * 2\n",
        "        pad = kernel_size // 2\n",
        "        self.conv = nn.Conv1d(conv_in, cnn_channels, kernel_size=kernel_size, padding=pad)\n",
        "        self.se   = SELayer1D(cnn_channels)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(cnn_channels*2, 128), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x): \n",
        "        t = self.tokenizer(x)                \n",
        "        for blk in self.blocks:\n",
        "            t = blk(t)                        \n",
        "        lstm_out, _ = self.bi_lstm(t)         \n",
        "        z = lstm_out.transpose(1, 2)          \n",
        "        z = self.conv(z)                      \n",
        "        z = F.gelu(z)\n",
        "        z = self.se(z)                     \n",
        "        # Global avg + max pooling\n",
        "        gap = z.mean(dim=-1)\n",
        "        gmp, _ = z.max(dim=-1)\n",
        "        g = torch.cat([gap, gmp], dim=1)     \n",
        "        logit = self.head(g).squeeze(1)      \n",
        "        return logit\n",
        "    \n",
        "    def feature_importance(self, feature_names):\n",
        "        with torch.no_grad():\n",
        "            w = self.tokenizer.weight.detach().cpu().numpy() \n",
        "            imp = np.linalg.norm(w, axis=1)\n",
        "        return pd.DataFrame({\"feature\": feature_names, \"importance_proxy\": imp}).sort_values(\"importance_proxy\", ascending=False)\n",
        "\n",
        "def get_dl_defaults(seed=RANDOM_STATE):\n",
        "    return {\n",
        "        \"d_model\": 48,\n",
        "        \"n_heads\": 4,\n",
        "        \"n_layers\": 2,\n",
        "        \"lstm_hidden\": 64,\n",
        "        \"cnn_channels\": 64,\n",
        "        \"kernel_size\": 3,\n",
        "        \"dropout\": 0.2,\n",
        "        \"lr\": 1e-3,\n",
        "        \"weight_decay\": 1e-4,\n",
        "        \"batch_size\": 256,\n",
        "        \"epochs\": 100,\n",
        "        \"patience\": 12,\n",
        "        \"random_state\": seed\n",
        "    }\n",
        "\n",
        "def make_model(n_features, hp):\n",
        "    mdl = CCPNetLite(\n",
        "        n_features=n_features,\n",
        "        d_model=int(hp[\"d_model\"]),\n",
        "        n_heads=int(hp[\"n_heads\"]),\n",
        "        n_layers=int(hp[\"n_layers\"]),\n",
        "        lstm_hidden=int(hp[\"lstm_hidden\"]),\n",
        "        cnn_channels=int(hp[\"cnn_channels\"]),\n",
        "        kernel_size=int(hp[\"kernel_size\"]),\n",
        "        dropout=float(hp[\"dropout\"]) \n",
        "    ).to(DEVICE)\n",
        "    return mdl\n",
        "\n",
        "def train_one(model, X_tr, y_tr, X_va, y_va, hp, verbose=False):\n",
        "    set_seeds(RANDOM_STATE)\n",
        "    bs = int(hp[\"batch_size\"]) if \"batch_size\" in hp else 256\n",
        "    epochs = int(hp.get(\"epochs\", 100))\n",
        "    patience = int(hp.get(\"patience\", 12))\n",
        "    lr = float(hp.get(\"lr\", 1e-3))\n",
        "    wd = float(hp.get(\"weight_decay\", 1e-4))\n",
        "\n",
        "    did_smote = hp.get(\"_did_smote\", False)\n",
        "    pw = 1.0 if did_smote else class_pos_weight(y_tr)\n",
        "    pos_w = torch.tensor([pw], dtype=torch.float32, device=DEVICE)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_w)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    dl_tr = DataLoader(TabDataset(X_tr, y_tr), batch_size=bs, shuffle=True, num_workers=0, pin_memory=False)\n",
        "    dl_va = DataLoader(TabDataset(X_va, y_va), batch_size=bs, shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "    best_ap = -1.0\n",
        "    best_epoch = -1\n",
        "    best_state = None\n",
        "    wait = 0\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for xb, yb in dl_tr:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logit = model(xb)\n",
        "            loss = criterion(logit, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running += float(loss.item())\n",
        "\n",
        "        # Validación\n",
        "        model.eval()\n",
        "        all_probs = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dl_va:\n",
        "                xb = xb.to(DEVICE)\n",
        "                logit = model(xb)\n",
        "                prob = torch.sigmoid(logit).detach().cpu().numpy()\n",
        "                all_probs.append(prob)\n",
        "        va_proba = np.concatenate(all_probs, axis=0)\n",
        "        ap = average_precision_score(y_va, va_proba)\n",
        "\n",
        "        if verbose and ep % 10 == 0:\n",
        "            print(f\"[EP {ep:03d}] loss={running/len(dl_tr):.4f} | AP(val)={ap:.4f}\")\n",
        "\n",
        "        # Early stopping por AP\n",
        "        if ap > best_ap + 1e-6:\n",
        "            best_ap = ap\n",
        "            best_epoch = ep\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, best_epoch, best_ap\n",
        "\n",
        "def predict_proba(model, X, batch_size=512):\n",
        "    model.eval()\n",
        "    dl = DataLoader(TabDataset(X, None), batch_size=batch_size, shuffle=False)\n",
        "    probs = []\n",
        "    with torch.no_grad():\n",
        "        for xb in dl:\n",
        "            xb = xb.to(DEVICE)\n",
        "            logit = model(xb)\n",
        "            prob = torch.sigmoid(logit).detach().cpu().numpy()\n",
        "            probs.append(prob)\n",
        "    return np.concatenate(probs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec6",
      "metadata": {},
      "source": [
        "6 — Hiperparámetros persistentes (carga/guardado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "sec6code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HP] Cargando mejores hiperparámetros previos: BEST_DL_REDUCED_SMOTENC.json\n"
          ]
        }
      ],
      "source": [
        "VIEW_TAG = \"REDUCED\" if USE_REDUCED else \"FULL\"\n",
        "BAL_TAG  = \"SMOTENC\" if USE_BALANCED_TRAIN else \"IMB\"\n",
        "BEST_HP_FILE = OUT_PARAMS / f\"BEST_DL_{VIEW_TAG}_{BAL_TAG}.json\"\n",
        "\n",
        "def load_best_or_default():\n",
        "    base = get_dl_defaults()\n",
        "    if BEST_HP_FILE.exists():\n",
        "        try:\n",
        "            best = json.loads(BEST_HP_FILE.read_text())\n",
        "            print(\"[HP] Cargando mejores hiperparámetros previos:\", BEST_HP_FILE.name)\n",
        "            base.update(best)\n",
        "            return base, True\n",
        "        except Exception as e:\n",
        "            print(\"[HP] Aviso: no se pudo leer BEST (uso defaults).\", e)\n",
        "    print(\"[HP] Usando hiperparámetros DEFAULT de DL.\")\n",
        "    return base, False\n",
        "\n",
        "seed_params, loaded_best_flag = load_best_or_default()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec7",
      "metadata": {},
      "source": [
        "7 — Entrenamiento BASELINE + umbral (DL-R)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "sec7code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BASELINE] best_epoch: 18\n",
            "[BASELINE] Mejor umbral (val) por F1: 0.783 | F1(val)=0.6437\n",
            "[BASELINE] Métricas val: {'pr_auc': 0.675, 'roc_auc': 0.8524, 'precision': 0.6437, 'f1': 0.6437, 'recall': 0.6437, 'bal_acc': 0.7764}\n"
          ]
        }
      ],
      "source": [
        "set_seeds(RANDOM_STATE)\n",
        "\n",
        "feature_names_used = feature_names\n",
        "X_train_fit, X_val_fit, X_test_fit = X_train, X_val, X_test\n",
        "\n",
        "X_train_final, y_train_final = X_train_fit, y_train\n",
        "did_smote_flag = False\n",
        "if USE_BALANCED_TRAIN:\n",
        "    X_train_final, y_train_final = maybe_resample(X_train_fit, y_train)\n",
        "    did_smote_flag = True\n",
        "\n",
        "base_hp = dict(seed_params)\n",
        "base_hp[\"_did_smote\"] = did_smote_flag\n",
        "model = make_model(n_features=X_train_final.shape[1], hp=base_hp)\n",
        "model, best_epoch = train_one(model, X_train_final, y_train_final, X_val_fit, y_val, base_hp, verbose=False)[:2]\n",
        "print(f\"[BASELINE] best_epoch: {best_epoch}\")\n",
        "\n",
        "proba_val = predict_proba(model, X_val_fit)\n",
        "thr_val, best_f1_val = find_best_threshold(y_val, proba_val, metric=\"f1\")\n",
        "# Guardar predicciones de validación (baseline)\n",
        "val_preds_path = OUT_PREDS / f\"preds_val_{EXP_NAME}.parquet\"\n",
        "pd.DataFrame({\n",
        "    \"proba\": proba_val,\n",
        "    \"y_true\": y_val,\n",
        "    \"y_pred\": (proba_val >= thr_val).astype(int)\n",
        "}).to_parquet(val_preds_path, index=False)\n",
        "print(f\"[BASELINE] Mejor umbral (val) por F1: {thr_val:.3f} | F1(val)={best_f1_val:.4f}\")\n",
        "\n",
        "val_metrics = compute_all_metrics(y_val, proba_val, thr_val)\n",
        "print(\"[BASELINE] Métricas val:\", {k: (round(v,4) if isinstance(v,float) else v) for k,v in val_metrics.items()})\n",
        "\n",
        "baseline = model\n",
        "base_best_it = best_epoch\n",
        "tuned_model = None\n",
        "best_params = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec8",
      "metadata": {},
      "source": [
        "8 — Optimización incremental (Optuna) sobre AP(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "sec8code",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-11 20:26:28,681] A new study created in memory with name: DL_REDUCED_SMOTENC_AP\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OPTUNA] Enqueuing previous BEST as a trial seed.\n",
            "[OPTUNA] Iniciando estudio 'DL_REDUCED_SMOTENC_AP' con 40 pruebas...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-11 20:29:45,104] Trial 0 finished with value: 0.6800995608928453 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.330222144734749, 'lr': 0.0006202506844188844, 'weight_decay': 0.0007018196903507625, 'batch_size': 128}. Best is trial 0 with value: 0.6800995608928453.\n",
            "[I 2025-12-11 20:31:07,623] Trial 1 finished with value: 0.6804549872487516 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.2623782158161189, 'lr': 0.0008110848199986004, 'weight_decay': 1.461896279370496e-05, 'batch_size': 128}. Best is trial 1 with value: 0.6804549872487516.\n",
            "[I 2025-12-11 20:33:40,075] Trial 2 finished with value: 0.6655783515415348 and parameters: {'d_model': 64, 'n_heads': 8, 'n_layers': 1, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.06101911742238941, 'lr': 0.0009382059110341113, 'weight_decay': 1.3726318898045876e-06, 'batch_size': 128}. Best is trial 1 with value: 0.6804549872487516.\n",
            "[I 2025-12-11 20:38:49,577] Trial 3 finished with value: 0.6693257890348131 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 128, 'kernel_size': 5, 'dropout': 0.17837666334679464, 'lr': 0.0005728695840313613, 'weight_decay': 0.00014817820606039095, 'batch_size': 256}. Best is trial 1 with value: 0.6804549872487516.\n",
            "[I 2025-12-11 20:42:59,837] Trial 4 finished with value: 0.6823178815871469 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.36480308916903204, 'lr': 0.0013022031035668192, 'weight_decay': 0.0035387588647792408, 'batch_size': 512}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 20:44:46,382] Trial 5 finished with value: 0.6721225118493361 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 1, 'lstm_hidden': 96, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.038489954914396496, 'lr': 0.0005846187064070543, 'weight_decay': 4.414536876494481e-06, 'batch_size': 128}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 20:47:37,550] Trial 6 finished with value: 0.6712049552390105 and parameters: {'d_model': 32, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 128, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.0599326836668414, 'lr': 0.0006527342871386315, 'weight_decay': 0.005910698619088547, 'batch_size': 512}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 20:49:15,962] Trial 7 finished with value: 0.6710826249547257 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 1, 'lstm_hidden': 64, 'cnn_channels': 64, 'kernel_size': 5, 'dropout': 0.1210276357557502, 'lr': 0.0014101223696388783, 'weight_decay': 0.0011129571947046015, 'batch_size': 256}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 20:53:46,038] Trial 8 finished with value: 0.6729023921198235 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 1, 'lstm_hidden': 96, 'cnn_channels': 96, 'kernel_size': 3, 'dropout': 0.46836499436836726, 'lr': 0.0004117581451393256, 'weight_decay': 2.31347816122256e-05, 'batch_size': 256}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 20:55:14,882] Trial 9 finished with value: 0.6675407518462552 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 64, 'cnn_channels': 96, 'kernel_size': 3, 'dropout': 0.042069982497524416, 'lr': 0.00043526121378702535, 'weight_decay': 0.00392840951347923, 'batch_size': 128}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 20:57:14,616] Trial 10 finished with value: 0.6718181218539384 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 2, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.4287259193077081, 'lr': 0.0012660090657411408, 'weight_decay': 0.0011988702402602421, 'batch_size': 128}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 20:59:12,055] Trial 11 finished with value: 0.6692338631399083 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.4105478390421756, 'lr': 0.0008927891705292822, 'weight_decay': 1.7634130669464894e-05, 'batch_size': 128}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 21:01:42,899] Trial 12 finished with value: 0.6586414558634128 and parameters: {'d_model': 32, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.21046534040479656, 'lr': 0.0017436707485944856, 'weight_decay': 0.00019371768797919473, 'batch_size': 512}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 21:02:52,683] Trial 13 finished with value: 0.6745493920372643 and parameters: {'d_model': 32, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.17878661562935236, 'lr': 0.0010853060900192504, 'weight_decay': 1.926881616581825e-06, 'batch_size': 128}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 21:05:17,373] Trial 14 finished with value: 0.6628876760174341 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.2526098042619616, 'lr': 0.0007857342555035363, 'weight_decay': 0.0031166595588019964, 'batch_size': 512}. Best is trial 4 with value: 0.6823178815871469.\n",
            "[I 2025-12-11 21:07:30,117] Trial 15 finished with value: 0.6826182997671721 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.22214608643825792, 'lr': 0.00031320717119808503, 'weight_decay': 6.510041299647659e-06, 'batch_size': 128}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:10:05,640] Trial 16 finished with value: 0.6645722265280694 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 32, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.3133901269213137, 'lr': 0.0015809136630319798, 'weight_decay': 0.0009986673195670247, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:12:45,358] Trial 17 finished with value: 0.6616322964756612 and parameters: {'d_model': 32, 'n_heads': 2, 'n_layers': 2, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.1510255585646686, 'lr': 0.00031935422422303694, 'weight_decay': 2.329660122190237e-05, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:14:07,892] Trial 18 finished with value: 0.6712738274638133 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.007529708030892146, 'lr': 0.00041916678241096436, 'weight_decay': 2.015145390037271e-06, 'batch_size': 128}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:20:39,551] Trial 19 finished with value: 0.6669505385414684 and parameters: {'d_model': 32, 'n_heads': 8, 'n_layers': 2, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.34325105387647986, 'lr': 0.0011580606215211824, 'weight_decay': 0.003054108520478099, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:23:18,662] Trial 20 finished with value: 0.6639255151496595 and parameters: {'d_model': 32, 'n_heads': 8, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.3145570402362998, 'lr': 0.00040161945248885756, 'weight_decay': 7.828771825272853e-06, 'batch_size': 128}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:28:03,761] Trial 21 finished with value: 0.6763061657440694 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 2, 'lstm_hidden': 32, 'cnn_channels': 96, 'kernel_size': 5, 'dropout': 0.275496684186942, 'lr': 0.00044656364851723804, 'weight_decay': 0.00012592213571807091, 'batch_size': 128}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:30:01,814] Trial 22 finished with value: 0.6701968701142648 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.22519519801596383, 'lr': 0.000337168469061825, 'weight_decay': 1.5275209301236423e-05, 'batch_size': 128}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:35:00,244] Trial 23 finished with value: 0.6790780023425717 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.4248065860415017, 'lr': 0.0012319598714309284, 'weight_decay': 0.005382523502350273, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:36:38,923] Trial 24 finished with value: 0.6813640058792164 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.4073409304077744, 'lr': 0.0006664447980179472, 'weight_decay': 1.2247382357674004e-05, 'batch_size': 256}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:39:18,397] Trial 25 finished with value: 0.6770958074055443 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.39680254457324476, 'lr': 0.0005969635898489381, 'weight_decay': 2.1658564366940325e-05, 'batch_size': 256}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:41:50,256] Trial 26 finished with value: 0.6769671054924499 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.2992301027952126, 'lr': 0.0007631163713071362, 'weight_decay': 0.00015338771318339935, 'batch_size': 256}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:46:09,585] Trial 27 finished with value: 0.6805158188023366 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 64, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.44959179281857276, 'lr': 0.002095564807783489, 'weight_decay': 0.009784079820505356, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:51:12,016] Trial 28 finished with value: 0.6708455904457581 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 32, 'cnn_channels': 128, 'kernel_size': 5, 'dropout': 0.4158069671299806, 'lr': 0.000998665810011918, 'weight_decay': 0.0003992446686662372, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:53:30,299] Trial 29 finished with value: 0.6707619196053919 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 96, 'cnn_channels': 64, 'kernel_size': 5, 'dropout': 0.42669277566657243, 'lr': 0.0007709824031264846, 'weight_decay': 1.629526320516206e-06, 'batch_size': 256}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 21:56:49,009] Trial 30 finished with value: 0.6696305632675603 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 1, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.36564575188740794, 'lr': 0.0008549823339682083, 'weight_decay': 1.3841546848676599e-06, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 22:00:23,905] Trial 31 finished with value: 0.6698017299134322 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 64, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.4022903950911167, 'lr': 0.0022246807096271263, 'weight_decay': 0.008368574156282822, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 22:02:40,622] Trial 32 finished with value: 0.6564189694378891 and parameters: {'d_model': 32, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.4232097558628598, 'lr': 0.0023459963522694423, 'weight_decay': 0.003874602098903358, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 22:05:54,641] Trial 33 finished with value: 0.6650585486196944 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 64, 'cnn_channels': 96, 'kernel_size': 5, 'dropout': 0.35695862824292507, 'lr': 0.0022665192597771332, 'weight_decay': 0.005436303221947907, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 22:10:07,444] Trial 34 finished with value: 0.6702522417090926 and parameters: {'d_model': 32, 'n_heads': 8, 'n_layers': 3, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.4886845713617113, 'lr': 0.0025397506103001927, 'weight_decay': 0.0017211231331557652, 'batch_size': 256}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 22:12:54,933] Trial 35 finished with value: 0.6695238865312855 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 2, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.1152744978580575, 'lr': 0.00039538537661684437, 'weight_decay': 1.961565313100464e-06, 'batch_size': 256}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 22:15:18,340] Trial 36 finished with value: 0.6669136907222246 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 32, 'cnn_channels': 64, 'kernel_size': 5, 'dropout': 0.35411424675481185, 'lr': 0.0020994111930250927, 'weight_decay': 0.0082214937002706, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 22:17:43,058] Trial 37 finished with value: 0.6667417520281789 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 2, 'lstm_hidden': 64, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.49386700441155607, 'lr': 0.0016507791348079576, 'weight_decay': 0.009731117201890042, 'batch_size': 512}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 22:21:12,550] Trial 38 finished with value: 0.6815627426600563 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.36267219969961295, 'lr': 0.0004263549593979771, 'weight_decay': 1.1062914476976025e-05, 'batch_size': 128}. Best is trial 15 with value: 0.6826182997671721.\n",
            "[I 2025-12-11 22:23:44,306] Trial 39 finished with value: 0.6735264980427381 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 2, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.2680455638141338, 'lr': 0.0003462205847974607, 'weight_decay': 2.6433130083532084e-06, 'batch_size': 128}. Best is trial 15 with value: 0.6826182997671721.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OPTUNA] Mejor AP(val): 0.682618\n",
            "[OPTUNA] Params ganadores: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.22214608643825792, 'lr': 0.00031320717119808503, 'weight_decay': 6.510041299647659e-06, 'batch_size': 128}\n",
            "[OPTUNA] best_epoch (del trial): 38\n",
            "[OPTUNA] Guardado BEST en: BEST_DL_REDUCED_SMOTENC.json\n",
            "[OPTUNA] Reentreno final completado. best_epoch = 21\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "N_TRIALS = 40\n",
        "STUDY_NAME = f\"DL_{VIEW_TAG}_{BAL_TAG}_AP\"\n",
        "SAMPLER = TPESampler(seed=RANDOM_STATE, multivariate=True, group=False)\n",
        "study = optuna.create_study(direction=\"maximize\", study_name=STUDY_NAME, sampler=SAMPLER)\n",
        "\n",
        "def suggest_heads_for_dim(trial, d_model):\n",
        "\n",
        "    candidates = [2, 4, 8]\n",
        "    opts = [h for h in candidates if d_model % h == 0 and h <= d_model]\n",
        "    if not opts:\n",
        "        opts = [1]\n",
        "    return trial.suggest_categorical(\"n_heads\", opts)\n",
        "\n",
        "SEARCH_KEYS = [\n",
        "    \"d_model\",\"n_heads\",\"n_layers\",\"lstm_hidden\",\"cnn_channels\",\"kernel_size\",\n",
        "    \"dropout\",\"lr\",\"weight_decay\",\"batch_size\"\n",
        "]\n",
        "\n",
        "def suggest_dl_params(trial):\n",
        "    hp = {}\n",
        "    d_model = trial.suggest_categorical(\"d_model\", [32, 48, 64])\n",
        "    hp[\"d_model\"] = d_model\n",
        "    hp[\"n_heads\"] = suggest_heads_for_dim(trial, d_model)\n",
        "    hp[\"n_layers\"] = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "    hp[\"lstm_hidden\"] = trial.suggest_categorical(\"lstm_hidden\", [32, 64, 96, 128])\n",
        "    hp[\"cnn_channels\"] = trial.suggest_categorical(\"cnn_channels\", [32, 64, 96, 128])\n",
        "    hp[\"kernel_size\"] = trial.suggest_categorical(\"kernel_size\", [3, 5])\n",
        "    hp[\"dropout\"] = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
        "    hp[\"lr\"] = trial.suggest_float(\"lr\", 3e-4, 3e-3, log=True)\n",
        "    hp[\"weight_decay\"] = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
        "    hp[\"batch_size\"] = trial.suggest_categorical(\"batch_size\", [128, 256, 512])\n",
        "    hp[\"epochs\"] = seed_params.get(\"epochs\", 100)\n",
        "    hp[\"patience\"] = seed_params.get(\"patience\", 12)\n",
        "    hp[\"random_state\"] = RANDOM_STATE\n",
        "    hp[\"_did_smote\"] = did_smote_flag\n",
        "    return hp\n",
        "\n",
        "def objective(trial):\n",
        "    hp = suggest_dl_params(trial)\n",
        "    mdl = make_model(n_features=X_train_final.shape[1], hp=hp)\n",
        "    mdl, best_ep, best_ap = train_one(mdl, X_train_final, y_train_final, X_val_fit, y_val, hp, verbose=False)\n",
        "    proba_val_t = predict_proba(mdl, X_val_fit)\n",
        "    ap = average_precision_score(y_val, proba_val_t)\n",
        "    trial.set_user_attr(\"best_epoch\", best_ep)\n",
        "    return ap\n",
        "\n",
        "# Warm-start con BEST\n",
        "if BEST_HP_FILE.exists():\n",
        "    try:\n",
        "        prev = json.loads(BEST_HP_FILE.read_text())\n",
        "        warm = {k: prev[k] for k in SEARCH_KEYS if k in prev}\n",
        "        if warm:\n",
        "            print(\"[OPTUNA] Enqueuing previous BEST as a trial seed.\")\n",
        "            study.enqueue_trial(warm)\n",
        "    except Exception as e:\n",
        "        print(\"[OPTUNA] Aviso: no se pudo usar BEST para warm-start:\", e)\n",
        "\n",
        "print(f\"[OPTUNA] Iniciando estudio '{STUDY_NAME}' con {N_TRIALS} pruebas...\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
        "\n",
        "best = study.best_trial\n",
        "print(f\"[OPTUNA] Mejor AP(val): {best.value:.6f}\")\n",
        "print(f\"[OPTUNA] Params ganadores:\", best.params)\n",
        "print(f\"[OPTUNA] best_epoch (del trial):\", best.user_attrs.get(\"best_epoch\"))\n",
        "\n",
        "best_params = dict(best.params)\n",
        "best_params.update({\n",
        "    \"epochs\": seed_params.get(\"epochs\", 100),\n",
        "    \"patience\": seed_params.get(\"patience\", 12),\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "    \"_did_smote\": did_smote_flag\n",
        "})\n",
        "with open(BEST_HP_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
        "print(\"[OPTUNA] Guardado BEST en:\", BEST_HP_FILE.name)\n",
        "\n",
        "tuned_model = make_model(n_features=X_train_final.shape[1], hp=best_params)\n",
        "tuned_model, best_ep = train_one(tuned_model, X_train_final, y_train_final, X_val_fit, y_val, best_params, verbose=False)[:2]\n",
        "print(\"[OPTUNA] Reentreno final completado. best_epoch =\", best_ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec9",
      "metadata": {},
      "source": [
        "9 — Cross-Validation (OOF) para baseline y tuned (DL-R)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "sec9code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV-BASELINE] Guardados: cv_summary_DL_REDUCED_SMOTENC_BASELINE_CV5.csv | oof_DL_REDUCED_SMOTENC_BASELINE_CV5.parquet\n",
            "[CV-TUNED] Guardados: cv_summary_DL_REDUCED_SMOTENC_TUNED_CV5.csv | oof_DL_REDUCED_SMOTENC_TUNED_CV5.parquet\n"
          ]
        }
      ],
      "source": [
        "def run_oof_cv_dl(model_hp, X, y, k_folds=CV_FOLDS, seed=RANDOM_STATE, exp_suffix=\"BASELINE\"):\n",
        "    cv_tag = f\"{EXP_NAME}_{exp_suffix}_CV{k_folds}\"\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
        "    oof_proba = np.zeros_like(y, dtype=float)\n",
        "    fold_rows = []\n",
        "\n",
        "    for f, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        X_tr0, X_va0 = X[tr_idx], X[va_idx]\n",
        "        y_tr0, y_va0 = y[tr_idx], y[va_idx]\n",
        "\n",
        "        X_tr, y_tr = X_tr0, y_tr0\n",
        "        did_smote = False\n",
        "        if BALANCE_IN_CV and USE_BALANCED_TRAIN:\n",
        "            X_tr, y_tr = maybe_resample(X_tr0, y_tr0)\n",
        "            did_smote = True\n",
        "\n",
        "        hp = dict(model_hp)\n",
        "        hp[\"_did_smote\"] = did_smote\n",
        "        mdl = make_model(n_features=X.shape[1], hp=hp)\n",
        "        adapter, best_ep, _ = train_one(mdl, X_tr, y_tr, X_va0, y_va0, hp, verbose=False)\n",
        "        proba_va = predict_proba(adapter, X_va0)\n",
        "        oof_proba[va_idx] = proba_va\n",
        "\n",
        "        fold_tag = f\"preds_val_fold{f}_{cv_tag}.parquet\"\n",
        "        pd.DataFrame({\n",
        "            \"idx\": va_idx,\n",
        "            \"proba\": proba_va,\n",
        "            \"y_true\": y_va0\n",
        "        }).to_parquet(OUT_PREDS / fold_tag, index=False)\n",
        "\n",
        "        fold_rows.append({\n",
        "            \"fold\": f,\n",
        "            \"pr_auc\": average_precision_score(y_va0, proba_va),\n",
        "            \"roc_auc\": roc_auc_score(y_va0, proba_va),\n",
        "            \"best_iteration\": best_ep if best_ep is not None else np.nan\n",
        "        })\n",
        "\n",
        "    oof_pr = average_precision_score(y, oof_proba)\n",
        "    oof_roc = roc_auc_score(y, oof_proba)\n",
        "    thr_oof, _ = find_best_threshold(y, oof_proba, metric=\"f1\")\n",
        "    y_oof_pred = (oof_proba >= thr_oof).astype(int)\n",
        "    oof_f1  = f1_score(y, y_oof_pred, zero_division=0)\n",
        "    oof_rec = recall_score(y, y_oof_pred, zero_division=0)\n",
        "    oof_bal = balanced_accuracy_score(y, y_oof_pred)\n",
        "\n",
        "    cv_csv = OUT_RESULTS / f\"cv_summary_{cv_tag}.csv\"\n",
        "    folds_df = pd.DataFrame(fold_rows)\n",
        "    agg_row = pd.DataFrame([{\n",
        "        \"fold\": \"OOF\", \"pr_auc\": oof_pr, \"roc_auc\": oof_roc,\n",
        "        \"thr\": thr_oof, \"f1\": oof_f1, \"recall\": oof_rec, \"bal_acc\": oof_bal\n",
        "    }])\n",
        "    pd.concat([folds_df, agg_row], ignore_index=True).to_csv(cv_csv, index=False)\n",
        "\n",
        "    oof_path = OUT_PREDS / f\"oof_{cv_tag}.parquet\"\n",
        "    pd.DataFrame({\"oof_proba\": oof_proba, \"y_true\": y}).to_parquet(oof_path, index=False)\n",
        "    print(f\"[CV-{exp_suffix}] Guardados: {cv_csv.name} | {oof_path.name}\")\n",
        "    return {\n",
        "        \"oof_pr_auc\": oof_pr,\n",
        "        \"oof_roc_auc\": oof_roc,\n",
        "        \"thr\": thr_oof,\n",
        "        \"oof_f1\": oof_f1,\n",
        "        \"oof_recall\": oof_rec,\n",
        "        \"oof_bal_acc\": oof_bal\n",
        "    }\n",
        "\n",
        "cv_baseline = None\n",
        "cv_tuned = None\n",
        "\n",
        "if DO_CV_BASELINE:\n",
        "    cv_baseline = run_oof_cv_dl(base_hp, X_train_fit, y_train, exp_suffix=\"BASELINE\")\n",
        "\n",
        "if DO_CV_TUNED and \"d_model\" in (best_params or {}):\n",
        "    cv_tuned = run_oof_cv_dl(best_params, X_train_fit, y_train, exp_suffix=\"TUNED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec10",
      "metadata": {},
      "source": [
        "10 — Evaluación en test + guardados (curvas, importancias proxy, preds, baselines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "sec10code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK][BASE] Guardados:\n",
            "  - Seed HPs   : DL_REDUCED_SMOTENC_BASE_seed_params.json \n",
            "  - Fitted HPs : DL_REDUCED_SMOTENC_BASE_fitted_params.json \n",
            "  - Importancias: DL_REDUCED_SMOTENC_feature_importances.csv \n",
            "  - Preds test  : preds_test_DL_REDUCED_SMOTENC.parquet \n",
            "  - Baselines   : baselines.csv\n",
            "[OK][TUNED] Guardados:\n",
            "  - Fitted HPs : DL_REDUCED_SMOTENC_TUNED_fitted_params.json \n",
            "  - Importancias: DL_REDUCED_SMOTENC_TUNED_feature_importances.csv \n",
            "  - Preds test  : preds_test_DL_REDUCED_SMOTENC_TUNED.parquet \n",
            "  - Baselines   : baselines.csv\n"
          ]
        }
      ],
      "source": [
        "base = EXP_NAME\n",
        "\n",
        "# BASELINE\n",
        "proba_test = predict_proba(model, X_test_fit)\n",
        "y_pred_test = (proba_test >= thr_val).astype(int)\n",
        "test_metrics = compute_all_metrics(y_test, proba_test, thr_val)\n",
        "\n",
        "# Guardar HP baseline\n",
        "params_seed_path = OUT_PARAMS / f\"{base}_BASE_seed_params.json\"\n",
        "with open(params_seed_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(base_hp, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "params_fitted_path = OUT_PARAMS / f\"{base}_BASE_fitted_params.json\"\n",
        "with open(params_fitted_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(base_hp, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Figuras baseline\n",
        "plot_pr_curve(y_val,  proba_val,  f\"{base} — PR (val)\",  OUT_FIGS / f\"{base}_pr_val.png\")\n",
        "plot_pr_curve(y_test, proba_test, f\"{base} — PR (test)\", OUT_FIGS / f\"{base}_pr_test.png\")\n",
        "plot_roc_curve(y_val,  proba_val,  f\"{base} — ROC (val)\",  OUT_FIGS / f\"{base}_roc_val.png\")\n",
        "plot_roc_curve(y_test, proba_test, f\"{base} — ROC (test)\", OUT_FIGS / f\"{base}_roc_test.png\")\n",
        "plot_confusion(y_test, y_pred_test, f\"{base} — Confusion (test @thr={thr_val:.3f})\", OUT_FIGS / f\"{base}_cm_test.png\")\n",
        "\n",
        "# Importancias proxy\n",
        "try:\n",
        "    imp_df = model.feature_importance(feature_names_used)\n",
        "except Exception:\n",
        "    imp_df = pd.DataFrame({\"feature\": feature_names_used, \"importance_proxy\": np.zeros(len(feature_names_used))})\n",
        "imp_path = OUT_RESULTS / f\"{base}_feature_importances.csv\"\n",
        "imp_df.to_csv(imp_path, index=False)\n",
        "\n",
        "# Preds test baseline\n",
        "preds_path = OUT_PREDS / f\"preds_test_{base}.parquet\"\n",
        "pd.DataFrame({\"proba\": proba_test, \"y_true\": y_test}).to_parquet(preds_path, index=False)\n",
        "\n",
        "row_base = {\n",
        "    \"model\": base,\n",
        "    \"thr_val\": thr_val,\n",
        "    \"val_pr_auc\": val_metrics[\"pr_auc\"],\n",
        "    \"val_roc_auc\": val_metrics[\"roc_auc\"],\n",
        "    \"val_precision\": val_metrics[\"precision\"],\n",
        "    \"val_f1\": val_metrics[\"f1\"],\n",
        "    \"val_recall\": val_metrics[\"recall\"],\n",
        "    \"val_bal_acc\": val_metrics[\"bal_acc\"],\n",
        "    \"test_pr_auc\": test_metrics[\"pr_auc\"],\n",
        "    \"test_roc_auc\": test_metrics[\"roc_auc\"],\n",
        "    \"test_precision\": test_metrics[\"precision\"],\n",
        "    \"test_f1\": test_metrics[\"f1\"],\n",
        "    \"test_recall\": test_metrics[\"recall\"],\n",
        "    \"test_bal_acc\": test_metrics[\"bal_acc\"],\n",
        "    \"best_iteration\": base_best_it if base_best_it is not None else np.nan\n",
        "}\n",
        "res_csv = OUT_RESULTS / \"baselines.csv\"\n",
        "pd.DataFrame([row_base]).to_csv(res_csv, mode=(\"a\" if res_csv.exists() else \"w\"), index=False, header=not res_csv.exists())\n",
        "\n",
        "print(\"[OK][BASE] Guardados:\\n  - Seed HPs   :\", params_seed_path.name,\n",
        "      \"\\n  - Fitted HPs :\", params_fitted_path.name,\n",
        "      \"\\n  - Importancias:\", imp_path.name,\n",
        "      \"\\n  - Preds test  :\", preds_path.name,\n",
        "      \"\\n  - Baselines   :\", res_csv.name)\n",
        "\n",
        "# TUNED\n",
        "if tuned_model is not None and best_params is not None:\n",
        "    proba_val_tuned = predict_proba(tuned_model, X_val_fit)\n",
        "    thr_val_tuned, _ = find_best_threshold(y_val, proba_val_tuned, metric=\"f1\")\n",
        "    # Guardar predicciones de validación (tuned)\n",
        "    val_tuned_path = OUT_PREDS / f\"preds_val_{base}_TUNED.parquet\"\n",
        "    pd.DataFrame({\n",
        "        \"proba\": proba_val_tuned,\n",
        "        \"y_true\": y_val,\n",
        "        \"y_pred\": (proba_val_tuned >= thr_val_tuned).astype(int)\n",
        "    }).to_parquet(val_tuned_path, index=False)\n",
        "    val_metrics_tuned = compute_all_metrics(y_val, proba_val_tuned, thr_val_tuned)\n",
        "\n",
        "    proba_test_tuned = predict_proba(tuned_model, X_test_fit)\n",
        "    y_pred_test_tuned = (proba_test_tuned >= thr_val_tuned).astype(int)\n",
        "    test_metrics_tuned = compute_all_metrics(y_test, proba_test_tuned, thr_val_tuned)\n",
        "\n",
        "    tuned_fitted_path = OUT_PARAMS / f\"{base}_TUNED_fitted_params.json\"\n",
        "    with open(tuned_fitted_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    base_t = base + \"_TUNED\"\n",
        "    plot_pr_curve(y_val,  proba_val_tuned,  f\"{base_t} — PR (val)\",  OUT_FIGS / f\"{base_t}_pr_val.png\")\n",
        "    plot_pr_curve(y_test, proba_test_tuned, f\"{base_t} — PR (test)\", OUT_FIGS / f\"{base_t}_pr_test.png\")\n",
        "    plot_roc_curve(y_val,  proba_val_tuned,  f\"{base_t} — ROC (val)\",  OUT_FIGS / f\"{base_t}_roc_val.png\")\n",
        "    plot_roc_curve(y_test, proba_test_tuned, f\"{base_t} — ROC (test)\", OUT_FIGS / f\"{base_t}_roc_test.png\")\n",
        "    plot_confusion(y_test, y_pred_test_tuned, f\"{base_t} — Confusion (test @thr={thr_val_tuned:.3f})\", OUT_FIGS / f\"{base_t}_cm_test.png\")\n",
        "\n",
        "    try:\n",
        "        imp_t_df = tuned_model.feature_importance(feature_names_used)\n",
        "    except Exception:\n",
        "        imp_t_df = pd.DataFrame({\"feature\": feature_names_used, \"importance_proxy\": np.zeros(len(feature_names_used))})\n",
        "    imp_t_path = OUT_RESULTS / f\"{base_t}_feature_importances.csv\"\n",
        "    imp_t_df.to_csv(imp_t_path, index=False)\n",
        "\n",
        "    preds_t_path = OUT_PREDS / f\"preds_test_{base_t}.parquet\"\n",
        "    pd.DataFrame({\"proba\": proba_test_tuned, \"y_true\": y_test}).to_parquet(preds_t_path, index=False)\n",
        "\n",
        "    row_t = {\n",
        "        \"model\": base_t,\n",
        "        \"thr_val\": thr_val_tuned,\n",
        "        \"val_pr_auc\": val_metrics_tuned[\"pr_auc\"],\n",
        "        \"val_roc_auc\": val_metrics_tuned[\"roc_auc\"],\n",
        "        \"val_precision\": val_metrics_tuned[\"precision\"],\n",
        "        \"val_f1\": val_metrics_tuned[\"f1\"],\n",
        "        \"val_recall\": val_metrics_tuned[\"recall\"],\n",
        "        \"val_bal_acc\": val_metrics_tuned[\"bal_acc\"],\n",
        "        \"test_pr_auc\": test_metrics_tuned[\"pr_auc\"],\n",
        "        \"test_roc_auc\": test_metrics_tuned[\"roc_auc\"],\n",
        "        \"test_precision\": test_metrics_tuned[\"precision\"],\n",
        "        \"test_f1\": test_metrics_tuned[\"f1\"],\n",
        "        \"test_recall\": test_metrics_tuned[\"recall\"],\n",
        "        \"test_bal_acc\": test_metrics_tuned[\"bal_acc\"],\n",
        "        \"best_iteration\": best_ep if best_ep is not None else np.nan\n",
        "    }\n",
        "    pd.DataFrame([row_t]).to_csv(res_csv, mode=\"a\", index=False, header=False)\n",
        "\n",
        "    print(\"[OK][TUNED] Guardados:\\n  - Fitted HPs :\", tuned_fitted_path.name,\n",
        "          \"\\n  - Importancias:\", imp_t_path.name,\n",
        "          \"\\n  - Preds test  :\", preds_t_path.name,\n",
        "          \"\\n  - Baselines   :\", res_csv.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec11",
      "metadata": {},
      "source": [
        "11 — Mejores resultados + resumen CV (formato similar a tu XGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "sec11code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MEJORES EN TEST (por métrica) ===\n",
            "- test_pr_auc: DL_REDUCED_SMOTENC_TUNED | PR-AUC=0.7060 | ROC-AUC=0.8645 | F1=0.6157 | Recall=0.6830 | Precision=0.5605 | thr(val)=0.569 | best_iter=21\n",
            "- test_roc_auc: DL_REDUCED_SMOTENC_TUNED | PR-AUC=0.7060 | ROC-AUC=0.8645 | F1=0.6157 | Recall=0.6830 | Precision=0.5605 | thr(val)=0.569 | best_iter=21\n",
            "- test_recall: DL_REDUCED_SMOTENC_TUNED | PR-AUC=0.7060 | ROC-AUC=0.8645 | F1=0.6157 | Recall=0.6830 | Precision=0.5605 | thr(val)=0.569 | best_iter=21\n",
            "- test_f1: DL_REDUCED_SMOTENC | PR-AUC=0.6979 | ROC-AUC=0.8587 | F1=0.6198 | Recall=0.6167 | Precision=0.6228 | thr(val)=0.783 | best_iter=18\n",
            "- test_precision: DL_REDUCED_SMOTENC | PR-AUC=0.6979 | ROC-AUC=0.8587 | F1=0.6198 | Recall=0.6167 | Precision=0.6228 | thr(val)=0.783 | best_iter=18\n",
            "=== RESUMEN CV-OOF (por experimento) ===\n",
            "                            tag   pr_auc  roc_auc       f1   recall  bal_acc   thr\n",
            "DL_REDUCED_SMOTENC_BASELINE_CV5 0.677179 0.846174 0.610200 0.562551 0.745270 0.736\n",
            "   DL_REDUCED_SMOTENC_TUNED_CV5 0.676297 0.849335 0.607726 0.623876 0.756987 0.675\n",
            "[OK] Normalizado. Backup: baselines_legacy_backup.csv\n"
          ]
        }
      ],
      "source": [
        "AGGREGATE_ALL_RUNS = False\n",
        "\n",
        "def safe(v, fmt=\".4f\"):\n",
        "    try:\n",
        "        return f\"{float(v):{fmt}}\"\n",
        "    except Exception:\n",
        "        return \"NA\"\n",
        "\n",
        "base_csv = OUT_RESULTS / \"baselines.csv\"\n",
        "if not base_csv.exists():\n",
        "    raise FileNotFoundError(f\"No existe {base_csv}\")\n",
        "\n",
        "df = pd.read_csv(base_csv)\n",
        "\n",
        "needed = [\n",
        "    \"model\",\"thr_val\",\n",
        "    \"val_pr_auc\",\"val_roc_auc\",\"val_precision\",\"val_f1\",\"val_recall\",\"val_bal_acc\",\n",
        "    \"test_pr_auc\",\"test_roc_auc\",\"test_precision\",\"test_f1\",\"test_recall\",\"test_bal_acc\",\n",
        "    \"best_iteration\"\n",
        "]\n",
        "for c in needed:\n",
        "    if c not in df.columns:\n",
        "        df[c] = pd.NA \n",
        "\n",
        "df = df[needed].copy()\n",
        "\n",
        "num_cols = [c for c in needed if c not in (\"model\",)]\n",
        "for c in num_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "if AGGREGATE_ALL_RUNS:\n",
        "    root_art = ARTIF_DIR.parent\n",
        "    for p in (root_art).glob(\"DL_*/results/baselines.csv\"):\n",
        "        if p == base_csv:\n",
        "            continue\n",
        "        try:\n",
        "            d2 = pd.read_csv(p)\n",
        "            for c in needed:\n",
        "                if c not in d2.columns:\n",
        "                    d2[c] = pd.NA\n",
        "            d2 = d2[needed]\n",
        "            for c in num_cols:\n",
        "                d2[c] = pd.to_numeric(d2[c], errors=\"coerce\")\n",
        "            df = pd.concat([df, d2], ignore_index=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"El dataframe de resultados está vacío.\")\n",
        "\n",
        "df = df.drop_duplicates(subset=[\"model\"], keep=\"last\").copy()\n",
        "\n",
        "def best_by(metric):\n",
        "    if metric not in df.columns or df[metric].dropna().empty:\n",
        "        return None\n",
        "    r = df.loc[df[metric].idxmax()]\n",
        "    print(\n",
        "        f\"- {metric}: {r['model']} | \"\n",
        "        f\"PR-AUC={safe(r['test_pr_auc'])} | \"\n",
        "        f\"ROC-AUC={safe(r['test_roc_auc'])} | \"\n",
        "        f\"F1={safe(r['test_f1'])} | \"\n",
        "        f\"Recall={safe(r['test_recall'])} | \"\n",
        "        f\"Precision={safe(r['test_precision'])} | \"\n",
        "        f\"thr(val)={safe(r['thr_val'], '.3f')} | \"\n",
        "        f\"best_iter={int(r['best_iteration']) if pd.notna(r['best_iteration']) else 'NA'}\"\n",
        "    )\n",
        "    return r\n",
        "\n",
        "print(\"=== MEJORES EN TEST (por métrica) ===\")\n",
        "winners = {}\n",
        "for m in [\"test_pr_auc\",\"test_roc_auc\",\"test_recall\",\"test_f1\",\"test_precision\"]:\n",
        "    w = best_by(m)\n",
        "    if w is not None:\n",
        "        winners[m] = w\n",
        "\n",
        "cv_files = list(OUT_RESULTS.glob(\"cv_summary_*_CV*.csv\"))\n",
        "if cv_files:\n",
        "    print(\"=== RESUMEN CV-OOF (por experimento) ===\")\n",
        "    rows = []\n",
        "    for f in cv_files:\n",
        "        tag = re.sub(r\"^cv_summary_|\\.csv$\", \"\", f.name)\n",
        "        cv = pd.read_csv(f)\n",
        "        oof = cv.loc[cv[\"fold\"] == \"OOF\"]\n",
        "        if not oof.empty:\n",
        "            r = oof.iloc[0]\n",
        "            rows.append({\n",
        "                \"tag\": tag,\n",
        "                \"pr_auc\": r.get(\"pr_auc\"),\n",
        "                \"roc_auc\": r.get(\"roc_auc\"),\n",
        "                \"f1\": r.get(\"f1\"),\n",
        "                \"recall\": r.get(\"recall\"),\n",
        "                \"bal_acc\": r.get(\"bal_acc\"),\n",
        "                \"thr\": r.get(\"thr\"),\n",
        "            })\n",
        "    if rows:\n",
        "        print(pd.DataFrame(rows).sort_values([\"pr_auc\",\"roc_auc\"], ascending=False).to_string(index=False))\n",
        "else:\n",
        "    print(\"(No se hallaron archivos de CV para este experimento)\")\n",
        "\n",
        "# Normalización/backup\n",
        "backup = OUT_RESULTS / \"baselines_legacy_backup.csv\"\n",
        "base_csv.replace(backup)\n",
        "df.to_csv(base_csv, index=False)\n",
        "print(\"[OK] Normalizado. Backup:\", backup.name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
