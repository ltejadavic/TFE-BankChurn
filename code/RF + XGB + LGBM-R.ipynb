{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d61c7a4",
   "metadata": {},
   "source": [
    "1 — Imports, config y rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11fd130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT:      /Users/luistejada/Downloads/TFE Churn Bancario\n",
      "DATA_DIR:  /Users/luistejada/Downloads/TFE Churn Bancario/preproc_datasets/full\n",
      "ARTIF_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/artifacts\n",
      "{'xgb': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_REDUCED_SMOTENC', 'lgbm': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_REDUCED_SMOTENC', 'rf': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/RF_REDUCED_SMOTENC', 'ens': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_REDUCED_SMOTENC'}\n"
     ]
    }
   ],
   "source": [
    "import json, os, warnings, time, re\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Métricas / CV / Modelos\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, precision_recall_curve, roc_auc_score, roc_curve,\n",
    "    f1_score, recall_score, balanced_accuracy_score, confusion_matrix, precision_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Balanceo\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTENC, SMOTE\n",
    "    _HAS_IMBLEARN = True\n",
    "except Exception:\n",
    "    _HAS_IMBLEARN = False\n",
    "\n",
    "# Modelos base\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Optuna\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "USE_REDUCED = True\n",
    "USE_BALANCED_TRAIN = True\n",
    "BALANCE_IN_CV = True\n",
    "RANDOM_STATE = 42\n",
    "DO_TUNE_XGB = True\n",
    "DO_TUNE_LGBM = True\n",
    "DO_CV_BASELINE = True\n",
    "DO_CV_TUNED = True\n",
    "CV_FOLDS = 5\n",
    "MI_TOPK = 30\n",
    "\n",
    "# --- Localizador ---\n",
    "def _auto_project_root():\n",
    "    env_root = os.environ.get(\"PROJECT_ROOT\")\n",
    "    if env_root and (Path(env_root)/\"preproc_datasets\"/\"full\").exists():\n",
    "        return Path(env_root)\n",
    "    \n",
    "    here = Path.cwd().resolve()\n",
    "    candidates = [here, here.parent, here.parent.parent, here.parent.parent.parent]\n",
    "    for base in candidates:\n",
    "        if (base/\"preproc_datasets\"/\"full\"/\"X_train_full.npy\").exists():\n",
    "            return base\n",
    "        if (base/\"preproc_datasets\"/\"full\").exists():\n",
    "            return base\n",
    "\n",
    "    home_fallback = Path.home() / \"Downloads\" / \"TFE Churn Bancario\"\n",
    "    if (home_fallback/\"preproc_datasets\"/\"full\").exists():\n",
    "        return home_fallback\n",
    "\n",
    "    return here\n",
    "\n",
    "ROOT = _auto_project_root()\n",
    "\n",
    "# === Rutas ===\n",
    "DATA_DIR = ROOT / \"preproc_datasets\" / \"full\"\n",
    "ARTIF_ROOT = ROOT / \"artifacts\"\n",
    "\n",
    "# Tags\n",
    "VIEW_TAG = \"REDUCED\" if USE_REDUCED else \"FULL\"\n",
    "BAL_TAG  = \"SMOTENC\" if USE_BALANCED_TRAIN else \"IMB\"\n",
    "\n",
    "# Carpetas por modelo y ensamble\n",
    "DIRS = {\n",
    "    \"xgb\":   ARTIF_ROOT / f\"XGB_{VIEW_TAG}_{BAL_TAG}\",\n",
    "    \"lgbm\":  ARTIF_ROOT / f\"LGBM_{VIEW_TAG}_{BAL_TAG}\",\n",
    "    \"rf\":    ARTIF_ROOT / f\"RF_{VIEW_TAG}_{BAL_TAG}\",\n",
    "    \"ens\":   ARTIF_ROOT / f\"ENS_{VIEW_TAG}_{BAL_TAG}\",\n",
    "}\n",
    "for k, base in DIRS.items():\n",
    "    for sub in [\"results\", \"figs\", \"preds\", \"best_params\", \"export\"]:\n",
    "        (base / sub).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:     \", ROOT)\n",
    "print(\"DATA_DIR: \", DATA_DIR)\n",
    "print(\"ARTIF_DIR:\", ARTIF_ROOT)\n",
    "print({k: str(v) for k, v in DIRS.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7b3d6",
   "metadata": {},
   "source": [
    "2 — Carga de datos y metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b45c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (6000, 15) (2000, 15) (2000, 15)\n",
      "y: (6000,) (2000,) (2000,)\n",
      "n features: 15\n",
      "[META] Archivo detectado: N/D | columnas categóricas=0\n",
      "[META] No hay categóricas => caerá en SMOTE estándar.\n",
      "[L1] Máscara L1 cargada con 12 features.\n",
      "[L1] Shapes reducidos: train (6000, 12) | val (2000, 12) | test (2000, 12)\n",
      "[L1] Ejemplo de features usados: ['num__CreditScore', 'num__Age', 'num__Tenure', 'num__Balance', 'num__EstimatedSalary', 'Geography_1', 'Gender_1', 'HasCrCard_1', 'IsActiveMember_1', 'NumOfProducts_1', 'NumOfProducts_2', 'NumOfProducts_3']\n"
     ]
    }
   ],
   "source": [
    "def load_xy_full(dir_full: Path):\n",
    "    expected = [\n",
    "        dir_full / \"X_train_full.npy\",\n",
    "        dir_full / \"X_val_full.npy\",\n",
    "        dir_full / \"X_test_full.npy\",\n",
    "        dir_full / \"y_train.parquet\",\n",
    "        dir_full / \"y_val.parquet\",\n",
    "        dir_full / \"y_test.parquet\",\n",
    "        dir_full / \"feature_names_full.parquet\",\n",
    "    ]\n",
    "    missing = [p.name for p in expected if not p.exists()]\n",
    "    if missing:\n",
    "        listing = [p.name for p in dir_full.glob(\"*\")]\n",
    "        raise FileNotFoundError(\n",
    "            f\"No se encuentran estos archivos en {dir_full} -> {missing}\\n\"\n",
    "            f\"Contenido detectado: {listing}\\n\"\n",
    "            f\"Sugerencia: verifica que ROOT (impreso arriba) apunte a la carpeta del proyecto.\"\n",
    "        )\n",
    "\n",
    "    X_train = np.load(dir_full / \"X_train_full.npy\")\n",
    "    X_val   = np.load(dir_full / \"X_val_full.npy\")\n",
    "    X_test  = np.load(dir_full / \"X_test_full.npy\")\n",
    "\n",
    "    y_train = pd.read_parquet(dir_full / \"y_train.parquet\")[\"Exited\"].to_numpy()\n",
    "    y_val   = pd.read_parquet(dir_full / \"y_val.parquet\")[\"Exited\"].to_numpy()\n",
    "    y_test  = pd.read_parquet(dir_full / \"y_test.parquet\")[\"Exited\"].to_numpy()\n",
    "\n",
    "    feature_names = pd.read_parquet(dir_full / \"feature_names_full.parquet\")[\"feature\"].tolist()\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, feature_names\n",
    "\n",
    "# Cargar datos\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_xy_full(DATA_DIR)\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"y:\", y_train.shape, y_val.shape, y_test.shape)\n",
    "print(\"n features:\", len(feature_names))\n",
    "\n",
    "# --- Metadatos para SMOTENC ---\n",
    "def _read_feature_roles(dir_full: Path):\n",
    "    candidates = [\n",
    "        dir_full / \"feature_roles_full.parquet\",\n",
    "        dir_full / \"feature_roles.parquet\",\n",
    "        dir_full / \"feature_meta_full.parquet\",\n",
    "        dir_full / \"feature_meta.parquet\",\n",
    "        dir_full / \"feature_types_full.parquet\",\n",
    "        dir_full / \"feature_types.parquet\",\n",
    "        dir_full / \"feature_meta.json\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            if p.suffix == \".parquet\":\n",
    "                df = pd.read_parquet(p)\n",
    "            elif p.suffix == \".json\":\n",
    "                obj = json.loads(p.read_text())\n",
    "                if isinstance(obj, dict) and \"features\" in obj:\n",
    "                    df = pd.DataFrame(obj[\"features\"])\n",
    "                else:\n",
    "                    df = pd.DataFrame(obj)\n",
    "            else:\n",
    "                continue\n",
    "            return df, p.name\n",
    "    return None, None\n",
    "\n",
    "def _build_cat_idx(feature_names, roles_df):\n",
    "    if roles_df is None or len(roles_df) == 0:\n",
    "        return []\n",
    "    df = roles_df.copy()\n",
    "    df.columns = [str(c).lower() for c in df.columns]\n",
    "    if \"feature\" not in df.columns:\n",
    "        if \"name\" in df.columns:\n",
    "            df[\"feature\"] = df[\"name\"]\n",
    "        else:\n",
    "            return []\n",
    "    cat_names = set()\n",
    "    if \"role\" in df.columns:\n",
    "        cat_names = set(df.loc[df[\"role\"].astype(str).str.lower().isin(\n",
    "            [\"cat\",\"categorical\",\"bin\",\"binary\",\"ordinal\"]), \"feature\"])\n",
    "    elif \"dtype\" in df.columns:\n",
    "        cat_names = set(df.loc[df[\"dtype\"].astype(str).str.lower().isin(\n",
    "            [\"category\",\"categorical\",\"object\",\"bool\"]), \"feature\"])\n",
    "    elif \"is_cat\" in df.columns:\n",
    "        cat_names = set(df.loc[df[\"is_cat\"].astype(bool), \"feature\"])\n",
    "    else:\n",
    "        return []\n",
    "    idx = [i for i, f in enumerate(feature_names) if f in cat_names]\n",
    "    return sorted(idx)\n",
    "\n",
    "roles_df, meta_file = _read_feature_roles(DATA_DIR)\n",
    "CAT_IDX_FULL = _build_cat_idx(feature_names, roles_df)\n",
    "print(f\"[META] Archivo detectado: {meta_file or 'N/D'} | columnas categóricas={len(CAT_IDX_FULL)}\")\n",
    "if CAT_IDX_FULL:\n",
    "    print(\"[META] Ejemplo de índices categóricos:\", CAT_IDX_FULL[:10], \"...\")\n",
    "else:\n",
    "    print(\"[META] No hay categóricas => caerá en SMOTE estándar.\")\n",
    "\n",
    "# === L1: cargar máscara desde XGB_REDUCED_SMOTENC ===\n",
    "XGB_L1_FS_DIR = ARTIF_ROOT / \"XGB_REDUCED_SMOTENC\" / \"best_params\" / \"feature_selection\"\n",
    "keep_idx_path = XGB_L1_FS_DIR / \"keep_idx_L1.npy\"\n",
    "\n",
    "if not keep_idx_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"[L1] No se encontró keep_idx_L1.npy en {keep_idx_path}\\n\"\n",
    "        f\"Asegúrate de haber corrido antes el experimento XGB_REDUCED_SMOTENC con selección L1.\"\n",
    "    )\n",
    "\n",
    "# Índices de columnas a conservar según L1 (en el espacio FULL)\n",
    "keep_idx_global = np.load(keep_idx_path).astype(int).tolist()\n",
    "print(f\"[L1] Máscara L1 cargada con {len(keep_idx_global)} features.\")\n",
    "\n",
    "# Helper para aplicar máscara\n",
    "def apply_keep_idx(X, keep_idx):\n",
    "    return X[:, keep_idx]\n",
    "\n",
    "# Aplicar la máscara a los datasets FULL\n",
    "X_train_fit = apply_keep_idx(X_train, keep_idx_global)\n",
    "X_val_fit   = apply_keep_idx(X_val,   keep_idx_global)\n",
    "X_test_fit  = apply_keep_idx(X_test,  keep_idx_global)\n",
    "\n",
    "feature_names_used = [feature_names[i] for i in keep_idx_global]\n",
    "\n",
    "print(\"[L1] Shapes reducidos:\",\n",
    "      \"train\", X_train_fit.shape,\n",
    "      \"| val\", X_val_fit.shape,\n",
    "      \"| test\", X_test_fit.shape)\n",
    "print(\"[L1] Ejemplo de features usados:\", feature_names_used[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b76afe4",
   "metadata": {},
   "source": [
    "3 — Métricas, umbral, y plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789d35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "def pr_auc(y_true, y_proba): \n",
    "    return float(average_precision_score(y_true, y_proba))\n",
    "\n",
    "def roc_auc(y_true, y_proba): \n",
    "    return float(roc_auc_score(y_true, y_proba))\n",
    "\n",
    "def find_best_threshold(y_true, y_proba, metric=\"f1\"):\n",
    "    thr_grid = np.linspace(0.0, 1.0, 1001)\n",
    "    best_thr, best_score = 0.5, -1.0\n",
    "    for thr in thr_grid:\n",
    "        y_pred = (y_proba >= thr).astype(int)\n",
    "        if metric == \"f1\":\n",
    "            score = f1_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == \"recall\":\n",
    "            score = recall_score(y_true, y_pred, zero_division=0)\n",
    "        else:\n",
    "            raise ValueError(\"metric no soportada\")\n",
    "        if score > best_score:\n",
    "            best_score, best_thr = score, thr\n",
    "    return float(best_thr), float(best_score)\n",
    "\n",
    "def compute_all_metrics(y_true, y_proba, thr):\n",
    "    y_pred = (y_proba >= thr).astype(int)\n",
    "    return {\n",
    "        \"pr_auc\": pr_auc(y_true, y_proba),\n",
    "        \"roc_auc\": roc_auc(y_true, y_proba),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"bal_acc\": balanced_accuracy_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def plot_pr_curve(y_true, y_proba, title, out_path):\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n",
    "    ap = average_precision_score(y_true, y_proba)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.step(rec, prec, where='post')\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.title(f'{title} (AP={ap:.4f})')\n",
    "    plt.grid(True, linestyle='--', alpha=.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "def plot_roc_curve(y_true, y_proba, title, out_path):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, lw=2)\n",
    "    plt.plot([0,1],[0,1], 'k--', lw=1)\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title} (AUC={auc:.4f})')\n",
    "    plt.grid(True, linestyle='--', alpha=.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "def plot_confusion(y_true, y_pred, title, out_path, normalize=False):\n",
    "    norm = 'true' if normalize else None\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=norm)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(2)\n",
    "    plt.xticks(ticks, ['0','1']); plt.yticks(ticks, ['0','1'])\n",
    "    thresh = cm.max()/2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            txt = f'{cm[i,j]:.2f}' if normalize else str(cm[i,j])\n",
    "            plt.text(j, i, txt, ha='center', va='center',\n",
    "                     color='white' if cm[i,j] > thresh else 'black')\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5651c79",
   "metadata": {},
   "source": [
    "4 — Helpers de balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be6c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _map_cat_idx_for_keep(keep_idx, cat_idx_full):\n",
    "    if not cat_idx_full:\n",
    "        return []\n",
    "    if keep_idx is None:\n",
    "        return sorted(cat_idx_full)\n",
    "    pos = {old_i: j for j, old_i in enumerate(keep_idx)}\n",
    "    return sorted([pos[i] for i in cat_idx_full if i in pos])\n",
    "\n",
    "def apply_keep_idx(X, keep_idx):\n",
    "    return X[:, keep_idx]\n",
    "\n",
    "def maybe_smote(X, y, keep_idx=None, random_state=RANDOM_STATE, k_neighbors=5):\n",
    "    if not _HAS_IMBLEARN:\n",
    "        print(\"[BAL] imbalanced-learn no disponible. Se omite balanceo.\")\n",
    "        return X, y\n",
    "    y_int = y.astype(int)\n",
    "    if y_int.max() == 0:\n",
    "        print(\"[BAL] Solo 1 clase en y. Se omite balanceo.\")\n",
    "        return X, y\n",
    "    counts = np.bincount(y_int)\n",
    "    if len(counts) < 2 or counts.min() < 2:\n",
    "        print(\"[BAL] Minoría < 2 muestras. Se omite balanceo.\")\n",
    "        return X, y\n",
    "    k = int(max(1, min(5, counts.min() - 1)))\n",
    "    cat_idx = _map_cat_idx_for_keep(keep_idx, CAT_IDX_FULL)\n",
    "    if cat_idx:\n",
    "        sm = SMOTENC(categorical_features=cat_idx, k_neighbors=k, random_state=random_state)\n",
    "        kind = \"SMOTENC\"\n",
    "    else:\n",
    "        sm = SMOTE(k_neighbors=k, random_state=random_state)\n",
    "        kind = \"SMOTE\"\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    try:\n",
    "        X_res = X_res.astype(X.dtype, copy=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f\"[BAL] {kind} aplicado | k_neighbors={k} | cat_cols={len(cat_idx)}\")\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "def _json_dump(path, obj):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def _floatify_metrics(d):\n",
    "    return {k: float(v) for k, v in d.items()}\n",
    "\n",
    "def write_minimal_loader(out_dir: Path):\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    code = r'''# -*- coding: utf-8 -*-\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def load_xgb_from_manifest(manifest_path):\n",
    "    import xgboost as xgb\n",
    "    mp = Path(manifest_path)\n",
    "    m = json.loads(mp.read_text(encoding=\"utf-8\"))\n",
    "    mdl_file = mp.parent / m[\"files\"][\"model_json\"]\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(str(mdl_file))\n",
    "\n",
    "    best_it = m.get(\"training\", {}).get(\"best_iteration\", None)\n",
    "    feat_names = m[\"features\"][\"names\"]\n",
    "\n",
    "    def predict_proba(X):\n",
    "        d = xgb.DMatrix(X, feature_names=feat_names)\n",
    "        if best_it is not None and isinstance(best_it, int):\n",
    "            p = booster.predict(d, iteration_range=(0, best_it+1))\n",
    "        else:\n",
    "            p = booster.predict(d)\n",
    "        return np.column_stack([1.0 - p, p])\n",
    "\n",
    "    return m, predict_proba\n",
    "\n",
    "def load_lgbm_from_manifest(manifest_path):\n",
    "    import lightgbm as lgb\n",
    "    mp = Path(manifest_path)\n",
    "    m = json.loads(mp.read_text(encoding=\"utf-8\"))\n",
    "    mdl_file = mp.parent / m[\"files\"][\"model_txt\"]\n",
    "    booster = lgb.Booster(model_file=str(mdl_file))\n",
    "\n",
    "    feat_names = m[\"features\"][\"names\"]\n",
    "    best_it = m.get(\"training\", {}).get(\"best_iteration\", None)\n",
    "\n",
    "    def predict_proba(X):\n",
    "        p1 = booster.predict(X, num_iteration=best_it)\n",
    "        return np.column_stack([1.0 - p1, p1])\n",
    "\n",
    "    return m, predict_proba\n",
    "'''\n",
    "    (out_dir / \"loader_example.py\").write_text(code, encoding=\"utf-8\")\n",
    "\n",
    "def export_xgb(adapter, out_dir, exp_name, feature_names, threshold, val_metrics, test_metrics):\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    booster = adapter.get_booster()\n",
    "    json_path = out_dir / f\"{exp_name}.xgb.json\"\n",
    "    ubj_path  = out_dir / f\"{exp_name}.xgb.ubj\"\n",
    "\n",
    "    booster.save_model(str(json_path))\n",
    "\n",
    "    files = {\"model_json\": json_path.name}\n",
    "    try:\n",
    "        booster.save_model(str(ubj_path))\n",
    "        files[\"model_ubj\"] = ubj_path.name\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import joblib\n",
    "        joblib.dump(\n",
    "            {\"params\": adapter.get_params(), \"best_iteration\": adapter.best_iteration},\n",
    "            out_dir / f\"{exp_name}.sk_params.joblib\"\n",
    "        )\n",
    "        files[\"sk_params_joblib\"] = f\"{exp_name}.sk_params.joblib\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    manifest = {\n",
    "        \"export_schema\": \"v1\",\n",
    "        \"model_key\": exp_name,\n",
    "        \"kind\": \"xgboost_binary_classifier\",\n",
    "        \"framework\": \"xgboost\",\n",
    "        \"framework_version\": getattr(xgb, \"__version__\", \"unknown\"),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"files\": files,\n",
    "        \"features\": {\n",
    "            \"names\": list(map(str, feature_names)),\n",
    "            \"dtype\": \"float32\"\n",
    "        },\n",
    "        \"inference\": {\n",
    "            \"class_labels\": [0, 1],\n",
    "            \"threshold\": float(threshold)\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"best_iteration\": int(adapter.best_iteration) if adapter.best_iteration is not None else None\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"validation\": _floatify_metrics(val_metrics),\n",
    "            \"test\": _floatify_metrics(test_metrics)\n",
    "        }\n",
    "    }\n",
    "    _json_dump(out_dir / f\"{exp_name}_MANIFEST.json\", manifest)\n",
    "    write_minimal_loader(out_dir)\n",
    "    print(f\"[EXPORT][XGB] {exp_name} -> {out_dir}\")\n",
    "    return manifest\n",
    "\n",
    "def export_lgbm(adapter, out_dir, exp_name, feature_names, threshold, val_metrics, test_metrics):\n",
    "    \"\"\"\n",
    "    Exporta:\n",
    "      - Modelo LightGBM en TXT (formato de texto oficial).\n",
    "      - Manifiesto JSON con nombres de features, umbral y métricas.\n",
    "      - Loader de ejemplo.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    booster = adapter.booster_\n",
    "    txt_path = out_dir / f\"{exp_name}.lgbm.txt\"\n",
    "    booster.save_model(str(txt_path))\n",
    "\n",
    "    manifest = {\n",
    "        \"export_schema\": \"v1\",\n",
    "        \"model_key\": exp_name,\n",
    "        \"kind\": \"lightgbm_binary_classifier\",\n",
    "        \"framework\": \"lightgbm\",\n",
    "        \"framework_version\": getattr(lgb, \"__version__\", \"unknown\"),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"files\": {\n",
    "            \"model_txt\": txt_path.name\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"names\": list(map(str, feature_names)),\n",
    "            \"dtype\": \"float32\"\n",
    "        },\n",
    "        \"inference\": {\n",
    "            \"class_labels\": [0, 1],\n",
    "            \"threshold\": float(threshold)\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"best_iteration\": int(adapter.best_iteration) if getattr(adapter, \"best_iteration\", None) is not None else None\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"validation\": _floatify_metrics(val_metrics),\n",
    "            \"test\": _floatify_metrics(test_metrics)\n",
    "        }\n",
    "    }\n",
    "    _json_dump(out_dir / f\"{exp_name}_MANIFEST.json\", manifest)\n",
    "    write_minimal_loader(out_dir)\n",
    "    print(f\"[EXPORT][LGBM] {exp_name} -> {out_dir}\")\n",
    "    return manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95e23c",
   "metadata": {},
   "source": [
    "5 — Adaptadores de entrenamiento con Early-Stopping (XGB & LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828777f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost ---\n",
    "class XGBAdapter:\n",
    "    def __init__(self, booster, params, best_iteration, feature_names=None):\n",
    "        self._booster = booster\n",
    "        self._params = dict(params)\n",
    "        self.best_iteration = best_iteration\n",
    "        self._feature_names = feature_names\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        d = xgb.DMatrix(X, feature_names=self._feature_names)\n",
    "        if self.best_iteration is not None:\n",
    "            pred = self._booster.predict(d, iteration_range=(0, int(self.best_iteration)+1))\n",
    "        else:\n",
    "            pred = self._booster.predict(d)\n",
    "        return np.column_stack([1.0 - pred, pred])\n",
    "\n",
    "    def get_booster(self):\n",
    "        return self._booster\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict(self._params)\n",
    "\n",
    "\n",
    "def xgb_fit_with_es(sk_model, X_tr, y_tr, X_va, y_va, feature_names=None, rounds=200, verbose=False):\n",
    "    p = sk_model.get_params()\n",
    "    n_estimators = int(p.pop(\"n_estimators\", 1000))\n",
    "    seed = p.pop(\"random_state\", p.pop(\"seed\", RANDOM_STATE))\n",
    "    nthread = p.pop(\"n_jobs\", None)\n",
    "    if nthread is not None:\n",
    "        p[\"nthread\"] = nthread\n",
    "    p.setdefault(\"seed\", seed)\n",
    "    p.setdefault(\"objective\", \"binary:logistic\")\n",
    "    p.setdefault(\"eval_metric\", \"aucpr\")\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr, feature_names=feature_names)\n",
    "    dvalid = xgb.DMatrix(X_va, label=y_va, feature_names=feature_names)\n",
    "    evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "    booster = xgb.train(\n",
    "        params=p, dtrain=dtrain, num_boost_round=n_estimators,\n",
    "        evals=evals, early_stopping_rounds=rounds, verbose_eval=verbose\n",
    "    )\n",
    "    best_iter = getattr(booster, \"best_iteration\", None)\n",
    "    return XGBAdapter(booster, {**sk_model.get_params(), \"best_iteration\": best_iter}, best_iter, feature_names)\n",
    "\n",
    "\n",
    "def xgb_gain_importances(booster, feature_names):\n",
    "    gain_dict = booster.get_score(importance_type=\"gain\")\n",
    "    name_to_idx = {n: i for i, n in enumerate(feature_names)}\n",
    "    imp_gain = np.zeros(len(feature_names), dtype=float)\n",
    "    for k, v in gain_dict.items():\n",
    "        if k.startswith(\"f\") and k[1:].isdigit():\n",
    "            idx = int(k[1:])\n",
    "        else:\n",
    "            idx = name_to_idx.get(k, None)\n",
    "        if idx is not None and 0 <= idx < len(imp_gain):\n",
    "            imp_gain[idx] = v\n",
    "    return imp_gain\n",
    "\n",
    "\n",
    "# --- LightGBM ---\n",
    "class LGBMAdapter:\n",
    "    \"\"\"Adapter consistente con .booster_ como atributo y .best_iteration.\"\"\"\n",
    "    def __init__(self, booster, params, best_iteration, feature_names=None):\n",
    "        self.booster_ = booster\n",
    "        self._params = dict(params)\n",
    "        self.best_iteration = best_iteration\n",
    "        self._feature_names = feature_names\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        p1 = self.booster_.predict(X, num_iteration=self.best_iteration)\n",
    "        return np.column_stack([1.0 - p1, p1])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict(self._params)\n",
    "\n",
    "\n",
    "def lgbm_fit_with_es(sk_model, X_tr, y_tr, X_va, y_va, feature_names=None, rounds=200, verbose=False):\n",
    "    p = dict(sk_model.get_params())\n",
    "    num_boost_round = int(p.pop(\"n_estimators\", 1000))\n",
    "\n",
    "    # Defaults coherentes con AP\n",
    "    lgb_params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": p.pop(\"metric\", \"average_precision\"),\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": p.pop(\"random_state\", RANDOM_STATE),\n",
    "    }\n",
    "\n",
    "    # Mapear nombres scikit->lightgbm\n",
    "    if \"colsample_bytree\" in p:\n",
    "        lgb_params[\"feature_fraction\"] = p.pop(\"colsample_bytree\")\n",
    "    if \"subsample\" in p:\n",
    "        lgb_params[\"bagging_fraction\"] = p.pop(\"subsample\")\n",
    "        lgb_params[\"bagging_freq\"] = 1\n",
    "\n",
    "    # Resto de hiperparámetros\n",
    "    lgb_params.update(p)\n",
    "\n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr, feature_name=feature_names, free_raw_data=True)\n",
    "    dvalid = lgb.Dataset(X_va, label=y_va, feature_name=feature_names, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "    booster = lgb.train(\n",
    "        params=lgb_params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        valid_names=[\"train\", \"valid\"],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=rounds, verbose=verbose)]\n",
    "    )\n",
    "    best_iter = booster.best_iteration\n",
    "    return LGBMAdapter(booster, {**sk_model.get_params(), \"best_iteration\": best_iter}, best_iter, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be5066",
   "metadata": {},
   "source": [
    "6 — Carga/seed de hiperparámetros y defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df2c238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HP][xgb] Cargando BEST previo: BEST_XGB_REDUCED_SMOTENC.json\n",
      "[HP][lgbm] Cargando BEST previo: BEST_LGBM_REDUCED_SMOTENC.json\n"
     ]
    }
   ],
   "source": [
    "def get_xgb_defaults(seed=RANDOM_STATE):\n",
    "    mdl = XGBClassifier(\n",
    "        random_state=seed, n_jobs=-1, eval_metric=\"aucpr\",\n",
    "        tree_method=\"hist\", verbosity=0\n",
    "    )\n",
    "    p = mdl.get_params()\n",
    "    p.pop(\"verbose\", None)\n",
    "    p.setdefault(\"verbosity\", 0)\n",
    "    p.setdefault(\"n_estimators\", 1000)\n",
    "    return p\n",
    "\n",
    "def get_lgbm_defaults(seed=RANDOM_STATE):\n",
    "    mdl = LGBMClassifier(\n",
    "        random_state=seed, n_estimators=1000, metric=\"average_precision\"\n",
    "    )\n",
    "    return mdl.get_params()\n",
    "\n",
    "def _best_file(model_key):\n",
    "    return DIRS[model_key] / \"best_params\" / f\"BEST_{model_key.upper()}_{VIEW_TAG}_{BAL_TAG}.json\"\n",
    "\n",
    "def load_best_or_default(model_key):\n",
    "    best_fp = _best_file(model_key)\n",
    "    if best_fp.exists():\n",
    "        try:\n",
    "            best = json.loads(best_fp.read_text())\n",
    "            print(f\"[HP][{model_key}] Cargando BEST previo:\", best_fp.name)\n",
    "            base = get_xgb_defaults() if model_key==\"xgb\" else get_lgbm_defaults()\n",
    "            base.update(best)\n",
    "            return base, True\n",
    "        except Exception as e:\n",
    "            print(f\"[HP][{model_key}] No se pudo leer BEST. Uso defaults. {e}\")\n",
    "    print(f\"[HP][{model_key}] Usando defaults.\")\n",
    "    return (get_xgb_defaults() if model_key==\"xgb\" else get_lgbm_defaults()), False\n",
    "\n",
    "xgb_params_seed, _ = load_best_or_default(\"xgb\")\n",
    "lgbm_params_seed, _ = load_best_or_default(\"lgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3747f83",
   "metadata": {},
   "source": [
    "7 — Entrenamiento BASELINE (XGB y LGBM) + umbral + guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45f8257e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[EXPORT][XGB] XGB_REDUCED_SMOTENC_BASE -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_REDUCED_SMOTENC/export\n",
      "[OK][XGB BASE] Guardados en /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_REDUCED_SMOTENC\n",
      "[EXPORT][LGBM] LGBM_REDUCED_SMOTENC_BASE -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_REDUCED_SMOTENC/export\n",
      "[OK][LGBM BASE] Guardados en /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_REDUCED_SMOTENC\n"
     ]
    }
   ],
   "source": [
    "if \"X_train_fit\" not in globals():\n",
    "    X_train_fit, X_val_fit, X_test_fit = X_train, X_val, X_test\n",
    "\n",
    "if \"feature_names_used\" not in globals():\n",
    "    feature_names_used = feature_names\n",
    "\n",
    "if \"keep_idx_global\" not in globals():\n",
    "    keep_idx_global = None  # sin L1\n",
    "\n",
    "# Balanceo global para baseline\n",
    "X_train_final, y_train_final = X_train_fit, y_train\n",
    "if USE_BALANCED_TRAIN:\n",
    "    X_train_final, y_train_final = maybe_smote(\n",
    "        X_train_fit,\n",
    "        y_train,\n",
    "        keep_idx=keep_idx_global\n",
    "    )\n",
    "\n",
    "# ---- XGB BASELINE ----\n",
    "xgb_seed = dict(xgb_params_seed)\n",
    "xgb_seed.setdefault(\"n_estimators\", xgb_seed.get(\"n_estimators\", 1000))\n",
    "xgb_seed.setdefault(\"random_state\", RANDOM_STATE)\n",
    "xgb_seed.setdefault(\"n_jobs\", -1)\n",
    "xgb_seed.setdefault(\"eval_metric\", \"aucpr\")\n",
    "xgb_seed.setdefault(\"tree_method\", \"hist\")\n",
    "xgb_seed.setdefault(\"verbosity\", 0)\n",
    "xgb_seed.pop(\"verbose\", None)\n",
    "\n",
    "xgb_baseline_model = XGBClassifier(**xgb_seed)\n",
    "xgb_baseline_model = xgb_fit_with_es(\n",
    "    xgb_baseline_model, X_train_final, y_train_final,\n",
    "    X_val_fit, y_val, feature_names=feature_names_used,\n",
    "    rounds=200, verbose=False\n",
    ")\n",
    "\n",
    "# Validación\n",
    "proba_val_xgb = xgb_baseline_model.predict_proba(X_val_fit)[:,1]\n",
    "thr_val_xgb, best_f1_val_xgb = find_best_threshold(y_val, proba_val_xgb, metric=\"f1\")\n",
    "val_metrics_xgb = compute_all_metrics(y_val, proba_val_xgb, thr_val_xgb)\n",
    "\n",
    "# Test\n",
    "proba_test_xgb = xgb_baseline_model.predict_proba(X_test_fit)[:,1]\n",
    "y_pred_test_xgb = (proba_test_xgb >= thr_val_xgb).astype(int)\n",
    "test_metrics_xgb = compute_all_metrics(y_test, proba_test_xgb, thr_val_xgb)\n",
    "\n",
    "# === Guardados ===\n",
    "EXP_NAME_XGB = f\"XGB_{VIEW_TAG}_{BAL_TAG}\"\n",
    "bx = DIRS[\"xgb\"]\n",
    "OUT_RESULTS_X = bx / \"results\"\n",
    "OUT_FIGS_X    = bx / \"figs\"\n",
    "OUT_PREDS_X   = bx / \"preds\"\n",
    "OUT_PARAMS_X  = bx / \"best_params\"\n",
    "OUT_EXPORT_X  = bx / \"export\"\n",
    "\n",
    "# HP seed y \"fitted\"\n",
    "with open(OUT_PARAMS_X / f\"{EXP_NAME_XGB}_BASE_seed_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(xgb_seed, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(OUT_PARAMS_X / f\"{EXP_NAME_XGB}_BASE_fitted_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(xgb_baseline_model.get_params(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Plots\n",
    "plot_pr_curve(y_val,  proba_val_xgb,  f\"{EXP_NAME_XGB} — PR (val)\",  OUT_FIGS_X / f\"{EXP_NAME_XGB}_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_xgb, f\"{EXP_NAME_XGB} — PR (test)\", OUT_FIGS_X / f\"{EXP_NAME_XGB}_pr_test.png\")\n",
    "plot_roc_curve(y_val,  proba_val_xgb,  f\"{EXP_NAME_XGB} — ROC (val)\",  OUT_FIGS_X / f\"{EXP_NAME_XGB}_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_xgb, f\"{EXP_NAME_XGB} — ROC (test)\", OUT_FIGS_X / f\"{EXP_NAME_XGB}_roc_test.png\")\n",
    "plot_confusion(y_test, y_pred_test_xgb,\n",
    "               f\"{EXP_NAME_XGB} — Confusion (test @thr={thr_val_xgb:.3f})\",\n",
    "               OUT_FIGS_X / f\"{EXP_NAME_XGB}_cm_test.png\")\n",
    "\n",
    "# Importancias en el espacio L1-reducido\n",
    "try:\n",
    "    booster = xgb_baseline_model.get_booster()\n",
    "    imp_gain_x = xgb_gain_importances(booster, feature_names_used)\n",
    "except Exception:\n",
    "    imp_gain_x = getattr(xgb_baseline_model, \"feature_importances_\", np.zeros(len(feature_names_used)))\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\"feature\": feature_names_used[:len(imp_gain_x)], \"importance_gain\": imp_gain_x}\n",
    ").sort_values(\"importance_gain\", ascending=False)\\\n",
    " .to_csv(OUT_RESULTS_X / f\"{EXP_NAME_XGB}_feature_importances.csv\", index=False)\n",
    "\n",
    "# Preds test\n",
    "pd.DataFrame({\"proba\": proba_test_xgb, \"y_true\": y_test}).to_parquet(\n",
    "    OUT_PREDS_X / f\"preds_test_{EXP_NAME_XGB}.parquet\", index=False\n",
    ")\n",
    "\n",
    "# Registro baselines.csv\n",
    "best_iter_base_xgb = getattr(xgb_baseline_model, \"best_iteration\", None)\n",
    "row_base_xgb = {\n",
    "    \"model\": EXP_NAME_XGB,\n",
    "    \"thr_val\": thr_val_xgb,\n",
    "    \"val_pr_auc\": val_metrics_xgb[\"pr_auc\"],\n",
    "    \"val_roc_auc\": val_metrics_xgb[\"roc_auc\"],\n",
    "    \"val_precision\": val_metrics_xgb[\"precision\"],\n",
    "    \"val_f1\": val_metrics_xgb[\"f1\"],\n",
    "    \"val_recall\": val_metrics_xgb[\"recall\"],\n",
    "    \"val_bal_acc\": val_metrics_xgb[\"bal_acc\"],\n",
    "    \"test_pr_auc\": test_metrics_xgb[\"pr_auc\"],\n",
    "    \"test_roc_auc\": test_metrics_xgb[\"roc_auc\"],\n",
    "    \"test_precision\": test_metrics_xgb[\"precision\"],\n",
    "    \"test_f1\": test_metrics_xgb[\"f1\"],\n",
    "    \"test_recall\": test_metrics_xgb[\"recall\"],\n",
    "    \"test_bal_acc\": test_metrics_xgb[\"bal_acc\"],\n",
    "    \"best_iteration\": best_iter_base_xgb if best_iter_base_xgb is not None else np.nan\n",
    "}\n",
    "csv_x = OUT_RESULTS_X / \"baselines.csv\"\n",
    "pd.DataFrame([row_base_xgb]).to_csv(csv_x, mode=(\"a\" if csv_x.exists() else \"w\"),\n",
    "                                    index=False, header=not csv_x.exists())\n",
    "\n",
    "# === EXPORT: XGB BASE ===\n",
    "export_xgb(\n",
    "    adapter=xgb_baseline_model,\n",
    "    out_dir=OUT_EXPORT_X,\n",
    "    exp_name=f\"{EXP_NAME_XGB}_BASE\",\n",
    "    feature_names=feature_names_used,\n",
    "    threshold=thr_val_xgb,\n",
    "    val_metrics=val_metrics_xgb,\n",
    "    test_metrics=test_metrics_xgb\n",
    ")\n",
    "print(\"[OK][XGB BASE] Guardados en\", bx)\n",
    "\n",
    "# ---- LGBM BASELINE ----\n",
    "lgbm_seed = dict(lgbm_params_seed)\n",
    "lgbm_seed.setdefault(\"n_estimators\", lgbm_seed.get(\"n_estimators\", 1000))\n",
    "lgbm_seed.setdefault(\"random_state\", RANDOM_STATE)\n",
    "lgbm_seed.setdefault(\"objective\", \"binary\")\n",
    "if (\"metric\" not in lgbm_seed) or (lgbm_seed[\"metric\"] in (None, \"\", [], \"None\")):\n",
    "    lgbm_seed[\"metric\"] = \"average_precision\"\n",
    "lgbm_seed.setdefault(\"n_jobs\", -1)\n",
    "lgbm_seed.setdefault(\"verbosity\", -1)\n",
    "\n",
    "lgbm_baseline_model = LGBMClassifier(**lgbm_seed)\n",
    "lgbm_adapter = lgbm_fit_with_es(\n",
    "    lgbm_baseline_model, X_train_final, y_train_final,\n",
    "    X_val_fit, y_val, feature_names=feature_names_used,\n",
    "    rounds=200, verbose=False\n",
    ")\n",
    "\n",
    "# Validación\n",
    "proba_val_lgb = lgbm_adapter.predict_proba(X_val_fit)[:,1]\n",
    "thr_val_lgb, best_f1_val_lgb = find_best_threshold(y_val, proba_val_lgb, metric=\"f1\")\n",
    "val_metrics_lgb = compute_all_metrics(y_val, proba_val_lgb, thr_val_lgb)\n",
    "\n",
    "# Test\n",
    "proba_test_lgb = lgbm_adapter.predict_proba(X_test_fit)[:,1]\n",
    "y_pred_test_lgb = (proba_test_lgb >= thr_val_lgb).astype(int)\n",
    "test_metrics_lgb = compute_all_metrics(y_test, proba_test_lgb, thr_val_lgb)\n",
    "\n",
    "# === Guardados ===\n",
    "EXP_NAME_LGB = f\"LGBM_{VIEW_TAG}_{BAL_TAG}\"\n",
    "bl = DIRS[\"lgbm\"]\n",
    "OUT_RESULTS_L = bl / \"results\"\n",
    "OUT_FIGS_L    = bl / \"figs\"\n",
    "OUT_PREDS_L   = bl / \"preds\"\n",
    "OUT_PARAMS_L  = bl / \"best_params\"\n",
    "OUT_EXPORT_L  = bl / \"export\"\n",
    "\n",
    "with open(OUT_PARAMS_L / f\"{EXP_NAME_LGB}_BASE_seed_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(lgbm_seed, f, indent=2, ensure_ascii=False)\n",
    "with open(OUT_PARAMS_L / f\"{EXP_NAME_LGB}_BASE_fitted_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(lgbm_baseline_model.get_params(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Plots\n",
    "plot_pr_curve(y_val,  proba_val_lgb,  f\"{EXP_NAME_LGB} — PR (val)\",  OUT_FIGS_L / f\"{EXP_NAME_LGB}_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_lgb, f\"{EXP_NAME_LGB} — PR (test)\", OUT_FIGS_L / f\"{EXP_NAME_LGB}_pr_test.png\")\n",
    "plot_roc_curve(y_val,  proba_val_lgb,  f\"{EXP_NAME_LGB} — ROC (val)\",  OUT_FIGS_L / f\"{EXP_NAME_LGB}_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_lgb, f\"{EXP_NAME_LGB} — ROC (test)\", OUT_FIGS_L / f\"{EXP_NAME_LGB}_roc_test.png\")\n",
    "plot_confusion(y_test, y_pred_test_lgb,\n",
    "               f\"{EXP_NAME_LGB} — Confusion (test @thr_used={thr_val_lgb:.3f})\",\n",
    "               OUT_FIGS_L / f\"{EXP_NAME_LGB}_cm_test.png\")\n",
    "\n",
    "# Importancias LGBM (gain) en el espacio L1-reducido\n",
    "try:\n",
    "    imp_gain_l = lgbm_adapter.booster_.feature_importance(importance_type=\"gain\")\n",
    "except Exception:\n",
    "    imp_gain_l = np.zeros(len(feature_names_used))\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\"feature\": feature_names_used[:len(imp_gain_l)], \"importance_gain\": imp_gain_l}\n",
    ").sort_values(\"importance_gain\", ascending=False)\\\n",
    " .to_csv(OUT_RESULTS_L / f\"{EXP_NAME_LGB}_feature_importances.csv\", index=False)\n",
    "\n",
    "# Preds test\n",
    "pd.DataFrame({\"proba\": proba_test_lgb, \"y_true\": y_test})\\\n",
    "  .to_parquet(OUT_PREDS_L / f\"preds_test_{EXP_NAME_LGB}.parquet\", index=False)\n",
    "\n",
    "# Registro baselines.csv (LGBM)\n",
    "row_base_lgb = {\n",
    "    \"model\": EXP_NAME_LGB,\n",
    "    \"thr_val\": thr_val_lgb,\n",
    "    \"thr_oof\": np.nan,\n",
    "    \"thr_used\": thr_val_lgb,\n",
    "    \"val_pr_auc\": val_metrics_lgb[\"pr_auc\"],\n",
    "    \"val_roc_auc\": val_metrics_lgb[\"roc_auc\"],\n",
    "    \"val_precision\": val_metrics_lgb[\"precision\"],\n",
    "    \"val_f1\": val_metrics_lgb[\"f1\"],\n",
    "    \"val_recall\": val_metrics_lgb[\"recall\"],\n",
    "    \"val_bal_acc\": val_metrics_lgb[\"bal_acc\"],\n",
    "    \"test_pr_auc\": test_metrics_lgb[\"pr_auc\"],\n",
    "    \"test_roc_auc\": test_metrics_lgb[\"roc_auc\"],\n",
    "    \"test_precision\": test_metrics_lgb[\"precision\"],\n",
    "    \"test_f1\": test_metrics_lgb[\"f1\"],\n",
    "    \"test_recall\": test_metrics_lgb[\"recall\"],\n",
    "    \"test_bal_acc\": test_metrics_lgb[\"bal_acc\"],\n",
    "    \"best_iteration\": lgbm_adapter.best_iteration if hasattr(lgbm_adapter, \"best_iteration\") else np.nan\n",
    "}\n",
    "csv_l = OUT_RESULTS_L / \"baselines.csv\"\n",
    "pd.DataFrame([row_base_lgb]).to_csv(\n",
    "    csv_l,\n",
    "    mode=(\"a\" if csv_l.exists() else \"w\"),\n",
    "    index=False,\n",
    "    header=not csv_l.exists()\n",
    ")\n",
    "\n",
    "# === EXPORT: LGBM BASE ===\n",
    "export_lgbm(\n",
    "    adapter=lgbm_adapter,\n",
    "    out_dir=OUT_EXPORT_L,\n",
    "    exp_name=f\"{EXP_NAME_LGB}_BASE\",\n",
    "    feature_names=feature_names_used,\n",
    "    threshold=thr_val_lgb,\n",
    "    val_metrics=val_metrics_lgb,\n",
    "    test_metrics=test_metrics_lgb\n",
    ")\n",
    "print(\"[OK][LGBM BASE] Guardados en\", bl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ce5f1",
   "metadata": {},
   "source": [
    "8 — Optuna incremental (XGB y LGBM), re-entreno TUNED + guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6491c887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 23:59:43,799] A new study created in memory with name: XGB_REDUCED_SMOTENC_AP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA][XGB] Enqueue BEST anterior.\n",
      "[OPTUNA][XGB] Iniciando 40 pruebas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 23:59:44,959] Trial 0 finished with value: 0.6985261084210692 and parameters: {'learning_rate': 0.11606982623590888, 'n_estimators': 1900, 'max_depth': 3, 'min_child_weight': 0.9277474205244098, 'subsample': 0.7326900127737387, 'colsample_bytree': 0.8024358862026536, 'gamma': 4.187670208393132e-06, 'reg_alpha': 3.373539017033962e-06, 'reg_lambda': 2.261756498442988e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 23:59:49,028] Trial 1 finished with value: 0.6870151419929501 and parameters: {'learning_rate': 0.008468008575248327, 'n_estimators': 2900, 'max_depth': 8, 'min_child_weight': 4.550475813202184, 'subsample': 0.6624074561769746, 'colsample_bytree': 0.662397808134481, 'gamma': 3.200866785899844e-08, 'reg_alpha': 1.156732719914599, 'reg_lambda': 0.016136341713591334}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 23:59:50,048] Trial 2 finished with value: 0.6815970793795767 and parameters: {'learning_rate': 0.05675206026988748, 'n_estimators': 800, 'max_depth': 10, 'min_child_weight': 10.779361932748845, 'subsample': 0.6849356442713105, 'colsample_bytree': 0.6727299868828402, 'gamma': 3.939402261362697e-07, 'reg_alpha': 0.000134801802908908, 'reg_lambda': 0.004712973756110786}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 23:59:53,337] Trial 3 finished with value: 0.6874037558252651 and parameters: {'learning_rate': 0.01174843954800703, 'n_estimators': 1450, 'max_depth': 7, 'min_child_weight': 0.8364645453054501, 'subsample': 0.7168578594140873, 'colsample_bytree': 0.7465447373174767, 'gamma': 9.275538076980542e-05, 'reg_alpha': 0.3134958021096912, 'reg_lambda': 2.498713568466947e-05}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 23:59:56,507] Trial 4 finished with value: 0.6767858481231521 and parameters: {'learning_rate': 0.018785426399210624, 'n_estimators': 2100, 'max_depth': 3, 'min_child_weight': 4.702115628087815, 'subsample': 0.6682096494749166, 'colsample_bytree': 0.6260206371941118, 'gamma': 1.7960847528705854, 'reg_alpha': 5.746775499181867, 'reg_lambda': 0.45580746840273345}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 23:59:59,548] Trial 5 finished with value: 0.6803345681182145 and parameters: {'learning_rate': 0.0056828375585122656, 'n_estimators': 1000, 'max_depth': 8, 'min_child_weight': 2.5358333235759627, 'subsample': 0.6488152939379115, 'colsample_bytree': 0.798070764044508, 'gamma': 1.9913367728263115e-08, 'reg_alpha': 2.318690670290199, 'reg_lambda': 6.478282331897332e-05}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:01,697] Trial 6 finished with value: 0.6813013971295447 and parameters: {'learning_rate': 0.043767126303409544, 'n_estimators': 1500, 'max_depth': 7, 'min_child_weight': 3.7569262495760847, 'subsample': 0.6739417822102108, 'colsample_bytree': 0.9878338511058234, 'gamma': 0.05531681668096113, 'reg_alpha': 3.7713131110779936, 'reg_lambda': 1.8356566544355097}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:04,130] Trial 7 finished with value: 0.6930980084551537 and parameters: {'learning_rate': 0.03027406546455652, 'n_estimators': 2850, 'max_depth': 3, 'min_child_weight': 1.0302587393796305, 'subsample': 0.6180909155642152, 'colsample_bytree': 0.7301321323053057, 'gamma': 2.4048726561760165e-05, 'reg_alpha': 7.933105363733024e-05, 'reg_lambda': 0.6326486185661588}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:07,120] Trial 8 finished with value: 0.6856611302098745 and parameters: {'learning_rate': 0.0076510536667541975, 'n_estimators': 1400, 'max_depth': 7, 'min_child_weight': 0.8408897660399112, 'subsample': 0.9208787923016158, 'colsample_bytree': 0.6298202574719083, 'gamma': 3.845031120156871, 'reg_alpha': 0.2545150013091294, 'reg_lambda': 2.4604229580184192e-05}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:14,738] Trial 9 finished with value: 0.6713396538935019 and parameters: {'learning_rate': 0.0010319982330247674, 'n_estimators': 2600, 'max_depth': 8, 'min_child_weight': 7.360091638366141, 'subsample': 0.9085081386743783, 'colsample_bytree': 0.6296178606936361, 'gamma': 1.3130541002425655e-05, 'reg_alpha': 6.472669269538641e-06, 'reg_lambda': 1.100839441018132}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:17,668] Trial 10 finished with value: 0.6969358534629874 and parameters: {'learning_rate': 0.028304338560259375, 'n_estimators': 2350, 'max_depth': 3, 'min_child_weight': 1.0472289957751704, 'subsample': 0.7935242144561879, 'colsample_bytree': 0.6637225526979548, 'gamma': 7.681601539365231e-07, 'reg_alpha': 2.476386872182822e-06, 'reg_lambda': 1.072368902873431e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:21,408] Trial 11 finished with value: 0.6927468902565489 and parameters: {'learning_rate': 0.0160083706077498, 'n_estimators': 2300, 'max_depth': 3, 'min_child_weight': 2.7577263485591916, 'subsample': 0.8010340622918176, 'colsample_bytree': 0.6735211865706826, 'gamma': 3.078557117558923e-06, 'reg_alpha': 1.4742090057536537e-05, 'reg_lambda': 0.00014098297142562958}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:22,013] Trial 12 finished with value: 0.6944471307121453 and parameters: {'learning_rate': 0.19685199737418815, 'n_estimators': 2050, 'max_depth': 3, 'min_child_weight': 0.9910642942475216, 'subsample': 0.6501710610062617, 'colsample_bytree': 0.7815771392680276, 'gamma': 1.0591109662015561e-05, 'reg_alpha': 3.007895336581209e-06, 'reg_lambda': 1.970872455967331e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:22,687] Trial 13 finished with value: 0.6904253450669392 and parameters: {'learning_rate': 0.09953631636717705, 'n_estimators': 2000, 'max_depth': 4, 'min_child_weight': 1.7172164448435878, 'subsample': 0.8723751565051584, 'colsample_bytree': 0.9199609008851154, 'gamma': 0.0006027646959910485, 'reg_alpha': 4.653518715822291e-06, 'reg_lambda': 1.1568649457330939e-05}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:23,520] Trial 14 finished with value: 0.6864053758172707 and parameters: {'learning_rate': 0.07541791988891157, 'n_estimators': 2300, 'max_depth': 5, 'min_child_weight': 0.8170642037167101, 'subsample': 0.9344632303169983, 'colsample_bytree': 0.7154042383240529, 'gamma': 5.349385195769777e-06, 'reg_alpha': 2.1114205740662962e-05, 'reg_lambda': 1.938495803006502e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:28,358] Trial 15 finished with value: 0.6903672397661142 and parameters: {'learning_rate': 0.00713400293075799, 'n_estimators': 2600, 'max_depth': 4, 'min_child_weight': 0.7355710359537192, 'subsample': 0.7412667205080979, 'colsample_bytree': 0.906298874342615, 'gamma': 1.294912063772593e-08, 'reg_alpha': 8.40906316395031e-05, 'reg_lambda': 3.862453819083224e-05}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:29,828] Trial 16 finished with value: 0.6882949579448491 and parameters: {'learning_rate': 0.037336245956555214, 'n_estimators': 1200, 'max_depth': 7, 'min_child_weight': 0.7584124798386164, 'subsample': 0.7166653626000349, 'colsample_bytree': 0.8806480377352929, 'gamma': 1.7352040551518834e-08, 'reg_alpha': 3.946272244604698e-05, 'reg_lambda': 2.1253985744835327e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:32,706] Trial 17 finished with value: 0.6762270002684848 and parameters: {'learning_rate': 0.006488677479842333, 'n_estimators': 1700, 'max_depth': 3, 'min_child_weight': 0.8697657889231751, 'subsample': 0.7518376492735205, 'colsample_bytree': 0.6126036567710205, 'gamma': 0.00017213831411584595, 'reg_alpha': 0.0002242190491863164, 'reg_lambda': 2.279401926477103e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:40,078] Trial 18 finished with value: 0.6863139267086094 and parameters: {'learning_rate': 0.0032030060617531646, 'n_estimators': 2600, 'max_depth': 7, 'min_child_weight': 0.5863334321109573, 'subsample': 0.847142852076887, 'colsample_bytree': 0.6189963350334643, 'gamma': 3.3334184613381995e-07, 'reg_alpha': 5.591184980105869e-06, 'reg_lambda': 1.533866236826477e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:40,845] Trial 19 finished with value: 0.681572200952074 and parameters: {'learning_rate': 0.10970668795261575, 'n_estimators': 1700, 'max_depth': 6, 'min_child_weight': 0.5155809590609095, 'subsample': 0.669832907231363, 'colsample_bytree': 0.7539796407337818, 'gamma': 1.4853921735747645e-06, 'reg_alpha': 0.00033191986759026506, 'reg_lambda': 0.007838334668939904}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:41,471] Trial 20 finished with value: 0.6927182890686895 and parameters: {'learning_rate': 0.13720326610782393, 'n_estimators': 1550, 'max_depth': 4, 'min_child_weight': 1.9912837820619198, 'subsample': 0.7603813306741566, 'colsample_bytree': 0.6575380721275549, 'gamma': 2.2160418365970487e-08, 'reg_alpha': 0.00030696597083838237, 'reg_lambda': 2.195183470349407e-05}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:42,322] Trial 21 finished with value: 0.6931894946263361 and parameters: {'learning_rate': 0.11007028519692846, 'n_estimators': 2750, 'max_depth': 3, 'min_child_weight': 0.7444284047306119, 'subsample': 0.6736139495057059, 'colsample_bytree': 0.7582090911054618, 'gamma': 1.2314089228807442e-06, 'reg_alpha': 1.65892246448083e-06, 'reg_lambda': 3.1258728640465053e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:43,495] Trial 22 finished with value: 0.6942723757709135 and parameters: {'learning_rate': 0.06518317338700105, 'n_estimators': 1800, 'max_depth': 3, 'min_child_weight': 1.521288160083238, 'subsample': 0.6871983783759206, 'colsample_bytree': 0.8704360377657935, 'gamma': 0.00022013026165720398, 'reg_alpha': 5.7296532126252385e-05, 'reg_lambda': 2.5159344234651223e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:44,720] Trial 23 finished with value: 0.6920193846958207 and parameters: {'learning_rate': 0.05582705033195827, 'n_estimators': 1250, 'max_depth': 3, 'min_child_weight': 1.0060962125212798, 'subsample': 0.7918305570233064, 'colsample_bytree': 0.7757930789034854, 'gamma': 2.4925317927977047e-06, 'reg_alpha': 2.980048363536986e-06, 'reg_lambda': 1.0426834378475767e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:45,276] Trial 24 finished with value: 0.6939266281994547 and parameters: {'learning_rate': 0.21503214217072028, 'n_estimators': 1700, 'max_depth': 5, 'min_child_weight': 2.693062181189402, 'subsample': 0.7042331304663629, 'colsample_bytree': 0.7510765992847765, 'gamma': 2.147578842589465e-05, 'reg_alpha': 2.8598120355455645e-06, 'reg_lambda': 7.3324665343643e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:45,802] Trial 25 finished with value: 0.6906635841782546 and parameters: {'learning_rate': 0.19646237723183735, 'n_estimators': 2050, 'max_depth': 3, 'min_child_weight': 2.812061130599993, 'subsample': 0.7821028632573472, 'colsample_bytree': 0.8383259548581983, 'gamma': 2.4570937081587013e-08, 'reg_alpha': 1.872328486784592e-06, 'reg_lambda': 2.9262428540782017e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:46,422] Trial 26 finished with value: 0.6883640398960995 and parameters: {'learning_rate': 0.2076976013549047, 'n_estimators': 1700, 'max_depth': 3, 'min_child_weight': 0.5466718137373172, 'subsample': 0.6403576562310348, 'colsample_bytree': 0.7115531215576071, 'gamma': 0.0004936204202299263, 'reg_alpha': 3.8640725445155054e-06, 'reg_lambda': 1.80040028331496e-05}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:47,028] Trial 27 finished with value: 0.6826876389529224 and parameters: {'learning_rate': 0.28816682063779453, 'n_estimators': 1850, 'max_depth': 3, 'min_child_weight': 0.7734591682132363, 'subsample': 0.6701188972723573, 'colsample_bytree': 0.9026080153786235, 'gamma': 2.3185145200651983e-06, 'reg_alpha': 1.403206426317835e-06, 'reg_lambda': 1.4795533565360988e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:48,545] Trial 28 finished with value: 0.6943272363527292 and parameters: {'learning_rate': 0.04504136295668321, 'n_estimators': 1950, 'max_depth': 3, 'min_child_weight': 1.0417969917437513, 'subsample': 0.8270421770435953, 'colsample_bytree': 0.6416604518728846, 'gamma': 3.6563875840795693e-08, 'reg_alpha': 2.101021981126071e-05, 'reg_lambda': 2.7970095601217576e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:51,206] Trial 29 finished with value: 0.6910936107353797 and parameters: {'learning_rate': 0.013052692537617571, 'n_estimators': 2850, 'max_depth': 5, 'min_child_weight': 2.6830434637077056, 'subsample': 0.9243952712229502, 'colsample_bytree': 0.6485370104825706, 'gamma': 9.324675045706503e-06, 'reg_alpha': 0.014935439487259141, 'reg_lambda': 1.492974375459467e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:55,543] Trial 30 finished with value: 0.6832333430649249 and parameters: {'learning_rate': 0.005723889151239883, 'n_estimators': 2550, 'max_depth': 3, 'min_child_weight': 0.97959881119956, 'subsample': 0.9052137769265846, 'colsample_bytree': 0.6271997931863098, 'gamma': 5.355959610314077e-08, 'reg_alpha': 1.9613570571009432e-05, 'reg_lambda': 2.4077743274108777e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:56,441] Trial 31 finished with value: 0.6913769637551843 and parameters: {'learning_rate': 0.07949045888193638, 'n_estimators': 2950, 'max_depth': 4, 'min_child_weight': 0.6101946139769013, 'subsample': 0.8488660186792414, 'colsample_bytree': 0.6205211172004278, 'gamma': 4.1294075564278804e-08, 'reg_alpha': 1.3087901325428268e-06, 'reg_lambda': 6.472316878786021e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:58,493] Trial 32 finished with value: 0.6932032025278745 and parameters: {'learning_rate': 0.03642315981164597, 'n_estimators': 1350, 'max_depth': 3, 'min_child_weight': 0.7366565206666625, 'subsample': 0.8879687323579836, 'colsample_bytree': 0.6081668730400498, 'gamma': 1.4310535117135359e-07, 'reg_alpha': 0.0005318143302206478, 'reg_lambda': 1.5707378353494738e-05}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:00:59,149] Trial 33 finished with value: 0.6972385244478406 and parameters: {'learning_rate': 0.136057876622998, 'n_estimators': 2150, 'max_depth': 5, 'min_child_weight': 0.7865371440615943, 'subsample': 0.7478394801178562, 'colsample_bytree': 0.7238940023915518, 'gamma': 1.901486543092087e-05, 'reg_alpha': 2.0051687872977984e-05, 'reg_lambda': 3.902224544599346e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:01:00,733] Trial 34 finished with value: 0.688987106379641 and parameters: {'learning_rate': 0.03063077052680262, 'n_estimators': 2150, 'max_depth': 6, 'min_child_weight': 1.337298166992469, 'subsample': 0.7615022286610728, 'colsample_bytree': 0.7282129485070883, 'gamma': 0.0038426972354210975, 'reg_alpha': 6.7443481010330595e-06, 'reg_lambda': 7.886387548925967e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:01:01,982] Trial 35 finished with value: 0.689525790502701 and parameters: {'learning_rate': 0.06647730495284433, 'n_estimators': 1950, 'max_depth': 5, 'min_child_weight': 0.8156241060542155, 'subsample': 0.7120917792899797, 'colsample_bytree': 0.7759989552796949, 'gamma': 4.868664158132852e-06, 'reg_alpha': 1.3800450451896873e-05, 'reg_lambda': 1.3912167588518576e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:01:02,932] Trial 36 finished with value: 0.6901691300002883 and parameters: {'learning_rate': 0.09417407025120865, 'n_estimators': 2100, 'max_depth': 3, 'min_child_weight': 1.8993036087257569, 'subsample': 0.6345023132283644, 'colsample_bytree': 0.8112649108381456, 'gamma': 6.7894792011096e-08, 'reg_alpha': 0.00033687787424117703, 'reg_lambda': 1.101508465491629e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:01:06,778] Trial 37 finished with value: 0.6806681520685144 and parameters: {'learning_rate': 0.006617224649266256, 'n_estimators': 2100, 'max_depth': 3, 'min_child_weight': 0.6425553708111885, 'subsample': 0.6981647851341181, 'colsample_bytree': 0.613373908293974, 'gamma': 6.175752313303042e-08, 'reg_alpha': 2.0176591471802947e-06, 'reg_lambda': 1.812394323164593e-05}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:01:08,193] Trial 38 finished with value: 0.6947025939813635 and parameters: {'learning_rate': 0.06500470137927329, 'n_estimators': 2000, 'max_depth': 3, 'min_child_weight': 1.8756521089127083, 'subsample': 0.6507811992407317, 'colsample_bytree': 0.6919391711193206, 'gamma': 1.1123402823140596e-06, 'reg_alpha': 2.5027763849251195e-06, 'reg_lambda': 1.1576986773538814e-06}. Best is trial 0 with value: 0.6985261084210692.\n",
      "[I 2025-12-13 00:01:10,019] Trial 39 finished with value: 0.6936988983731855 and parameters: {'learning_rate': 0.02655133142019758, 'n_estimators': 2500, 'max_depth': 4, 'min_child_weight': 3.61939644905539, 'subsample': 0.673459872050579, 'colsample_bytree': 0.6127602957411732, 'gamma': 4.105742776899563e-06, 'reg_alpha': 0.0003806748557399184, 'reg_lambda': 2.728620745852609e-06}. Best is trial 0 with value: 0.6985261084210692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA][XGB] Mejor AP(val): 0.698526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-13 00:01:11,909] A new study created in memory with name: LGBM_REDUCED_SMOTENC_AP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPORT][XGB] XGB_REDUCED_SMOTENC_TUNED -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_REDUCED_SMOTENC/export\n",
      "[OPTUNA][LGBM] Enqueue BEST anterior.\n",
      "[OPTUNA][LGBM] Iniciando 40 pruebas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-13 00:01:13,295] Trial 0 finished with value: 0.6880474586286291 and parameters: {'learning_rate': 0.04163472304640577, 'n_estimators': 900, 'num_leaves': 20, 'max_depth': 4, 'min_child_samples': 33, 'subsample': 0.8509340279497106, 'colsample_bytree': 0.802267100716837, 'reg_alpha': 0.5255105942888963, 'reg_lambda': 0.003706120281152843}. Best is trial 0 with value: 0.6880474586286291.\n",
      "[I 2025-12-13 00:01:18,635] Trial 1 finished with value: 0.686645800041543 and parameters: {'learning_rate': 0.008468008575248327, 'n_estimators': 2900, 'num_leaves': 192, 'max_depth': 7, 'min_child_samples': 35, 'subsample': 0.662397808134481, 'colsample_bytree': 0.6232334448672797, 'reg_alpha': 1.156732719914599, 'reg_lambda': 0.016136341713591334}. Best is trial 0 with value: 0.6880474586286291.\n",
      "[I 2025-12-13 00:01:20,871] Trial 2 finished with value: 0.6797808136383815 and parameters: {'learning_rate': 0.05675206026988748, 'n_estimators': 800, 'num_leaves': 249, 'max_depth': 10, 'min_child_samples': 46, 'subsample': 0.6727299868828402, 'colsample_bytree': 0.6733618039413735, 'reg_alpha': 0.000134801802908908, 'reg_lambda': 0.004712973756110786}. Best is trial 0 with value: 0.6880474586286291.\n",
      "[I 2025-12-13 00:01:26,976] Trial 3 finished with value: 0.680770189292002 and parameters: {'learning_rate': 0.01174843954800703, 'n_estimators': 1450, 'num_leaves': 163, 'max_depth': 0, 'min_child_samples': 62, 'subsample': 0.7465447373174767, 'colsample_bytree': 0.7824279936868144, 'reg_alpha': 0.3134958021096912, 'reg_lambda': 2.498713568466947e-05}. Best is trial 0 with value: 0.6880474586286291.\n",
      "[I 2025-12-13 00:01:29,960] Trial 4 finished with value: 0.6760810704101783 and parameters: {'learning_rate': 0.018785426399210624, 'n_estimators': 2100, 'num_leaves': 27, 'max_depth': 7, 'min_child_samples': 38, 'subsample': 0.6260206371941118, 'colsample_bytree': 0.9795542149013333, 'reg_alpha': 5.746775499181867, 'reg_lambda': 0.45580746840273345}. Best is trial 0 with value: 0.6880474586286291.\n",
      "[I 2025-12-13 00:01:32,472] Trial 5 finished with value: 0.6715779081865891 and parameters: {'learning_rate': 0.0056828375585122656, 'n_estimators': 1000, 'num_leaves': 180, 'max_depth': 5, 'min_child_samples': 28, 'subsample': 0.798070764044508, 'colsample_bytree': 0.6137554084460873, 'reg_alpha': 2.318690670290199, 'reg_lambda': 6.478282331897332e-05}. Best is trial 0 with value: 0.6880474586286291.\n",
      "[I 2025-12-13 00:01:34,629] Trial 6 finished with value: 0.6818477612327599 and parameters: {'learning_rate': 0.043767126303409544, 'n_estimators': 1500, 'num_leaves': 141, 'max_depth': 6, 'min_child_samples': 41, 'subsample': 0.9878338511058234, 'colsample_bytree': 0.9100531293444458, 'reg_alpha': 3.7713131110779936, 'reg_lambda': 1.8356566544355097}. Best is trial 0 with value: 0.6880474586286291.\n",
      "[I 2025-12-13 00:01:35,215] Trial 7 finished with value: 0.6360067165734983 and parameters: {'learning_rate': 0.03027406546455652, 'n_estimators': 2850, 'num_leaves': 37, 'max_depth': 1, 'min_child_samples': 13, 'subsample': 0.7301321323053057, 'colsample_bytree': 0.7554709158757928, 'reg_alpha': 7.933105363733024e-05, 'reg_lambda': 0.6326486185661588}. Best is trial 0 with value: 0.6880474586286291.\n",
      "[I 2025-12-13 00:01:40,838] Trial 8 finished with value: 0.6682205574896196 and parameters: {'learning_rate': 0.0076510536667541975, 'n_estimators': 1400, 'num_leaves': 146, 'max_depth': 0, 'min_child_samples': 162, 'subsample': 0.6298202574719083, 'colsample_bytree': 0.9947547746402069, 'reg_alpha': 0.2545150013091294, 'reg_lambda': 2.4604229580184192e-05}. Best is trial 0 with value: 0.6880474586286291.\n",
      "[I 2025-12-13 00:01:49,727] Trial 9 finished with value: 0.6542466272812266 and parameters: {'learning_rate': 0.0010319982330247674, 'n_estimators': 2600, 'num_leaves': 186, 'max_depth': 9, 'min_child_samples': 156, 'subsample': 0.6296178606936361, 'colsample_bytree': 0.7433862914177091, 'reg_alpha': 6.472669269538641e-06, 'reg_lambda': 1.100839441018132}. Best is trial 0 with value: 0.6880474586286291.\n",
      "[I 2025-12-13 00:01:50,444] Trial 10 finished with value: 0.6903705526308561 and parameters: {'learning_rate': 0.20706032513178046, 'n_estimators': 1000, 'num_leaves': 21, 'max_depth': 7, 'min_child_samples': 8, 'subsample': 0.8782602332769841, 'colsample_bytree': 0.8284563758495063, 'reg_alpha': 0.08366125738309844, 'reg_lambda': 0.05476312645223597}. Best is trial 10 with value: 0.6903705526308561.\n",
      "[I 2025-12-13 00:01:51,781] Trial 11 finished with value: 0.6862075804887794 and parameters: {'learning_rate': 0.05409759409427456, 'n_estimators': 850, 'num_leaves': 43, 'max_depth': 6, 'min_child_samples': 64, 'subsample': 0.7898329741492761, 'colsample_bytree': 0.8046740328963448, 'reg_alpha': 0.6203948018154364, 'reg_lambda': 0.0010582718599728412}. Best is trial 10 with value: 0.6903705526308561.\n",
      "[I 2025-12-13 00:01:52,586] Trial 12 finished with value: 0.6893464065869895 and parameters: {'learning_rate': 0.130223405535476, 'n_estimators': 850, 'num_leaves': 17, 'max_depth': 11, 'min_child_samples': 16, 'subsample': 0.8581510546413754, 'colsample_bytree': 0.8468676589783102, 'reg_alpha': 0.0003960812820097152, 'reg_lambda': 0.42652351574428254}. Best is trial 10 with value: 0.6903705526308561.\n",
      "[I 2025-12-13 00:01:53,906] Trial 13 finished with value: 0.687511660717548 and parameters: {'learning_rate': 0.058940162687339835, 'n_estimators': 850, 'num_leaves': 24, 'max_depth': 12, 'min_child_samples': 36, 'subsample': 0.8213086528780335, 'colsample_bytree': 0.8990000424634671, 'reg_alpha': 0.0001220619836432966, 'reg_lambda': 2.104384764763959}. Best is trial 10 with value: 0.6903705526308561.\n",
      "[I 2025-12-13 00:01:55,470] Trial 14 finished with value: 0.6784786239879158 and parameters: {'learning_rate': 0.23441383925842826, 'n_estimators': 1900, 'num_leaves': 66, 'max_depth': 8, 'min_child_samples': 22, 'subsample': 0.8928630169315488, 'colsample_bytree': 0.8063357862201935, 'reg_alpha': 0.000453910279229829, 'reg_lambda': 0.03308136683504696}. Best is trial 10 with value: 0.6903705526308561.\n",
      "[I 2025-12-13 00:01:56,668] Trial 15 finished with value: 0.6861253414263903 and parameters: {'learning_rate': 0.09413703406189669, 'n_estimators': 1000, 'num_leaves': 46, 'max_depth': 3, 'min_child_samples': 58, 'subsample': 0.9735640930621176, 'colsample_bytree': 0.7673244192630848, 'reg_alpha': 0.0015647950246898239, 'reg_lambda': 0.8101170186304166}. Best is trial 10 with value: 0.6903705526308561.\n",
      "[I 2025-12-13 00:01:58,180] Trial 16 finished with value: 0.6922481497609045 and parameters: {'learning_rate': 0.08575545769868445, 'n_estimators': 1250, 'num_leaves': 44, 'max_depth': 10, 'min_child_samples': 20, 'subsample': 0.8011474919442853, 'colsample_bytree': 0.6215956362140892, 'reg_alpha': 0.0007627929780991444, 'reg_lambda': 0.02408607126346641}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:03,201] Trial 17 finished with value: 0.68807982233714 and parameters: {'learning_rate': 0.02730854779469642, 'n_estimators': 1250, 'num_leaves': 82, 'max_depth': 11, 'min_child_samples': 11, 'subsample': 0.8568789995769479, 'colsample_bytree': 0.6149894553975629, 'reg_alpha': 2.7520046066920936e-05, 'reg_lambda': 0.006896799219387559}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:04,530] Trial 18 finished with value: 0.6828977106680748 and parameters: {'learning_rate': 0.23704054407462755, 'n_estimators': 1650, 'num_leaves': 71, 'max_depth': 7, 'min_child_samples': 9, 'subsample': 0.7549864300061877, 'colsample_bytree': 0.63256145239932, 'reg_alpha': 0.021331847889841216, 'reg_lambda': 1.0204401363513702}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:08,966] Trial 19 finished with value: 0.6704331664447297 and parameters: {'learning_rate': 0.0041538816506271085, 'n_estimators': 800, 'num_leaves': 115, 'max_depth': 10, 'min_child_samples': 81, 'subsample': 0.669832907231363, 'colsample_bytree': 0.7539796407337818, 'reg_alpha': 0.00033191986759026506, 'reg_lambda': 0.007838334668939904}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:10,043] Trial 20 finished with value: 0.6744102931882728 and parameters: {'learning_rate': 0.25912814130174344, 'n_estimators': 1550, 'num_leaves': 45, 'max_depth': 9, 'min_child_samples': 66, 'subsample': 0.8514999910012987, 'colsample_bytree': 0.6287320449895333, 'reg_alpha': 0.02480532206241855, 'reg_lambda': 0.00013948534993403827}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:10,926] Trial 21 finished with value: 0.6842459842933436 and parameters: {'learning_rate': 0.09760907522108013, 'n_estimators': 1400, 'num_leaves': 20, 'max_depth': 9, 'min_child_samples': 63, 'subsample': 0.8096525266055377, 'colsample_bytree': 0.8178063737811927, 'reg_alpha': 0.04374138563336756, 'reg_lambda': 0.3704210617183264}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:13,248] Trial 22 finished with value: 0.6854173115969423 and parameters: {'learning_rate': 0.035732217279955514, 'n_estimators': 800, 'num_leaves': 43, 'max_depth': 11, 'min_child_samples': 64, 'subsample': 0.841556589506664, 'colsample_bytree': 0.6217451657762284, 'reg_alpha': 0.16040751544184992, 'reg_lambda': 5.734434738803019}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:15,248] Trial 23 finished with value: 0.68387783173088 and parameters: {'learning_rate': 0.16730375431834518, 'n_estimators': 1050, 'num_leaves': 77, 'max_depth': 12, 'min_child_samples': 15, 'subsample': 0.949667043234508, 'colsample_bytree': 0.7959221319035937, 'reg_alpha': 0.026639223646907195, 'reg_lambda': 0.9076939880244435}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:16,460] Trial 24 finished with value: 0.6838696395210557 and parameters: {'learning_rate': 0.07806421010770301, 'n_estimators': 800, 'num_leaves': 128, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.8540116911554982, 'colsample_bytree': 0.9201526156971491, 'reg_alpha': 0.008491274431530436, 'reg_lambda': 0.009236729184032767}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:17,274] Trial 25 finished with value: 0.6878129481795937 and parameters: {'learning_rate': 0.25418852456756397, 'n_estimators': 1900, 'num_leaves': 28, 'max_depth': 5, 'min_child_samples': 19, 'subsample': 0.8684879799272418, 'colsample_bytree': 0.8243343512070822, 'reg_alpha': 1.971251159284358, 'reg_lambda': 6.585175986885216}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:18,089] Trial 26 finished with value: 0.6916907749202593 and parameters: {'learning_rate': 0.16284428986840116, 'n_estimators': 800, 'num_leaves': 21, 'max_depth': 12, 'min_child_samples': 15, 'subsample': 0.9431780037354707, 'colsample_bytree': 0.827072029356449, 'reg_alpha': 3.0294065216340884e-06, 'reg_lambda': 0.01140606952825825}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:18,777] Trial 27 finished with value: 0.6886949134836908 and parameters: {'learning_rate': 0.1486425879530082, 'n_estimators': 850, 'num_leaves': 17, 'max_depth': 11, 'min_child_samples': 12, 'subsample': 0.9151398178477523, 'colsample_bytree': 0.8458127512930124, 'reg_alpha': 6.3381608306265775e-06, 'reg_lambda': 0.0008157827240381553}. Best is trial 16 with value: 0.6922481497609045.\n",
      "[I 2025-12-13 00:02:20,251] Trial 28 finished with value: 0.6938208627502066 and parameters: {'learning_rate': 0.05258869785011397, 'n_estimators': 1250, 'num_leaves': 31, 'max_depth': 11, 'min_child_samples': 16, 'subsample': 0.6951833272024068, 'colsample_bytree': 0.6706115018316209, 'reg_alpha': 0.004129846644690991, 'reg_lambda': 0.010239040908764659}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:21,472] Trial 29 finished with value: 0.6671055396018797 and parameters: {'learning_rate': 0.22061992614987413, 'n_estimators': 1100, 'num_leaves': 47, 'max_depth': 11, 'min_child_samples': 66, 'subsample': 0.600214503205595, 'colsample_bytree': 0.7245282476208509, 'reg_alpha': 0.03213124334876901, 'reg_lambda': 0.04052126891073338}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:22,583] Trial 30 finished with value: 0.6812400858897406 and parameters: {'learning_rate': 0.21750001414567846, 'n_estimators': 1250, 'num_leaves': 57, 'max_depth': 11, 'min_child_samples': 99, 'subsample': 0.750085956467876, 'colsample_bytree': 0.6694951541912618, 'reg_alpha': 1.0628145940547152e-06, 'reg_lambda': 0.5115045364786938}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:24,612] Trial 31 finished with value: 0.6902065955289735 and parameters: {'learning_rate': 0.03233090088537231, 'n_estimators': 1650, 'num_leaves': 35, 'max_depth': 9, 'min_child_samples': 16, 'subsample': 0.6371691651884722, 'colsample_bytree': 0.6746204629456275, 'reg_alpha': 0.0014042378303970207, 'reg_lambda': 0.22506564908632978}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:26,962] Trial 32 finished with value: 0.6896578826793494 and parameters: {'learning_rate': 0.013071442146130412, 'n_estimators': 850, 'num_leaves': 18, 'max_depth': 11, 'min_child_samples': 12, 'subsample': 0.6381180100865372, 'colsample_bytree': 0.6292513715783437, 'reg_alpha': 5.69979508819143e-05, 'reg_lambda': 0.0011334085371152427}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:27,757] Trial 33 finished with value: 0.6822627441612503 and parameters: {'learning_rate': 0.11444850065152355, 'n_estimators': 1300, 'num_leaves': 18, 'max_depth': 8, 'min_child_samples': 21, 'subsample': 0.9874976510465169, 'colsample_bytree': 0.8671011027261047, 'reg_alpha': 0.015171555529654436, 'reg_lambda': 0.0022811893100679486}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:29,517] Trial 34 finished with value: 0.6856915442431077 and parameters: {'learning_rate': 0.027960846274539347, 'n_estimators': 1300, 'num_leaves': 27, 'max_depth': 10, 'min_child_samples': 34, 'subsample': 0.6062931091918523, 'colsample_bytree': 0.6409545961892426, 'reg_alpha': 2.139818790073423, 'reg_lambda': 0.0037943680329308002}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:30,508] Trial 35 finished with value: 0.6851597158603067 and parameters: {'learning_rate': 0.11162959077193219, 'n_estimators': 800, 'num_leaves': 24, 'max_depth': 12, 'min_child_samples': 92, 'subsample': 0.9349306107832321, 'colsample_bytree': 0.744864031750056, 'reg_alpha': 1.7713101811620652e-06, 'reg_lambda': 0.0017140406706881565}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:33,935] Trial 36 finished with value: 0.68169837118328 and parameters: {'learning_rate': 0.03694454263246195, 'n_estimators': 1550, 'num_leaves': 67, 'max_depth': 10, 'min_child_samples': 7, 'subsample': 0.7288597576630284, 'colsample_bytree': 0.7467294260064532, 'reg_alpha': 0.002582716689714694, 'reg_lambda': 0.0001236248869440847}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:35,723] Trial 37 finished with value: 0.6896891105382565 and parameters: {'learning_rate': 0.03919747435406795, 'n_estimators': 1350, 'num_leaves': 28, 'max_depth': 12, 'min_child_samples': 10, 'subsample': 0.764202665484915, 'colsample_bytree': 0.7011778751927655, 'reg_alpha': 0.00043820226410022005, 'reg_lambda': 0.027414067805541266}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:37,087] Trial 38 finished with value: 0.6798963755015062 and parameters: {'learning_rate': 0.06305450735616885, 'n_estimators': 1250, 'num_leaves': 58, 'max_depth': 7, 'min_child_samples': 95, 'subsample': 0.8331659515167733, 'colsample_bytree': 0.632376825004034, 'reg_alpha': 0.0007466973071783748, 'reg_lambda': 0.051682747237576744}. Best is trial 28 with value: 0.6938208627502066.\n",
      "[I 2025-12-13 00:02:38,777] Trial 39 finished with value: 0.6826647554120281 and parameters: {'learning_rate': 0.062395962501117716, 'n_estimators': 950, 'num_leaves': 31, 'max_depth': 11, 'min_child_samples': 56, 'subsample': 0.9984058821261149, 'colsample_bytree': 0.8973635498578505, 'reg_alpha': 4.748326663338274e-06, 'reg_lambda': 1.463803316427541}. Best is trial 28 with value: 0.6938208627502066.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA][LGBM] Mejor AP(val): 0.693821\n",
      "[EXPORT][LGBM] LGBM_REDUCED_SMOTENC_TUNED -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_REDUCED_SMOTENC/export\n"
     ]
    }
   ],
   "source": [
    "def tune_xgb_with_optuna(seed_params, X_train_final, y_train_final, X_val_fit, y_val, feature_names_used):\n",
    "    \"\"\"\n",
    "    Tuning de XGBoost maximizando AP(val) con Optuna.\n",
    "    Devuelve:\n",
    "      - adapter entrenado (XGBAdapter)\n",
    "      - best_params (dict con hiperparámetros finales)\n",
    "    \"\"\"\n",
    "    N_TRIALS = 40\n",
    "    STUDY_NAME = f\"XGB_{VIEW_TAG}_{BAL_TAG}_AP\"\n",
    "    sampler = TPESampler(seed=RANDOM_STATE, multivariate=True, group=False)\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                study_name=STUDY_NAME,\n",
    "                                sampler=sampler)\n",
    "\n",
    "    SEARCH_KEYS = [\n",
    "        \"learning_rate\", \"n_estimators\", \"max_depth\", \"min_child_weight\",\n",
    "        \"subsample\", \"colsample_bytree\", \"gamma\", \"reg_alpha\", \"reg_lambda\"\n",
    "    ]\n",
    "\n",
    "    best_fp = _best_file(\"xgb\")\n",
    "    if best_fp.exists():\n",
    "        try:\n",
    "            prev = json.loads(best_fp.read_text())\n",
    "            warm = {k: prev[k] for k in SEARCH_KEYS if k in prev}\n",
    "            if warm:\n",
    "                print(\"[OPTUNA][XGB] Enqueue BEST anterior.\")\n",
    "                study.enqueue_trial(warm)\n",
    "        except Exception as e:\n",
    "            print(\"[OPTUNA][XGB] Aviso warm-start:\", e)\n",
    "\n",
    "    def suggest(trial):\n",
    "        p = {}\n",
    "        p[\"learning_rate\"]    = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "        p[\"n_estimators\"]     = trial.suggest_int(\"n_estimators\", 800, 3000, step=50)\n",
    "        p[\"max_depth\"]        = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "        p[\"min_child_weight\"] = trial.suggest_float(\"min_child_weight\", 0.5, 20.0, log=True)\n",
    "        p[\"subsample\"]        = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        p[\"colsample_bytree\"] = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
    "        p[\"gamma\"]            = trial.suggest_float(\"gamma\", 1e-8, 5.0, log=True)\n",
    "        p[\"reg_alpha\"]        = trial.suggest_float(\"reg_alpha\", 1e-6, 10.0, log=True)\n",
    "        p[\"reg_lambda\"]       = trial.suggest_float(\"reg_lambda\", 1e-6, 10.0, log=True)\n",
    "        # Fijos\n",
    "        p[\"random_state\"]     = RANDOM_STATE\n",
    "        p[\"n_jobs\"]           = -1\n",
    "        p[\"eval_metric\"]      = \"aucpr\"\n",
    "        p[\"tree_method\"]      = \"hist\"\n",
    "        p[\"verbosity\"]        = 0\n",
    "        return p\n",
    "\n",
    "    def objective(trial):\n",
    "        hp = suggest(trial)\n",
    "        mdl = XGBClassifier(**{**seed_params, **hp})\n",
    "        mdl = xgb_fit_with_es(\n",
    "            mdl,\n",
    "            X_train_final, y_train_final,\n",
    "            X_val_fit, y_val,\n",
    "            feature_names=feature_names_used,\n",
    "            rounds=200,\n",
    "            verbose=False\n",
    "        )\n",
    "        proba_val = mdl.predict_proba(X_val_fit)[:, 1]\n",
    "        ap = average_precision_score(y_val, proba_val)\n",
    "        trial.set_user_attr(\"best_iteration\", getattr(mdl, \"best_iteration\", None))\n",
    "        return ap\n",
    "\n",
    "    print(f\"[OPTUNA][XGB] Iniciando {N_TRIALS} pruebas...\")\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "    best = study.best_trial\n",
    "    print(f\"[OPTUNA][XGB] Mejor AP(val): {best.value:.6f}\")\n",
    "\n",
    "    best_params = dict(best.params)\n",
    "    best_params.update({\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"n_jobs\": -1,\n",
    "        \"eval_metric\": \"aucpr\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"verbosity\": 0,\n",
    "    })\n",
    "\n",
    "    # Guardamos BEST global\n",
    "    with open(best_fp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Re-entrenamos con los mejores hiperparámetros\n",
    "    tuned_clf = XGBClassifier(**{**seed_params, **best_params})\n",
    "    tuned_adapter = xgb_fit_with_es(\n",
    "        tuned_clf,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names=feature_names_used,\n",
    "        rounds=200,\n",
    "        verbose=False\n",
    "    )\n",
    "    return tuned_adapter, best_params\n",
    "\n",
    "\n",
    "def tune_lgbm_with_optuna(seed_params, X_train_final, y_train_final, X_val_fit, y_val, feature_names_used):\n",
    "    N_TRIALS = 40\n",
    "    STUDY_NAME = f\"LGBM_{VIEW_TAG}_{BAL_TAG}_AP\"\n",
    "    sampler = TPESampler(seed=RANDOM_STATE, multivariate=True, group=False)\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                study_name=STUDY_NAME,\n",
    "                                sampler=sampler)\n",
    "\n",
    "    SEARCH_KEYS = [\n",
    "        \"learning_rate\", \"n_estimators\", \"num_leaves\", \"max_depth\",\n",
    "        \"min_child_samples\", \"subsample\", \"colsample_bytree\",\n",
    "        \"reg_alpha\", \"reg_lambda\"\n",
    "    ]\n",
    "\n",
    "    best_fp = _best_file(\"lgbm\")\n",
    "    if best_fp.exists():\n",
    "        try:\n",
    "            prev = json.loads(best_fp.read_text())\n",
    "            warm = {k: prev[k] for k in SEARCH_KEYS if k in prev}\n",
    "            if warm:\n",
    "                print(\"[OPTUNA][LGBM] Enqueue BEST anterior.\")\n",
    "                study.enqueue_trial(warm)\n",
    "        except Exception as e:\n",
    "            print(\"[OPTUNA][LGBM] Aviso warm-start:\", e)\n",
    "\n",
    "    def suggest(trial):\n",
    "        p = {}\n",
    "        p[\"learning_rate\"]     = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "        p[\"n_estimators\"]      = trial.suggest_int(\"n_estimators\", 800, 3000, step=50)\n",
    "        p[\"num_leaves\"]        = trial.suggest_int(\"num_leaves\", 16, 256)\n",
    "        p[\"max_depth\"]         = trial.suggest_int(\"max_depth\", -1, 12)\n",
    "        p[\"min_child_samples\"] = trial.suggest_int(\"min_child_samples\", 5, 200)\n",
    "        p[\"subsample\"]         = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        p[\"colsample_bytree\"]  = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
    "        p[\"reg_alpha\"]         = trial.suggest_float(\"reg_alpha\", 1e-6, 10.0, log=True)\n",
    "        p[\"reg_lambda\"]        = trial.suggest_float(\"reg_lambda\", 1e-6, 10.0, log=True)\n",
    "        return p\n",
    "\n",
    "    def objective(trial):\n",
    "        hp = suggest(trial)\n",
    "        params = {**seed_params, **hp}\n",
    "        params.setdefault(\"objective\", \"binary\")\n",
    "        params.setdefault(\"metric\", \"average_precision\")\n",
    "        params.setdefault(\"random_state\", RANDOM_STATE)\n",
    "        params.setdefault(\"n_jobs\", -1)\n",
    "\n",
    "        base_clf = LGBMClassifier(**params)\n",
    "        adapter = lgbm_fit_with_es(\n",
    "            base_clf,\n",
    "            X_train_final, y_train_final,\n",
    "            X_val_fit, y_val,\n",
    "            feature_names=feature_names_used,\n",
    "            rounds=200,\n",
    "            verbose=False\n",
    "        )\n",
    "        proba_val = adapter.predict_proba(X_val_fit)[:, 1]\n",
    "        ap = average_precision_score(y_val, proba_val)\n",
    "        trial.set_user_attr(\"best_iteration\", adapter.best_iteration)\n",
    "        return ap\n",
    "\n",
    "    print(f\"[OPTUNA][LGBM] Iniciando {N_TRIALS} pruebas...\")\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "    best = study.best_trial\n",
    "    print(f\"[OPTUNA][LGBM] Mejor AP(val): {best.value:.6f}\")\n",
    "\n",
    "    best_params = dict(best.params)\n",
    "    best_params.update({\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"average_precision\",\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"n_jobs\": -1,\n",
    "    })\n",
    "\n",
    "    # Guardamos BEST global\n",
    "    with open(best_fp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    tuned_base = LGBMClassifier(**{**seed_params, **best_params})\n",
    "    tuned_adapter = lgbm_fit_with_es(\n",
    "        tuned_base,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names=feature_names_used,\n",
    "        rounds=200,\n",
    "        verbose=False\n",
    "    )\n",
    "    return tuned_adapter, best_params\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#   EJECUCIÓN DEL TUNING + RE-ENTRENO + GUARDADO DE preds_val_*\n",
    "# ============================================================\n",
    "\n",
    "# Por si acaso, definimos nombres base\n",
    "EXP_NAME_XGB = f\"XGB_{VIEW_TAG}_{BAL_TAG}\"\n",
    "EXP_NAME_LGB = f\"LGBM_{VIEW_TAG}_{BAL_TAG}\"\n",
    "\n",
    "# Directorios para XGB\n",
    "bx = DIRS[\"xgb\"]\n",
    "OUT_RESULTS_X = bx / \"results\"\n",
    "OUT_FIGS_X    = bx / \"figs\"\n",
    "OUT_PREDS_X   = bx / \"preds\"\n",
    "OUT_PARAMS_X  = bx / \"best_params\"\n",
    "OUT_EXPORT_X  = bx / \"export\"\n",
    "\n",
    "# Directorios para LGBM\n",
    "bl = DIRS[\"lgbm\"]\n",
    "OUT_RESULTS_L = bl / \"results\"\n",
    "OUT_FIGS_L    = bl / \"figs\"\n",
    "OUT_PREDS_L   = bl / \"preds\"\n",
    "OUT_PARAMS_L  = bl / \"best_params\"\n",
    "OUT_EXPORT_L  = bl / \"export\"\n",
    "\n",
    "# === XGB TUNED ===\n",
    "if DO_TUNE_XGB:\n",
    "    xgb_tuned_adapter, xgb_best_params = tune_xgb_with_optuna(\n",
    "        xgb_params_seed,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names_used\n",
    "    )\n",
    "    xgb_tuned_model = xgb_tuned_adapter  \n",
    "\n",
    "    # Predicciones y métricas en VAL\n",
    "    proba_val_xgb_tuned = xgb_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "    thr_val_xgb_tuned, _ = find_best_threshold(y_val, proba_val_xgb_tuned, metric=\"f1\")\n",
    "    val_metrics_xgb_tuned = compute_all_metrics(y_val, proba_val_xgb_tuned, thr_val_xgb_tuned)\n",
    "\n",
    "    # Predicciones y métricas en TEST\n",
    "    proba_test_xgb_tuned = xgb_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "    test_metrics_xgb_tuned = compute_all_metrics(y_test, proba_test_xgb_tuned, thr_val_xgb_tuned)\n",
    "    y_pred_test_xgb_tuned = (proba_test_xgb_tuned >= thr_val_xgb_tuned).astype(int)\n",
    "\n",
    "    # --- GUARDAMOS preds_val_* y preds_test_* ---\n",
    "    EXP_NAME_XGB_TUNED = f\"{EXP_NAME_XGB}_TUNED\"\n",
    "\n",
    "    pd.DataFrame({\"proba\": proba_val_xgb_tuned, \"y_true\": y_val}).to_parquet(\n",
    "        OUT_PREDS_X / f\"preds_val_{EXP_NAME_XGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame({\"proba\": proba_test_xgb_tuned, \"y_true\": y_test}).to_parquet(\n",
    "        OUT_PREDS_X / f\"preds_test_{EXP_NAME_XGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Guardamos best_params específicos de TUNED\n",
    "    with open(OUT_PARAMS_X / f\"{EXP_NAME_XGB_TUNED}_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(xgb_best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Plots TUNED\n",
    "    plot_pr_curve(y_val,  proba_val_xgb_tuned,\n",
    "                  f\"{EXP_NAME_XGB_TUNED} — PR (val)\",\n",
    "                  OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_pr_val.png\")\n",
    "    plot_pr_curve(y_test, proba_test_xgb_tuned,\n",
    "                  f\"{EXP_NAME_XGB_TUNED} — PR (test)\",\n",
    "                  OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_pr_test.png\")\n",
    "    plot_roc_curve(y_val,  proba_val_xgb_tuned,\n",
    "                   f\"{EXP_NAME_XGB_TUNED} — ROC (val)\",\n",
    "                   OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_roc_val.png\")\n",
    "    plot_roc_curve(y_test, proba_test_xgb_tuned,\n",
    "                   f\"{EXP_NAME_XGB_TUNED} — ROC (test)\",\n",
    "                   OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_roc_test.png\")\n",
    "    plot_confusion(\n",
    "        y_test,\n",
    "        y_pred_test_xgb_tuned,\n",
    "        f\"{EXP_NAME_XGB_TUNED} — Confusion (test @thr={thr_val_xgb_tuned:.3f})\",\n",
    "        OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_cm_test.png\"\n",
    "    )\n",
    "\n",
    "    # Registro tuned.csv\n",
    "    best_iter_tuned_xgb = getattr(xgb_tuned_adapter, \"best_iteration\", None)\n",
    "    row_tuned_xgb = {\n",
    "        \"model\": EXP_NAME_XGB_TUNED,\n",
    "        \"thr_val\": thr_val_xgb_tuned,\n",
    "        \"val_pr_auc\": val_metrics_xgb_tuned[\"pr_auc\"],\n",
    "        \"val_roc_auc\": val_metrics_xgb_tuned[\"roc_auc\"],\n",
    "        \"val_precision\": val_metrics_xgb_tuned[\"precision\"],\n",
    "        \"val_f1\": val_metrics_xgb_tuned[\"f1\"],\n",
    "        \"val_recall\": val_metrics_xgb_tuned[\"recall\"],\n",
    "        \"val_bal_acc\": val_metrics_xgb_tuned[\"bal_acc\"],\n",
    "        \"test_pr_auc\": test_metrics_xgb_tuned[\"pr_auc\"],\n",
    "        \"test_roc_auc\": test_metrics_xgb_tuned[\"roc_auc\"],\n",
    "        \"test_precision\": test_metrics_xgb_tuned[\"precision\"],\n",
    "        \"test_f1\": test_metrics_xgb_tuned[\"f1\"],\n",
    "        \"test_recall\": test_metrics_xgb_tuned[\"recall\"],\n",
    "        \"test_bal_acc\": test_metrics_xgb_tuned[\"bal_acc\"],\n",
    "        \"best_iteration\": best_iter_tuned_xgb if best_iter_tuned_xgb is not None else np.nan,\n",
    "    }\n",
    "    csv_x_tuned = OUT_RESULTS_X / \"tuned.csv\"\n",
    "    pd.DataFrame([row_tuned_xgb]).to_csv(\n",
    "        csv_x_tuned,\n",
    "        mode=(\"a\" if csv_x_tuned.exists() else \"w\"),\n",
    "        index=False,\n",
    "        header=not csv_x_tuned.exists()\n",
    "    )\n",
    "\n",
    "    # Export XGB TUNED\n",
    "    export_xgb(\n",
    "        adapter=xgb_tuned_adapter,\n",
    "        out_dir=OUT_EXPORT_X,\n",
    "        exp_name=EXP_NAME_XGB_TUNED,\n",
    "        feature_names=feature_names_used,\n",
    "        threshold=thr_val_xgb_tuned,\n",
    "        val_metrics=val_metrics_xgb_tuned,\n",
    "        test_metrics=test_metrics_xgb_tuned,\n",
    "    )\n",
    "\n",
    "\n",
    "# === LGBM TUNED ===\n",
    "if DO_TUNE_LGBM:\n",
    "    lgbm_tuned_adapter, lgbm_best_params = tune_lgbm_with_optuna(\n",
    "        lgbm_params_seed,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names_used\n",
    "    )\n",
    "    # Alias de compatibilidad\n",
    "    lgbm_tuned_model = lgbm_tuned_adapter  \n",
    "\n",
    "    # Predicciones y métricas en VAL\n",
    "    proba_val_lgb_tuned = lgbm_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "    thr_val_lgb_tuned, _ = find_best_threshold(y_val, proba_val_lgb_tuned, metric=\"f1\")\n",
    "    val_metrics_lgb_tuned = compute_all_metrics(y_val, proba_val_lgb_tuned, thr_val_lgb_tuned)\n",
    "\n",
    "    # Predicciones y métricas en TEST\n",
    "    proba_test_lgb_tuned = lgbm_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "    test_metrics_lgb_tuned = compute_all_metrics(y_test, proba_test_lgb_tuned, thr_val_lgb_tuned)\n",
    "    y_pred_test_lgb_tuned = (proba_test_lgb_tuned >= thr_val_lgb_tuned).astype(int)\n",
    "\n",
    "    EXP_NAME_LGB_TUNED = f\"{EXP_NAME_LGB}_TUNED\"\n",
    "\n",
    "    pd.DataFrame({\"proba\": proba_val_lgb_tuned, \"y_true\": y_val}).to_parquet(\n",
    "        OUT_PREDS_L / f\"preds_val_{EXP_NAME_LGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame({\"proba\": proba_test_lgb_tuned, \"y_true\": y_test}).to_parquet(\n",
    "        OUT_PREDS_L / f\"preds_test_{EXP_NAME_LGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    with open(OUT_PARAMS_L / f\"{EXP_NAME_LGB_TUNED}_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(lgbm_best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Plots TUNED\n",
    "    plot_pr_curve(y_val,  proba_val_lgb_tuned,\n",
    "                  f\"{EXP_NAME_LGB_TUNED} — PR (val)\",\n",
    "                  OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_pr_val.png\")\n",
    "    plot_pr_curve(y_test, proba_test_lgb_tuned,\n",
    "                  f\"{EXP_NAME_LGB_TUNED} — PR (test)\",\n",
    "                  OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_pr_test.png\")\n",
    "    plot_roc_curve(y_val,  proba_val_lgb_tuned,\n",
    "                   f\"{EXP_NAME_LGB_TUNED} — ROC (val)\",\n",
    "                   OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_roc_val.png\")\n",
    "    plot_roc_curve(y_test, proba_test_lgb_tuned,\n",
    "                   f\"{EXP_NAME_LGB_TUNED} — ROC (test)\",\n",
    "                   OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_roc_test.png\")\n",
    "    plot_confusion(\n",
    "        y_test,\n",
    "        y_pred_test_lgb_tuned,\n",
    "        f\"{EXP_NAME_LGB_TUNED} — Confusion (test @thr={thr_val_lgb_tuned:.3f})\",\n",
    "        OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_cm_test.png\"\n",
    "    )\n",
    "\n",
    "    # Registro tuned.csv\n",
    "    row_tuned_lgb = {\n",
    "        \"model\": EXP_NAME_LGB_TUNED,\n",
    "        \"thr_val\": thr_val_lgb_tuned,\n",
    "        \"val_pr_auc\": val_metrics_lgb_tuned[\"pr_auc\"],\n",
    "        \"val_roc_auc\": val_metrics_lgb_tuned[\"roc_auc\"],\n",
    "        \"val_precision\": val_metrics_lgb_tuned[\"precision\"],\n",
    "        \"val_f1\": val_metrics_lgb_tuned[\"f1\"],\n",
    "        \"val_recall\": val_metrics_lgb_tuned[\"recall\"],\n",
    "        \"val_bal_acc\": val_metrics_lgb_tuned[\"bal_acc\"],\n",
    "        \"test_pr_auc\": test_metrics_lgb_tuned[\"pr_auc\"],\n",
    "        \"test_roc_auc\": test_metrics_lgb_tuned[\"roc_auc\"],\n",
    "        \"test_precision\": test_metrics_lgb_tuned[\"precision\"],\n",
    "        \"test_f1\": test_metrics_lgb_tuned[\"f1\"],\n",
    "        \"test_recall\": test_metrics_lgb_tuned[\"recall\"],\n",
    "        \"test_bal_acc\": test_metrics_lgb_tuned[\"bal_acc\"],\n",
    "        \"best_iteration\": lgbm_tuned_adapter.best_iteration if hasattr(lgbm_tuned_adapter, \"best_iteration\") else np.nan,\n",
    "    }\n",
    "    csv_l_tuned = OUT_RESULTS_L / \"tuned.csv\"\n",
    "    pd.DataFrame([row_tuned_lgb]).to_csv(\n",
    "        csv_l_tuned,\n",
    "        mode=(\"a\" if csv_l_tuned.exists() else \"w\"),\n",
    "        index=False,\n",
    "        header=not csv_l_tuned.exists()\n",
    "    )\n",
    "\n",
    "    # Export LGBM TUNED\n",
    "    export_lgbm(\n",
    "        adapter=lgbm_tuned_adapter,\n",
    "        out_dir=OUT_EXPORT_L,\n",
    "        exp_name=EXP_NAME_LGB_TUNED,\n",
    "        feature_names=feature_names_used,\n",
    "        threshold=thr_val_lgb_tuned,\n",
    "        val_metrics=val_metrics_lgb_tuned,\n",
    "        test_metrics=test_metrics_lgb_tuned,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94dfcfe",
   "metadata": {},
   "source": [
    "9 — Ensemble híbrido (stacking + soft voting XGB + LGBM + RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc70a583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SOFT VOTING (val) ===\n",
      "{'pr_auc': 0.6932527718875239, 'roc_auc': 0.8630433206704395, 'precision': 0.6631578947368421, 'f1': 0.6404066073697586, 'recall': 0.6191646191646192, 'bal_acc': 0.7694065405929813}\n",
      "\n",
      "=== SOFT VOTING (test) ===\n",
      "{'pr_auc': 0.7063962571891518, 'roc_auc': 0.8637913722659485, 'precision': 0.6349614395886889, 'f1': 0.6206030150753769, 'recall': 0.6068796068796068, 'bal_acc': 0.7588698097172673}\n",
      "\n",
      "=== STACKING (val) ===\n",
      "{'pr_auc': 0.6967103094287437, 'roc_auc': 0.8628304729999646, 'precision': 0.6929577464788732, 'f1': 0.6456692913385826, 'recall': 0.6044226044226044, 'bal_acc': 0.7679991239313273}\n",
      "\n",
      "=== STACKING (test) ===\n",
      "{'pr_auc': 0.7086679281606992, 'roc_auc': 0.8642263218534405, 'precision': 0.6539509536784741, 'f1': 0.6201550387596899, 'recall': 0.5896805896805897, 'bal_acc': 0.7549783990461956}\n",
      "\n",
      "[OK][ENSEMBLE] Métricas, preds y pesos del modelo híbrido (XGB+LGBM+RF) guardados en:\n",
      "  RESULTS: /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_REDUCED_SMOTENC/results\n",
      "  PREDS:   /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_REDUCED_SMOTENC/preds\n",
      "  PARAMS:  /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_REDUCED_SMOTENC/best_params\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "EXP_NAME_ENS = f\"ENS_{VIEW_TAG}_{BAL_TAG}\"\n",
    "be = DIRS[\"ens\"]\n",
    "OUT_RESULTS_E = be / \"results\"\n",
    "OUT_FIGS_E    = be / \"figs\"\n",
    "OUT_PREDS_E   = be / \"preds\"\n",
    "OUT_PARAMS_E  = be / \"best_params\"\n",
    "\n",
    "OUT_RESULTS_E.mkdir(parents=True, exist_ok=True)\n",
    "OUT_FIGS_E.mkdir(parents=True, exist_ok=True)\n",
    "OUT_PREDS_E.mkdir(parents=True, exist_ok=True)\n",
    "OUT_PARAMS_E.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Probabilidades XGB y LGBM (TUNED) ---\n",
    "proba_val_xgb_tuned  = xgb_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "proba_test_xgb_tuned = xgb_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "\n",
    "proba_val_lgb_tuned  = lgbm_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "proba_test_lgb_tuned = lgbm_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "\n",
    "# --- Cargar preds de Random Forest REDUCED SMOTENC ---\n",
    "EXP_NAME_RF = f\"RF_{VIEW_TAG}_{BAL_TAG}\"\n",
    "br = DIRS[\"rf\"]\n",
    "rf_preds_val_path  = br / \"preds\" / f\"preds_val_{EXP_NAME_RF}_TUNED.parquet\"\n",
    "rf_preds_test_path = br / \"preds\" / f\"preds_test_{EXP_NAME_RF}_TUNED.parquet\"\n",
    "\n",
    "df_rf_val  = pd.read_parquet(rf_preds_val_path)\n",
    "df_rf_test = pd.read_parquet(rf_preds_test_path)\n",
    "\n",
    "proba_val_rf  = df_rf_val[\"proba\"].to_numpy()\n",
    "proba_test_rf = df_rf_test[\"proba\"].to_numpy()\n",
    "\n",
    "assert len(proba_val_rf)  == len(y_val),  \"[RF] Longitud de preds val != y_val\"\n",
    "assert len(proba_test_rf) == len(y_test), \"[RF] Longitud de preds test != y_test\"\n",
    "\n",
    "# --- SOFT VOTING XGB + LGBM + RF ---\n",
    "proba_val_soft = (proba_val_xgb_tuned + proba_val_lgb_tuned + proba_val_rf) / 3.0\n",
    "thr_val_soft, f1_val_soft = find_best_threshold(y_val, proba_val_soft, metric=\"f1\")\n",
    "metrics_val_soft  = compute_all_metrics(y_val,  proba_val_soft,  thr_val_soft)\n",
    "\n",
    "proba_test_soft = (proba_test_xgb_tuned + proba_test_lgb_tuned + proba_test_rf) / 3.0\n",
    "metrics_test_soft = compute_all_metrics(y_test, proba_test_soft, thr_val_soft)\n",
    "y_pred_test_soft  = (proba_test_soft >= thr_val_soft).astype(int)\n",
    "\n",
    "# --- STACKING ---\n",
    "Z_val  = np.column_stack([proba_val_xgb_tuned,  proba_val_lgb_tuned,  proba_val_rf])\n",
    "Z_test = np.column_stack([proba_test_xgb_tuned, proba_test_lgb_tuned, proba_test_rf])\n",
    "\n",
    "stack_clf = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=1.0,\n",
    "    solver=\"liblinear\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "stack_clf.fit(Z_val, y_val)\n",
    "\n",
    "proba_val_stack = stack_clf.predict_proba(Z_val)[:, 1]\n",
    "thr_val_stack, f1_val_stack = find_best_threshold(y_val, proba_val_stack, metric=\"f1\")\n",
    "metrics_val_stack  = compute_all_metrics(y_val,  proba_val_stack,  thr_val_stack)\n",
    "\n",
    "proba_test_stack = stack_clf.predict_proba(Z_test)[:, 1]\n",
    "metrics_test_stack = compute_all_metrics(y_test, proba_test_stack, thr_val_stack)\n",
    "y_pred_test_stack  = (proba_test_stack >= thr_val_stack).astype(int)\n",
    "\n",
    "# --- Plots ---\n",
    "plot_pr_curve(y_val,  proba_val_soft,\n",
    "              f\"{EXP_NAME_ENS}_SOFT — PR (val)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_soft,\n",
    "              f\"{EXP_NAME_ENS}_SOFT — PR (test)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_pr_test.png\")\n",
    "\n",
    "plot_pr_curve(y_val,  proba_val_stack,\n",
    "              f\"{EXP_NAME_ENS}_STACK — PR (val)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_stack,\n",
    "              f\"{EXP_NAME_ENS}_STACK — PR (test)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_pr_test.png\")\n",
    "\n",
    "plot_roc_curve(y_val,  proba_val_soft,\n",
    "               f\"{EXP_NAME_ENS}_SOFT — ROC (val)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_soft,\n",
    "               f\"{EXP_NAME_ENS}_SOFT — ROC (test)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_roc_test.png\")\n",
    "\n",
    "plot_roc_curve(y_val,  proba_val_stack,\n",
    "               f\"{EXP_NAME_ENS}_STACK — ROC (val)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_stack,\n",
    "               f\"{EXP_NAME_ENS}_STACK — ROC (test)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_roc_test.png\")\n",
    "\n",
    "plot_confusion(y_test, y_pred_test_soft,\n",
    "               f\"{EXP_NAME_ENS}_SOFT — Confusion (test @thr={thr_val_soft:.3f})\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_cm_test.png\")\n",
    "\n",
    "plot_confusion(y_test, y_pred_test_stack,\n",
    "               f\"{EXP_NAME_ENS}_STACK — Confusion (test @thr={thr_val_stack:.3f})\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_cm_test.png\")\n",
    "\n",
    "# --- Resumen en JSON ---\n",
    "ens_summary = {\n",
    "    \"model\": EXP_NAME_ENS,\n",
    "    \"stack\": {\n",
    "        \"thr_val\": float(thr_val_stack),\n",
    "        \"metrics_val\":  _floatify_metrics(metrics_val_stack),\n",
    "        \"metrics_test\": _floatify_metrics(metrics_test_stack),\n",
    "    },\n",
    "    \"soft_voting\": {\n",
    "        \"thr_val\": float(thr_val_soft),\n",
    "        \"metrics_val\":  _floatify_metrics(metrics_val_soft),\n",
    "        \"metrics_test\": _floatify_metrics(metrics_test_soft),\n",
    "    }\n",
    "}\n",
    "_json_dump(OUT_RESULTS_E / f\"{EXP_NAME_ENS}_metrics.json\", ens_summary)\n",
    "\n",
    "# --- CSV ensembles.csv ---\n",
    "row_stack = {\n",
    "    \"model\": f\"{EXP_NAME_ENS}_STACK\",\n",
    "    \"thr_val\":        thr_val_stack,\n",
    "    \"val_pr_auc\":     metrics_val_stack[\"pr_auc\"],\n",
    "    \"val_roc_auc\":    metrics_val_stack[\"roc_auc\"],\n",
    "    \"val_precision\":  metrics_val_stack[\"precision\"],\n",
    "    \"val_f1\":         metrics_val_stack[\"f1\"],\n",
    "    \"val_recall\":     metrics_val_stack[\"recall\"],\n",
    "    \"val_bal_acc\":    metrics_val_stack[\"bal_acc\"],\n",
    "    \"test_pr_auc\":    metrics_test_stack[\"pr_auc\"],\n",
    "    \"test_roc_auc\":   metrics_test_stack[\"roc_auc\"],\n",
    "    \"test_precision\": metrics_test_stack[\"precision\"],\n",
    "    \"test_f1\":        metrics_test_stack[\"f1\"],\n",
    "    \"test_recall\":    metrics_test_stack[\"recall\"],\n",
    "    \"test_bal_acc\":   metrics_test_stack[\"bal_acc\"],\n",
    "}\n",
    "\n",
    "row_soft = {\n",
    "    \"model\": f\"{EXP_NAME_ENS}_SOFT\",\n",
    "    \"thr_val\":        thr_val_soft,\n",
    "    \"val_pr_auc\":     metrics_val_soft[\"pr_auc\"],\n",
    "    \"val_roc_auc\":    metrics_val_soft[\"roc_auc\"],\n",
    "    \"val_precision\":  metrics_val_soft[\"precision\"],\n",
    "    \"val_f1\":         metrics_val_soft[\"f1\"],\n",
    "    \"val_recall\":     metrics_val_soft[\"recall\"],\n",
    "    \"val_bal_acc\":    metrics_val_soft[\"bal_acc\"],\n",
    "    \"test_pr_auc\":    metrics_test_soft[\"pr_auc\"],\n",
    "    \"test_roc_auc\":   metrics_test_soft[\"roc_auc\"],\n",
    "    \"test_precision\": metrics_test_soft[\"precision\"],\n",
    "    \"test_f1\":        metrics_test_soft[\"f1\"],\n",
    "    \"test_recall\":    metrics_test_soft[\"recall\"],\n",
    "    \"test_bal_acc\":   metrics_test_soft[\"bal_acc\"],\n",
    "}\n",
    "\n",
    "csv_e = OUT_RESULTS_E / \"ensembles.csv\"\n",
    "pd.DataFrame([row_stack, row_soft]).to_csv(\n",
    "    csv_e,\n",
    "    mode=(\"a\" if csv_e.exists() else \"w\"),\n",
    "    index=False,\n",
    "    header=not csv_e.exists()\n",
    ")\n",
    "\n",
    "# --- Parámetros del meta-learner ---\n",
    "stack_params = {\n",
    "    \"type\": \"LogisticRegression\",\n",
    "    \"penalty\": stack_clf.penalty,\n",
    "    \"C\": float(stack_clf.C),\n",
    "    \"solver\": stack_clf.solver,\n",
    "    \"feature_names_level1\": [\"xgb_tuned_proba\", \"lgbm_tuned_proba\", \"rf_proba\"],\n",
    "    \"coef_\": stack_clf.coef_[0].tolist(),\n",
    "    \"intercept_\": float(stack_clf.intercept_[0]),\n",
    "}\n",
    "_json_dump(OUT_PARAMS_E / f\"BEST_{EXP_NAME_ENS}.json\", stack_params)\n",
    "\n",
    "# --- Guardar preds ---\n",
    "pd.DataFrame({\n",
    "    \"p_xgb\":   proba_val_xgb_tuned,\n",
    "    \"p_lgbm\":  proba_val_lgb_tuned,\n",
    "    \"p_rf\":    proba_val_rf,\n",
    "    \"p_soft\":  proba_val_soft,\n",
    "    \"p_stack\": proba_val_stack,\n",
    "    \"y_true\":  y_val,\n",
    "}).to_parquet(OUT_PREDS_E / f\"preds_val_{EXP_NAME_ENS}.parquet\", index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"p_xgb\":   proba_test_xgb_tuned,\n",
    "    \"p_lgbm\":  proba_test_lgb_tuned,\n",
    "    \"p_rf\":    proba_test_rf,\n",
    "    \"p_soft\":  proba_test_soft,\n",
    "    \"p_stack\": proba_test_stack,\n",
    "    \"y_true\":  y_test,\n",
    "}).to_parquet(OUT_PREDS_E / f\"preds_test_{EXP_NAME_ENS}.parquet\", index=False)\n",
    "\n",
    "print(\"\\n=== SOFT VOTING (val) ===\")\n",
    "print(metrics_val_soft)\n",
    "print(\"\\n=== SOFT VOTING (test) ===\")\n",
    "print(metrics_test_soft)\n",
    "\n",
    "print(\"\\n=== STACKING (val) ===\")\n",
    "print(metrics_val_stack)\n",
    "print(\"\\n=== STACKING (test) ===\")\n",
    "print(metrics_test_stack)\n",
    "\n",
    "print(\"\\n[OK][ENSEMBLE] Métricas, preds y pesos del modelo híbrido (XGB+LGBM+RF) guardados en:\")\n",
    "print(\"  RESULTS:\", OUT_RESULTS_E)\n",
    "print(\"  PREDS:  \", OUT_PREDS_E)\n",
    "print(\"  PARAMS: \", OUT_PARAMS_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f7b15",
   "metadata": {},
   "source": [
    "10 - Stacking con OOF (CV=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ead0c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[WARN][_find_one] múltiples matches para patrón 'oof_RF_*_TUNED_CV5.parquet':\n",
      "   - /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/RF_FULL_SMOTENC/preds/oof_RF_FULL_SMOTENC_TUNED_CV5.parquet\n",
      "   - /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/RF_REDUCED_SMOTENC/preds/oof_RF_REDUCED_SMOTENC_TUNED_CV5.parquet\n",
      "[RF OOF] Columnas encontradas en el parquet: Index(['oof_proba', 'y_true'], dtype='object')\n",
      "[RF OOF] Usando columna 'oof_proba' como probabilidad.\n",
      "[STACK OOF] Usando SOLO TRAIN para meta-learner (len = 6000 ).\n",
      "[STACK OOF] Mejor C para meta-learner: 10.0000 | AP OOF: 0.70110\n",
      "[STACK OOF] OK — resultados de VAL y TEST guardados.\n"
     ]
    }
   ],
   "source": [
    "DO_FULL_STACKING_OOF = True \n",
    "\n",
    "def _find_one(pattern, base=None):\n",
    "    \n",
    "    base = Path(base or ARTIF_ROOT)\n",
    "    matches = list(base.rglob(pattern))\n",
    "    if not matches:\n",
    "        return None\n",
    "    if len(matches) > 1:\n",
    "        print(f\"[WARN][_find_one] múltiples matches para patrón '{pattern}':\")\n",
    "        for m in matches:\n",
    "            print(\"   -\", m)\n",
    "    return matches[0]\n",
    "\n",
    "if DO_FULL_STACKING_OOF:\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    def oof_preds_tree(model_builder, X, y, name):\n",
    "        oof = np.zeros(len(y), dtype=float)\n",
    "        models = []\n",
    "        for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "            X_tr, y_tr = X[tr], y[tr]\n",
    "            X_va, y_va = X[va], y[va]\n",
    "            if USE_BALANCED_TRAIN:\n",
    "                X_tr, y_tr = maybe_smote(X_tr, y_tr, keep_idx=keep_idx_global)\n",
    "            mdl = model_builder()\n",
    "            if name == \"xgb\":\n",
    "                mdl = xgb_fit_with_es(\n",
    "                    mdl,\n",
    "                    X_tr, y_tr,\n",
    "                    X_va, y_va,\n",
    "                    feature_names=feature_names_used,\n",
    "                    rounds=200,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                mdl = lgbm_fit_with_es(\n",
    "                    mdl,\n",
    "                    X_tr, y_tr,\n",
    "                    X_va, y_va,\n",
    "                    feature_names=feature_names_used,\n",
    "                    rounds=200,\n",
    "                    verbose=False\n",
    "                )\n",
    "            oof[va] = mdl.predict_proba(X_va)[:, 1]\n",
    "            models.append(mdl)\n",
    "        return oof, models\n",
    "\n",
    "    # --- Builders de los modelos base ---\n",
    "    xgb_hp = xgb_best_params if ('xgb_best_params' in locals() and xgb_best_params is not None) else xgb_params_seed\n",
    "    def build_xgb():\n",
    "        return XGBClassifier(\n",
    "            **{\n",
    "                **xgb_hp,\n",
    "                \"n_jobs\": -1,\n",
    "                \"eval_metric\": \"aucpr\",\n",
    "                \"tree_method\": \"hist\",\n",
    "                \"verbosity\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    lgbm_hp = lgbm_best_params if ('lgbm_best_params' in locals() and lgbm_best_params is not None) else lgbm_params_seed\n",
    "    def build_lgb():\n",
    "        return LGBMClassifier(\n",
    "            **{\n",
    "                **lgbm_hp,\n",
    "                \"metric\": \"average_precision\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # --- OOF XGB / LGBM sobre TRAIN+VAL (L1) ---\n",
    "    X_all = np.vstack([X_train_fit, X_val_fit])\n",
    "    y_all = np.concatenate([y_train, y_val])\n",
    "\n",
    "    oof_xgb, xgb_models = oof_preds_tree(build_xgb, X_all, y_all, \"xgb\")\n",
    "    oof_lgb, lgb_models = oof_preds_tree(build_lgb, X_all, y_all, \"lgbm\")\n",
    "\n",
    "    # --- OOF RF ---\n",
    "    rf_oof_fp = _find_one(\"oof_RF_*_TUNED_CV5.parquet\") or _find_one(\"oof_RF_*_CV5.parquet\")\n",
    "    if rf_oof_fp is None:\n",
    "        raise FileNotFoundError(\"[RF] No se encontró oof del RF (oof_RF_*_CV5.parquet).\")\n",
    "\n",
    "    df_oof_rf = pd.read_parquet(rf_oof_fp)\n",
    "    print(\"[RF OOF] Columnas encontradas en el parquet:\", df_oof_rf.columns)\n",
    "\n",
    "    candidate_cols = [c for c in df_oof_rf.columns if c.lower() not in (\"y_true\", \"y\", \"target\", \"label\")]\n",
    "    if len(candidate_cols) != 1:\n",
    "        raise ValueError(f\"[RF] No puedo identificar de forma única la columna de probas. Columnas: {df_oof_rf.columns}\")\n",
    "    proba_col_rf = candidate_cols[0]\n",
    "    print(f\"[RF OOF] Usando columna '{proba_col_rf}' como probabilidad.\")\n",
    "\n",
    "    oof_rf = df_oof_rf[proba_col_rf].to_numpy()\n",
    "\n",
    "    # --- Alinear longitudes ---\n",
    "    n_rf    = len(oof_rf)\n",
    "    n_all   = len(y_all)\n",
    "    n_train = len(y_train)\n",
    "\n",
    "    if n_rf == n_all:\n",
    "        oof_xgb_meta = oof_xgb\n",
    "        oof_lgb_meta = oof_lgb\n",
    "        y_meta = y_all\n",
    "        print(\"[STACK OOF] Usando TRAIN+VAL para meta-learner (len =\", n_rf, \").\")\n",
    "    elif n_rf == n_train:\n",
    "        oof_xgb_meta = oof_xgb[:n_rf]\n",
    "        oof_lgb_meta = oof_lgb[:n_rf]\n",
    "        y_meta = y_train\n",
    "        print(\"[STACK OOF] Usando SOLO TRAIN para meta-learner (len =\", n_rf, \").\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"[RF] Longitud OOF RF {n_rf} no coincide ni con TRAIN ({n_train}) ni con TRAIN+VAL ({n_all}).\"\n",
    "        )\n",
    "\n",
    "    X_meta_oof = np.column_stack([oof_xgb_meta, oof_lgb_meta, oof_rf])\n",
    "    meta_oof = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        C=1.0,\n",
    "        solver=\"liblinear\"\n",
    "    )\n",
    "\n",
    "    best_c, best_ap = None, -1.0\n",
    "    for c in [0.01, 0.1, 1.0, 3.0, 10.0]:\n",
    "        tmp = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight=\"balanced\",\n",
    "            C=c,\n",
    "            solver=\"liblinear\"\n",
    "        )\n",
    "        tmp.fit(X_meta_oof, y_meta)\n",
    "        ap = average_precision_score(y_meta, tmp.predict_proba(X_meta_oof)[:, 1])\n",
    "        if ap > best_ap:\n",
    "            best_ap, best_c = ap, c\n",
    "\n",
    "    meta_oof.set_params(C=best_c)\n",
    "    meta_oof.fit(X_meta_oof, y_meta)\n",
    "    print(f\"[STACK OOF] Mejor C para meta-learner: {best_c:.4f} | AP OOF: {best_ap:.5f}\")\n",
    "\n",
    "    def fit_full_and_pred(models_builder, name):\n",
    "        mdl = models_builder()\n",
    "        if name == \"xgb\":\n",
    "            mdl = xgb_fit_with_es(\n",
    "                mdl,\n",
    "                X_all, y_all,\n",
    "                X_val_fit, y_val,\n",
    "                feature_names=feature_names_used,\n",
    "                rounds=200,\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            mdl = lgbm_fit_with_es(\n",
    "                mdl,\n",
    "                X_all, y_all,\n",
    "                X_val_fit, y_val,\n",
    "                feature_names=feature_names_used,\n",
    "                rounds=200,\n",
    "                verbose=False\n",
    "            )\n",
    "        return mdl.predict_proba(X_test_fit)[:, 1]\n",
    "\n",
    "    ptest_xgb_full = fit_full_and_pred(build_xgb, \"xgb\")\n",
    "    ptest_lgb_full = fit_full_and_pred(build_lgb, \"lgbm\")\n",
    "\n",
    "    # --- RF TEST ---\n",
    "    if proba_test_rf is None:\n",
    "        raise FileNotFoundError(\"[RF] Falta preds TEST del RF para completar stacking canónico.\")\n",
    "\n",
    "    X_meta_test = np.column_stack([ptest_xgb_full, ptest_lgb_full, proba_test_rf])\n",
    "\n",
    "    X_meta_val = np.column_stack([proba_val_xgb_tuned, proba_val_lgb_tuned, proba_val_rf])\n",
    "    p_val_stack_oof = meta_oof.predict_proba(X_meta_val)[:, 1]\n",
    "\n",
    "    thr_stack_oof, _ = find_best_threshold(y_val, p_val_stack_oof, metric=\"f1\")\n",
    "    metrics_val_stack_oof = compute_all_metrics(y_val, p_val_stack_oof, thr_stack_oof)\n",
    "\n",
    "    # --- PREDICCIONES STACK EN TEST ---\n",
    "    p_test_stack_oof = meta_oof.predict_proba(X_meta_test)[:, 1]\n",
    "    metrics_test_stack_oof = compute_all_metrics(y_test, p_test_stack_oof, thr_stack_oof)\n",
    "\n",
    "    # --- GUARDAR PREDS VAL Y TEST ---\n",
    "    pd.DataFrame({\"proba\": p_val_stack_oof, \"y_true\": y_val}).to_parquet(\n",
    "        OUT_PREDS_E / \"preds_val_ENS_STACK_OOF.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    pd.DataFrame({\"proba\": p_test_stack_oof, \"y_true\": y_test}).to_parquet(\n",
    "        OUT_PREDS_E / \"preds_test_ENS_STACK_OOF.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    with open(OUT_RESULTS_E / \"ensemble_stack_oof_summary.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"thr\": float(thr_stack_oof),\n",
    "                \"val\": {k: float(v) for k, v in metrics_val_stack_oof.items()},\n",
    "                \"test\": {k: float(v) for k, v in metrics_test_stack_oof.items()},\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"[STACK OOF] OK — resultados de VAL y TEST guardados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
