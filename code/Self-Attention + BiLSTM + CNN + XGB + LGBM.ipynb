{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d61c7a4",
   "metadata": {},
   "source": [
    "1 — Imports, config y rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e11fd130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT:      /Users/luistejada/Downloads/TFE Churn Bancario\n",
      "DATA_DIR:  /Users/luistejada/Downloads/TFE Churn Bancario/preproc_datasets/full\n",
      "ARTIF_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/artifacts\n",
      "{'xgb': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_FULL_SMOTENC', 'lgbm': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_FULL_SMOTENC', 'dl': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/DL_FULL_SMOTENC', 'ens': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_FULL_SMOTENC'}\n"
     ]
    }
   ],
   "source": [
    "import json, os, warnings, time, re\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Métricas / CV / Modelos\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, precision_recall_curve, roc_auc_score, roc_curve,\n",
    "    f1_score, recall_score, balanced_accuracy_score, confusion_matrix, precision_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Balanceo\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTENC, SMOTE\n",
    "    _HAS_IMBLEARN = True\n",
    "except Exception:\n",
    "    _HAS_IMBLEARN = False\n",
    "\n",
    "# Modelos base\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Optuna\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# === Toggles ===\n",
    "USE_REDUCED = False\n",
    "USE_BALANCED_TRAIN = True\n",
    "BALANCE_IN_CV = True\n",
    "RANDOM_STATE = 42\n",
    "DO_TUNE_XGB = True\n",
    "DO_TUNE_LGBM = True\n",
    "DO_CV_BASELINE = True\n",
    "DO_CV_TUNED = True\n",
    "CV_FOLDS = 5\n",
    "MI_TOPK = 30\n",
    "\n",
    "# --- Localizador ---\n",
    "def _auto_project_root():\n",
    "    env_root = os.environ.get(\"PROJECT_ROOT\")\n",
    "    if env_root and (Path(env_root)/\"preproc_datasets\"/\"full\").exists():\n",
    "        return Path(env_root)\n",
    "    \n",
    "    here = Path.cwd().resolve()\n",
    "    candidates = [here, here.parent, here.parent.parent, here.parent.parent.parent]\n",
    "    for base in candidates:\n",
    "        if (base/\"preproc_datasets\"/\"full\"/\"X_train_full.npy\").exists():\n",
    "            return base\n",
    "        if (base/\"preproc_datasets\"/\"full\").exists():\n",
    "            return base\n",
    "\n",
    "    home_fallback = Path.home() / \"Downloads\" / \"TFE Churn Bancario\"\n",
    "    if (home_fallback/\"preproc_datasets\"/\"full\").exists():\n",
    "        return home_fallback\n",
    "\n",
    "    return here\n",
    "\n",
    "ROOT = _auto_project_root()\n",
    "\n",
    "# === Rutas ===\n",
    "DATA_DIR = ROOT / \"preproc_datasets\" / (\"reduced\" if USE_REDUCED else \"full\")\n",
    "ARTIF_ROOT = ROOT / \"artifacts\"\n",
    "\n",
    "# Tags\n",
    "VIEW_TAG = \"REDUCED\" if USE_REDUCED else \"FULL\"\n",
    "BAL_TAG  = \"SMOTENC\" if USE_BALANCED_TRAIN else \"IMB\"\n",
    "\n",
    "# Carpetas por modelo y ensamble\n",
    "DIRS = {\n",
    "    \"xgb\":   ARTIF_ROOT / f\"XGB_{VIEW_TAG}_{BAL_TAG}\",\n",
    "    \"lgbm\":  ARTIF_ROOT / f\"LGBM_{VIEW_TAG}_{BAL_TAG}\",\n",
    "    \"dl\":   ARTIF_ROOT / f\"DL_{VIEW_TAG}_{BAL_TAG}\",\n",
    "    \"ens\":   ARTIF_ROOT / f\"ENS_{VIEW_TAG}_{BAL_TAG}\",\n",
    "}\n",
    "for k, base in DIRS.items():\n",
    "    for sub in [\"results\", \"figs\", \"preds\", \"best_params\"]:\n",
    "        (base / sub).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:     \", ROOT)\n",
    "print(\"DATA_DIR: \", DATA_DIR)\n",
    "print(\"ARTIF_DIR:\", ARTIF_ROOT)\n",
    "print({k: str(v) for k, v in DIRS.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7b3d6",
   "metadata": {},
   "source": [
    "2 — Carga de datos y metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "72b45c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (6000, 15) (2000, 15) (2000, 15)\n",
      "y: (6000,) (2000,) (2000,)\n",
      "n features: 15\n",
      "[META] Archivo detectado: N/D | columnas categóricas=0\n",
      "[META] No hay categóricas => caerá en SMOTE estándar.\n"
     ]
    }
   ],
   "source": [
    "def load_xy_full(dir_full: Path):\n",
    "    expected = [\n",
    "        dir_full / \"X_train_full.npy\",\n",
    "        dir_full / \"X_val_full.npy\",\n",
    "        dir_full / \"X_test_full.npy\",\n",
    "        dir_full / \"y_train.parquet\",\n",
    "        dir_full / \"y_val.parquet\",\n",
    "        dir_full / \"y_test.parquet\",\n",
    "        dir_full / \"feature_names_full.parquet\",\n",
    "    ]\n",
    "    missing = [p.name for p in expected if not p.exists()]\n",
    "    if missing:\n",
    "        listing = [p.name for p in dir_full.glob(\"*\")]\n",
    "        raise FileNotFoundError(\n",
    "            f\"No se encuentran estos archivos en {dir_full} -> {missing}\\n\"\n",
    "            f\"Contenido detectado: {listing}\\n\"\n",
    "            f\"Sugerencia: verifica que ROOT (impreso arriba) apunte a la carpeta del proyecto.\"\n",
    "        )\n",
    "\n",
    "    X_train = np.load(dir_full / \"X_train_full.npy\")\n",
    "    X_val   = np.load(dir_full / \"X_val_full.npy\")\n",
    "    X_test  = np.load(dir_full / \"X_test_full.npy\")\n",
    "\n",
    "    y_train = pd.read_parquet(dir_full / \"y_train.parquet\")[\"Exited\"].to_numpy()\n",
    "    y_val   = pd.read_parquet(dir_full / \"y_val.parquet\")[\"Exited\"].to_numpy()\n",
    "    y_test  = pd.read_parquet(dir_full / \"y_test.parquet\")[\"Exited\"].to_numpy()\n",
    "\n",
    "    feature_names = pd.read_parquet(dir_full / \"feature_names_full.parquet\")[\"feature\"].tolist()\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, feature_names\n",
    "\n",
    "# Cargar datos\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_xy_full(DATA_DIR)\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"y:\", y_train.shape, y_val.shape, y_test.shape)\n",
    "print(\"n features:\", len(feature_names))\n",
    "\n",
    "# --- Metadatos para SMOTENC ---\n",
    "def _read_feature_roles(dir_full: Path):\n",
    "    candidates = [\n",
    "        dir_full / \"feature_roles_full.parquet\",\n",
    "        dir_full / \"feature_roles.parquet\",\n",
    "        dir_full / \"feature_meta_full.parquet\",\n",
    "        dir_full / \"feature_meta.parquet\",\n",
    "        dir_full / \"feature_types_full.parquet\",\n",
    "        dir_full / \"feature_types.parquet\",\n",
    "        dir_full / \"feature_meta.json\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            if p.suffix == \".parquet\":\n",
    "                df = pd.read_parquet(p)\n",
    "            elif p.suffix == \".json\":\n",
    "                obj = json.loads(p.read_text())\n",
    "                if isinstance(obj, dict) and \"features\" in obj:\n",
    "                    df = pd.DataFrame(obj[\"features\"])\n",
    "                else:\n",
    "                    df = pd.DataFrame(obj)\n",
    "            else:\n",
    "                continue\n",
    "            return df, p.name\n",
    "    return None, None\n",
    "\n",
    "def _build_cat_idx(feature_names, roles_df):\n",
    "    if roles_df is None or len(roles_df) == 0:\n",
    "        return []\n",
    "    df = roles_df.copy()\n",
    "    df.columns = [str(c).lower() for c in df.columns]\n",
    "    if \"feature\" not in df.columns:\n",
    "        if \"name\" in df.columns:\n",
    "            df[\"feature\"] = df[\"name\"]\n",
    "        else:\n",
    "            return []\n",
    "    cat_names = set()\n",
    "    if \"role\" in df.columns:\n",
    "        cat_names = set(df.loc[df[\"role\"].astype(str).str.lower().isin(\n",
    "            [\"cat\",\"categorical\",\"bin\",\"binary\",\"ordinal\"]), \"feature\"])\n",
    "    elif \"dtype\" in df.columns:\n",
    "        cat_names = set(df.loc[df[\"dtype\"].astype(str).str.lower().isin(\n",
    "            [\"category\",\"categorical\",\"object\",\"bool\"]), \"feature\"])\n",
    "    elif \"is_cat\" in df.columns:\n",
    "        cat_names = set(df.loc[df[\"is_cat\"].astype(bool), \"feature\"])\n",
    "    else:\n",
    "        return []\n",
    "    idx = [i for i, f in enumerate(feature_names) if f in cat_names]\n",
    "    return sorted(idx)\n",
    "\n",
    "roles_df, meta_file = _read_feature_roles(DATA_DIR)\n",
    "CAT_IDX_FULL = _build_cat_idx(feature_names, roles_df)\n",
    "print(f\"[META] Archivo detectado: {meta_file or 'N/D'} | columnas categóricas={len(CAT_IDX_FULL)}\")\n",
    "if CAT_IDX_FULL:\n",
    "    print(\"[META] Ejemplo de índices categóricos:\", CAT_IDX_FULL[:10], \"...\")\n",
    "else:\n",
    "    print(\"[META] No hay categóricas => caerá en SMOTE estándar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b76afe4",
   "metadata": {},
   "source": [
    "3 — Métricas, umbral, y plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "789d35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "def pr_auc(y_true, y_proba): \n",
    "    return float(average_precision_score(y_true, y_proba))\n",
    "\n",
    "def roc_auc(y_true, y_proba): \n",
    "    return float(roc_auc_score(y_true, y_proba))\n",
    "\n",
    "def find_best_threshold(y_true, y_proba, metric=\"f1\"):\n",
    "    thr_grid = np.linspace(0.0, 1.0, 1001)\n",
    "    best_thr, best_score = 0.5, -1.0\n",
    "    for thr in thr_grid:\n",
    "        y_pred = (y_proba >= thr).astype(int)\n",
    "        if metric == \"f1\":\n",
    "            score = f1_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == \"recall\":\n",
    "            score = recall_score(y_true, y_pred, zero_division=0)\n",
    "        else:\n",
    "            raise ValueError(\"metric no soportada\")\n",
    "        if score > best_score:\n",
    "            best_score, best_thr = score, thr\n",
    "    return float(best_thr), float(best_score)\n",
    "\n",
    "def compute_all_metrics(y_true, y_proba, thr):\n",
    "    y_pred = (y_proba >= thr).astype(int)\n",
    "    return {\n",
    "        \"pr_auc\": pr_auc(y_true, y_proba),\n",
    "        \"roc_auc\": roc_auc(y_true, y_proba),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"bal_acc\": balanced_accuracy_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def plot_pr_curve(y_true, y_proba, title, out_path):\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n",
    "    ap = average_precision_score(y_true, y_proba)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.step(rec, prec, where='post')\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.title(f'{title} (AP={ap:.4f})')\n",
    "    plt.grid(True, linestyle='--', alpha=.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "def plot_roc_curve(y_true, y_proba, title, out_path):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, lw=2)\n",
    "    plt.plot([0,1],[0,1], 'k--', lw=1)\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title} (AUC={auc:.4f})')\n",
    "    plt.grid(True, linestyle='--', alpha=.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "def plot_confusion(y_true, y_pred, title, out_path, normalize=False):\n",
    "    norm = 'true' if normalize else None\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=norm)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(2)\n",
    "    plt.xticks(ticks, ['0','1']); plt.yticks(ticks, ['0','1'])\n",
    "    thresh = cm.max()/2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            txt = f'{cm[i,j]:.2f}' if normalize else str(cm[i,j])\n",
    "            plt.text(j, i, txt, ha='center', va='center',\n",
    "                     color='white' if cm[i,j] > thresh else 'black')\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5651c79",
   "metadata": {},
   "source": [
    "4 — Helpers de balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0be6c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _map_cat_idx_for_keep(keep_idx, cat_idx_full):\n",
    "    if not cat_idx_full:\n",
    "        return []\n",
    "    if keep_idx is None:\n",
    "        return sorted(cat_idx_full)\n",
    "    pos = {old_i: j for j, old_i in enumerate(keep_idx)}\n",
    "    return sorted([pos[i] for i in cat_idx_full if i in pos])\n",
    "\n",
    "def apply_keep_idx(X, keep_idx):\n",
    "    return X[:, keep_idx]\n",
    "\n",
    "def maybe_smote(X, y, keep_idx=None, random_state=RANDOM_STATE, k_neighbors=5):\n",
    "    if not _HAS_IMBLEARN:\n",
    "        print(\"[BAL] imbalanced-learn no disponible. Se omite balanceo.\")\n",
    "        return X, y\n",
    "    y_int = y.astype(int)\n",
    "    if y_int.max() == 0:\n",
    "        print(\"[BAL] Solo 1 clase en y. Se omite balanceo.\")\n",
    "        return X, y\n",
    "    counts = np.bincount(y_int)\n",
    "    if len(counts) < 2 or counts.min() < 2:\n",
    "        print(\"[BAL] Minoría < 2 muestras. Se omite balanceo.\")\n",
    "        return X, y\n",
    "    k = int(max(1, min(5, counts.min() - 1)))\n",
    "    cat_idx = _map_cat_idx_for_keep(keep_idx, CAT_IDX_FULL)\n",
    "    if cat_idx:\n",
    "        sm = SMOTENC(categorical_features=cat_idx, k_neighbors=k, random_state=random_state)\n",
    "        kind = \"SMOTENC\"\n",
    "    else:\n",
    "        sm = SMOTE(k_neighbors=k, random_state=random_state)\n",
    "        kind = \"SMOTE\"\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    try:\n",
    "        X_res = X_res.astype(X.dtype, copy=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f\"[BAL] {kind} aplicado | k_neighbors={k} | cat_cols={len(cat_idx)}\")\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "def _json_dump(path, obj):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def _floatify_metrics(d):\n",
    "    return {k: float(v) for k, v in d.items()}\n",
    "\n",
    "def write_minimal_loader(out_dir: Path):\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    code = r'''# -*- coding: utf-8 -*-\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def load_xgb_from_manifest(manifest_path):\n",
    "    import xgboost as xgb\n",
    "    mp = Path(manifest_path)\n",
    "    m = json.loads(mp.read_text(encoding=\"utf-8\"))\n",
    "    mdl_file = mp.parent / m[\"files\"][\"model_json\"]\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(str(mdl_file))\n",
    "\n",
    "    best_it = m.get(\"training\", {}).get(\"best_iteration\", None)\n",
    "    feat_names = m[\"features\"][\"names\"]\n",
    "\n",
    "    def predict_proba(X):\n",
    "        d = xgb.DMatrix(X, feature_names=feat_names)\n",
    "        if best_it is not None and isinstance(best_it, int):\n",
    "            p = booster.predict(d, iteration_range=(0, best_it+1))\n",
    "        else:\n",
    "            p = booster.predict(d)\n",
    "        return np.column_stack([1.0 - p, p])\n",
    "\n",
    "    return m, predict_proba\n",
    "\n",
    "def load_lgbm_from_manifest(manifest_path):\n",
    "    import lightgbm as lgb\n",
    "    mp = Path(manifest_path)\n",
    "    m = json.loads(mp.read_text(encoding=\"utf-8\"))\n",
    "    mdl_file = mp.parent / m[\"files\"][\"model_txt\"]\n",
    "    booster = lgb.Booster(model_file=str(mdl_file))\n",
    "\n",
    "    feat_names = m[\"features\"][\"names\"]\n",
    "    best_it = m.get(\"training\", {}).get(\"best_iteration\", None)\n",
    "\n",
    "    def predict_proba(X):\n",
    "        p1 = booster.predict(X, num_iteration=best_it)\n",
    "        return np.column_stack([1.0 - p1, p1])\n",
    "\n",
    "    return m, predict_proba\n",
    "'''\n",
    "    (out_dir / \"loader_example.py\").write_text(code, encoding=\"utf-8\")\n",
    "\n",
    "def export_xgb(adapter, out_dir, exp_name, feature_names, threshold, val_metrics, test_metrics):\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    booster = adapter.get_booster()\n",
    "    json_path = out_dir / f\"{exp_name}.xgb.json\"\n",
    "    ubj_path  = out_dir / f\"{exp_name}.xgb.ubj\"\n",
    "\n",
    "    booster.save_model(str(json_path))\n",
    "\n",
    "    files = {\"model_json\": json_path.name}\n",
    "    try:\n",
    "        booster.save_model(str(ubj_path))\n",
    "        files[\"model_ubj\"] = ubj_path.name\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import joblib\n",
    "        joblib.dump(\n",
    "            {\"params\": adapter.get_params(), \"best_iteration\": adapter.best_iteration},\n",
    "            out_dir / f\"{exp_name}.sk_params.joblib\"\n",
    "        )\n",
    "        files[\"sk_params_joblib\"] = f\"{exp_name}.sk_params.joblib\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    manifest = {\n",
    "        \"export_schema\": \"v1\",\n",
    "        \"model_key\": exp_name,\n",
    "        \"kind\": \"xgboost_binary_classifier\",\n",
    "        \"framework\": \"xgboost\",\n",
    "        \"framework_version\": getattr(xgb, \"__version__\", \"unknown\"),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"files\": files,\n",
    "        \"features\": {\n",
    "            \"names\": list(map(str, feature_names)),\n",
    "            \"dtype\": \"float32\"\n",
    "        },\n",
    "        \"inference\": {\n",
    "            \"class_labels\": [0, 1],\n",
    "            \"threshold\": float(threshold)\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"best_iteration\": int(adapter.best_iteration) if adapter.best_iteration is not None else None\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"validation\": _floatify_metrics(val_metrics),\n",
    "            \"test\": _floatify_metrics(test_metrics)\n",
    "        }\n",
    "    }\n",
    "    _json_dump(out_dir / f\"{exp_name}_MANIFEST.json\", manifest)\n",
    "    write_minimal_loader(out_dir)\n",
    "    print(f\"[EXPORT][XGB] {exp_name} -> {out_dir}\")\n",
    "    return manifest\n",
    "\n",
    "def export_lgbm(adapter, out_dir, exp_name, feature_names, threshold, val_metrics, test_metrics):\n",
    "    \"\"\"\n",
    "    Exporta:\n",
    "      - Modelo LightGBM en TXT (formato de texto oficial).\n",
    "      - Manifiesto JSON con nombres de features, umbral y métricas.\n",
    "      - Loader de ejemplo.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    booster = adapter.booster_\n",
    "    txt_path = out_dir / f\"{exp_name}.lgbm.txt\"\n",
    "    booster.save_model(str(txt_path))\n",
    "\n",
    "    manifest = {\n",
    "        \"export_schema\": \"v1\",\n",
    "        \"model_key\": exp_name,\n",
    "        \"kind\": \"lightgbm_binary_classifier\",\n",
    "        \"framework\": \"lightgbm\",\n",
    "        \"framework_version\": getattr(lgb, \"__version__\", \"unknown\"),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"files\": {\n",
    "            \"model_txt\": txt_path.name\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"names\": list(map(str, feature_names)),\n",
    "            \"dtype\": \"float32\"\n",
    "        },\n",
    "        \"inference\": {\n",
    "            \"class_labels\": [0, 1],\n",
    "            \"threshold\": float(threshold)\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"best_iteration\": int(adapter.best_iteration) if getattr(adapter, \"best_iteration\", None) is not None else None\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"validation\": _floatify_metrics(val_metrics),\n",
    "            \"test\": _floatify_metrics(test_metrics)\n",
    "        }\n",
    "    }\n",
    "    _json_dump(out_dir / f\"{exp_name}_MANIFEST.json\", manifest)\n",
    "    write_minimal_loader(out_dir)\n",
    "    print(f\"[EXPORT][LGBM] {exp_name} -> {out_dir}\")\n",
    "    return manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95e23c",
   "metadata": {},
   "source": [
    "5 — Adaptadores de entrenamiento con Early-Stopping (XGB & LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "828777f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost ---\n",
    "class XGBAdapter:\n",
    "    def __init__(self, booster, params, best_iteration, feature_names=None):\n",
    "        self._booster = booster\n",
    "        self._params = dict(params)\n",
    "        self.best_iteration = best_iteration\n",
    "        self._feature_names = feature_names\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        d = xgb.DMatrix(X, feature_names=self._feature_names)\n",
    "        if self.best_iteration is not None:\n",
    "            pred = self._booster.predict(d, iteration_range=(0, int(self.best_iteration)+1))\n",
    "        else:\n",
    "            pred = self._booster.predict(d)\n",
    "        return np.column_stack([1.0 - pred, pred])\n",
    "\n",
    "    def get_booster(self):\n",
    "        return self._booster\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict(self._params)\n",
    "\n",
    "\n",
    "def xgb_fit_with_es(sk_model, X_tr, y_tr, X_va, y_va, feature_names=None, rounds=200, verbose=False):\n",
    "    p = sk_model.get_params()\n",
    "    n_estimators = int(p.pop(\"n_estimators\", 1000))\n",
    "    seed = p.pop(\"random_state\", p.pop(\"seed\", RANDOM_STATE))\n",
    "    nthread = p.pop(\"n_jobs\", None)\n",
    "    if nthread is not None:\n",
    "        p[\"nthread\"] = nthread\n",
    "    p.setdefault(\"seed\", seed)\n",
    "    p.setdefault(\"objective\", \"binary:logistic\")\n",
    "    p.setdefault(\"eval_metric\", \"aucpr\")\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr, feature_names=feature_names)\n",
    "    dvalid = xgb.DMatrix(X_va, label=y_va, feature_names=feature_names)\n",
    "    evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "    booster = xgb.train(\n",
    "        params=p, dtrain=dtrain, num_boost_round=n_estimators,\n",
    "        evals=evals, early_stopping_rounds=rounds, verbose_eval=verbose\n",
    "    )\n",
    "    best_iter = getattr(booster, \"best_iteration\", None)\n",
    "    return XGBAdapter(booster, {**sk_model.get_params(), \"best_iteration\": best_iter}, best_iter, feature_names)\n",
    "\n",
    "\n",
    "def xgb_gain_importances(booster, feature_names):\n",
    "    gain_dict = booster.get_score(importance_type=\"gain\")\n",
    "    name_to_idx = {n: i for i, n in enumerate(feature_names)}\n",
    "    imp_gain = np.zeros(len(feature_names), dtype=float)\n",
    "    for k, v in gain_dict.items():\n",
    "        if k.startswith(\"f\") and k[1:].isdigit():\n",
    "            idx = int(k[1:])\n",
    "        else:\n",
    "            idx = name_to_idx.get(k, None)\n",
    "        if idx is not None and 0 <= idx < len(imp_gain):\n",
    "            imp_gain[idx] = v\n",
    "    return imp_gain\n",
    "\n",
    "\n",
    "# --- LightGBM ---\n",
    "class LGBMAdapter:\n",
    "    \"\"\"Adapter consistente con .booster_ como atributo y .best_iteration.\"\"\"\n",
    "    def __init__(self, booster, params, best_iteration, feature_names=None):\n",
    "        self.booster_ = booster\n",
    "        self._params = dict(params)\n",
    "        self.best_iteration = best_iteration\n",
    "        self._feature_names = feature_names\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        p1 = self.booster_.predict(X, num_iteration=self.best_iteration)\n",
    "        return np.column_stack([1.0 - p1, p1])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict(self._params)\n",
    "\n",
    "\n",
    "def lgbm_fit_with_es(sk_model, X_tr, y_tr, X_va, y_va, feature_names=None, rounds=200, verbose=False):\n",
    "    p = dict(sk_model.get_params())\n",
    "    num_boost_round = int(p.pop(\"n_estimators\", 1000))\n",
    "\n",
    "    # Defaults coherentes con AP\n",
    "    lgb_params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": p.pop(\"metric\", \"average_precision\"),\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": p.pop(\"random_state\", RANDOM_STATE),\n",
    "    }\n",
    "\n",
    "    # Mapear nombres scikit->lightgbm\n",
    "    if \"colsample_bytree\" in p:\n",
    "        lgb_params[\"feature_fraction\"] = p.pop(\"colsample_bytree\")\n",
    "    if \"subsample\" in p:\n",
    "        lgb_params[\"bagging_fraction\"] = p.pop(\"subsample\")\n",
    "        lgb_params[\"bagging_freq\"] = 1\n",
    "\n",
    "    # Resto de hiperparámetros\n",
    "    lgb_params.update(p)\n",
    "\n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr, feature_name=feature_names, free_raw_data=True)\n",
    "    dvalid = lgb.Dataset(X_va, label=y_va, feature_name=feature_names, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "    booster = lgb.train(\n",
    "        params=lgb_params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        valid_names=[\"train\", \"valid\"],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=rounds, verbose=verbose)]\n",
    "    )\n",
    "    best_iter = booster.best_iteration\n",
    "    return LGBMAdapter(booster, {**sk_model.get_params(), \"best_iteration\": best_iter}, best_iter, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be5066",
   "metadata": {},
   "source": [
    "6 — Carga/seed de hiperparámetros y defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "df2c238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HP][xgb] Cargando BEST previo: BEST_XGB_FULL_SMOTENC.json\n",
      "[HP][lgbm] Cargando BEST previo: BEST_LGBM_FULL_SMOTENC.json\n"
     ]
    }
   ],
   "source": [
    "def get_xgb_defaults(seed=RANDOM_STATE):\n",
    "    mdl = XGBClassifier(\n",
    "        random_state=seed, n_jobs=-1, eval_metric=\"aucpr\",\n",
    "        tree_method=\"hist\", verbosity=0\n",
    "    )\n",
    "    p = mdl.get_params()\n",
    "    p.pop(\"verbose\", None)\n",
    "    p.setdefault(\"verbosity\", 0)\n",
    "    p.setdefault(\"n_estimators\", 1000)\n",
    "    return p\n",
    "\n",
    "def get_lgbm_defaults(seed=RANDOM_STATE):\n",
    "    mdl = LGBMClassifier(\n",
    "        random_state=seed, n_estimators=1000, metric=\"average_precision\"\n",
    "    )\n",
    "    return mdl.get_params()\n",
    "\n",
    "def _best_file(model_key):\n",
    "    return DIRS[model_key] / \"best_params\" / f\"BEST_{model_key.upper()}_{VIEW_TAG}_{BAL_TAG}.json\"\n",
    "\n",
    "def load_best_or_default(model_key):\n",
    "    best_fp = _best_file(model_key)\n",
    "    if best_fp.exists():\n",
    "        try:\n",
    "            best = json.loads(best_fp.read_text())\n",
    "            print(f\"[HP][{model_key}] Cargando BEST previo:\", best_fp.name)\n",
    "            base = get_xgb_defaults() if model_key==\"xgb\" else get_lgbm_defaults()\n",
    "            base.update(best)\n",
    "            return base, True\n",
    "        except Exception as e:\n",
    "            print(f\"[HP][{model_key}] No se pudo leer BEST. Uso defaults. {e}\")\n",
    "    print(f\"[HP][{model_key}] Usando defaults.\")\n",
    "    return (get_xgb_defaults() if model_key==\"xgb\" else get_lgbm_defaults()), False\n",
    "\n",
    "xgb_params_seed, _ = load_best_or_default(\"xgb\")\n",
    "lgbm_params_seed, _ = load_best_or_default(\"lgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3747f83",
   "metadata": {},
   "source": [
    "7 — Entrenamiento BASELINE (XGB y LGBM) + umbral + guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "45f8257e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[EXPORT][XGB] XGB_FULL_SMOTENC_BASE -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_FULL_SMOTENC/export\n",
      "[OK][XGB BASE] Guardados en /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_FULL_SMOTENC\n",
      "[EXPORT][LGBM] LGBM_FULL_SMOTENC_BASE -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_FULL_SMOTENC/export\n",
      "[OK][LGBM BASE] Guardados en /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_FULL_SMOTENC\n"
     ]
    }
   ],
   "source": [
    "keep_idx_global = None\n",
    "feature_names_used = feature_names\n",
    "X_train_fit, X_val_fit, X_test_fit = X_train, X_val, X_test\n",
    "\n",
    "# Balanceo global para baseline\n",
    "X_train_final, y_train_final = X_train_fit, y_train\n",
    "if USE_BALANCED_TRAIN:\n",
    "    X_train_final, y_train_final = maybe_smote(X_train_fit, y_train, keep_idx=keep_idx_global)\n",
    "\n",
    "# ---- XGB BASELINE ----\n",
    "xgb_seed = dict(xgb_params_seed)\n",
    "xgb_seed.setdefault(\"n_estimators\", xgb_seed.get(\"n_estimators\", 1000))\n",
    "xgb_seed.setdefault(\"random_state\", RANDOM_STATE)\n",
    "xgb_seed.setdefault(\"n_jobs\", -1)\n",
    "xgb_seed.setdefault(\"eval_metric\", \"aucpr\")\n",
    "xgb_seed.setdefault(\"tree_method\", \"hist\")\n",
    "xgb_seed.setdefault(\"verbosity\", 0)\n",
    "xgb_seed.pop(\"verbose\", None)\n",
    "\n",
    "xgb_baseline_model = XGBClassifier(**xgb_seed)\n",
    "xgb_baseline_model = xgb_fit_with_es(\n",
    "    xgb_baseline_model, X_train_final, y_train_final,\n",
    "    X_val_fit, y_val, feature_names=feature_names_used,\n",
    "    rounds=200, verbose=False\n",
    ")\n",
    "\n",
    "# Validación\n",
    "proba_val_xgb = xgb_baseline_model.predict_proba(X_val_fit)[:,1]\n",
    "thr_val_xgb, best_f1_val_xgb = find_best_threshold(y_val, proba_val_xgb, metric=\"f1\")\n",
    "val_metrics_xgb = compute_all_metrics(y_val, proba_val_xgb, thr_val_xgb)\n",
    "\n",
    "# Test\n",
    "proba_test_xgb = xgb_baseline_model.predict_proba(X_test_fit)[:,1]\n",
    "y_pred_test_xgb = (proba_test_xgb >= thr_val_xgb).astype(int)\n",
    "test_metrics_xgb = compute_all_metrics(y_test, proba_test_xgb, thr_val_xgb)\n",
    "\n",
    "# === Guardados ===\n",
    "EXP_NAME_XGB = f\"XGB_{VIEW_TAG}_{BAL_TAG}\"\n",
    "bx = DIRS[\"xgb\"]\n",
    "OUT_RESULTS_X = bx / \"results\"\n",
    "OUT_FIGS_X    = bx / \"figs\"\n",
    "OUT_PREDS_X   = bx / \"preds\"\n",
    "OUT_PARAMS_X  = bx / \"best_params\"\n",
    "OUT_EXPORT_X  = bx / \"export\"\n",
    "\n",
    "# HP seed y \"fitted\"\n",
    "with open(OUT_PARAMS_X / f\"{EXP_NAME_XGB}_BASE_seed_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(xgb_seed, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(OUT_PARAMS_X / f\"{EXP_NAME_XGB}_BASE_fitted_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(xgb_baseline_model.get_params(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Plots\n",
    "plot_pr_curve(y_val,  proba_val_xgb,  f\"{EXP_NAME_XGB} — PR (val)\",  OUT_FIGS_X / f\"{EXP_NAME_XGB}_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_xgb, f\"{EXP_NAME_XGB} — PR (test)\", OUT_FIGS_X / f\"{EXP_NAME_XGB}_pr_test.png\")\n",
    "plot_roc_curve(y_val,  proba_val_xgb,  f\"{EXP_NAME_XGB} — ROC (val)\",  OUT_FIGS_X / f\"{EXP_NAME_XGB}_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_xgb, f\"{EXP_NAME_XGB} — ROC (test)\", OUT_FIGS_X / f\"{EXP_NAME_XGB}_roc_test.png\")\n",
    "plot_confusion(y_test, y_pred_test_xgb, f\"{EXP_NAME_XGB} — Confusion (test @thr={thr_val_xgb:.3f})\", OUT_FIGS_X / f\"{EXP_NAME_XGB}_cm_test.png\")\n",
    "\n",
    "# Importancias\n",
    "try:\n",
    "    booster = xgb_baseline_model.get_booster()\n",
    "    imp_gain_x = xgb_gain_importances(booster, feature_names_used)\n",
    "except Exception:\n",
    "    imp_gain_x = getattr(xgb_baseline_model, \"feature_importances_\", np.zeros(len(feature_names_used)))\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\"feature\": feature_names_used[:len(imp_gain_x)], \"importance_gain\": imp_gain_x}\n",
    ").sort_values(\"importance_gain\", ascending=False)\\\n",
    " .to_csv(OUT_RESULTS_X / f\"{EXP_NAME_XGB}_feature_importances.csv\", index=False)\n",
    "\n",
    "# Preds test\n",
    "pd.DataFrame({\"proba\": proba_test_xgb, \"y_true\": y_test}).to_parquet(OUT_PREDS_X / f\"preds_test_{EXP_NAME_XGB}.parquet\", index=False)\n",
    "\n",
    "# Registro baselines.csv\n",
    "best_iter_base_xgb = getattr(xgb_baseline_model, \"best_iteration\", None)\n",
    "row_base_xgb = {\n",
    "    \"model\": EXP_NAME_XGB,\n",
    "    \"thr_val\": thr_val_xgb,\n",
    "    \"val_pr_auc\": val_metrics_xgb[\"pr_auc\"],\n",
    "    \"val_roc_auc\": val_metrics_xgb[\"roc_auc\"],\n",
    "    \"val_precision\": val_metrics_xgb[\"precision\"],\n",
    "    \"val_f1\": val_metrics_xgb[\"f1\"],\n",
    "    \"val_recall\": val_metrics_xgb[\"recall\"],\n",
    "    \"val_bal_acc\": val_metrics_xgb[\"bal_acc\"],\n",
    "    \"test_pr_auc\": test_metrics_xgb[\"pr_auc\"],\n",
    "    \"test_roc_auc\": test_metrics_xgb[\"roc_auc\"],\n",
    "    \"test_precision\": test_metrics_xgb[\"precision\"],\n",
    "    \"test_f1\": test_metrics_xgb[\"f1\"],\n",
    "    \"test_recall\": test_metrics_xgb[\"recall\"],\n",
    "    \"test_bal_acc\": test_metrics_xgb[\"bal_acc\"],\n",
    "    \"best_iteration\": best_iter_base_xgb if best_iter_base_xgb is not None else np.nan\n",
    "}\n",
    "csv_x = OUT_RESULTS_X / \"baselines.csv\"\n",
    "pd.DataFrame([row_base_xgb]).to_csv(csv_x, mode=(\"a\" if csv_x.exists() else \"w\"), index=False, header=not csv_x.exists())\n",
    "\n",
    "# === EXPORT: XGB BASE ===\n",
    "export_xgb(\n",
    "    adapter=xgb_baseline_model,\n",
    "    out_dir=OUT_EXPORT_X,\n",
    "    exp_name=f\"{EXP_NAME_XGB}_BASE\",\n",
    "    feature_names=feature_names_used,\n",
    "    threshold=thr_val_xgb,\n",
    "    val_metrics=val_metrics_xgb,\n",
    "    test_metrics=test_metrics_xgb\n",
    ")\n",
    "print(\"[OK][XGB BASE] Guardados en\", bx)\n",
    "\n",
    "# ---- LGBM BASELINE ----\n",
    "lgbm_seed = dict(lgbm_params_seed)\n",
    "lgbm_seed.setdefault(\"n_estimators\", lgbm_seed.get(\"n_estimators\", 1000))\n",
    "lgbm_seed.setdefault(\"random_state\", RANDOM_STATE)\n",
    "lgbm_seed.setdefault(\"objective\", \"binary\")\n",
    "if (\"metric\" not in lgbm_seed) or (lgbm_seed[\"metric\"] in (None, \"\", [], \"None\")):\n",
    "    lgbm_seed[\"metric\"] = \"average_precision\"\n",
    "lgbm_seed.setdefault(\"n_jobs\", -1)\n",
    "lgbm_seed.setdefault(\"verbosity\", -1)\n",
    "\n",
    "lgbm_baseline_model = LGBMClassifier(**lgbm_seed)\n",
    "lgbm_adapter = lgbm_fit_with_es(\n",
    "    lgbm_baseline_model, X_train_final, y_train_final,\n",
    "    X_val_fit, y_val, feature_names=feature_names_used,\n",
    "    rounds=200, verbose=False\n",
    ")\n",
    "\n",
    "# Validación\n",
    "proba_val_lgb = lgbm_adapter.predict_proba(X_val_fit)[:,1]\n",
    "thr_val_lgb, best_f1_val_lgb = find_best_threshold(y_val, proba_val_lgb, metric=\"f1\")\n",
    "val_metrics_lgb = compute_all_metrics(y_val, proba_val_lgb, thr_val_lgb)\n",
    "\n",
    "# Test\n",
    "proba_test_lgb = lgbm_adapter.predict_proba(X_test_fit)[:,1]\n",
    "y_pred_test_lgb = (proba_test_lgb >= thr_val_lgb).astype(int)\n",
    "test_metrics_lgb = compute_all_metrics(y_test, proba_test_lgb, thr_val_lgb)\n",
    "\n",
    "# === Guardados ===\n",
    "EXP_NAME_LGB = f\"LGBM_{VIEW_TAG}_{BAL_TAG}\"\n",
    "bl = DIRS[\"lgbm\"]\n",
    "OUT_RESULTS_L = bl / \"results\"\n",
    "OUT_FIGS_L    = bl / \"figs\"\n",
    "OUT_PREDS_L   = bl / \"preds\"\n",
    "OUT_PARAMS_L  = bl / \"best_params\"\n",
    "OUT_EXPORT_L  = bl / \"export\"\n",
    "\n",
    "with open(OUT_PARAMS_L / f\"{EXP_NAME_LGB}_BASE_seed_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(lgbm_seed, f, indent=2, ensure_ascii=False)\n",
    "with open(OUT_PARAMS_L / f\"{EXP_NAME_LGB}_BASE_fitted_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(lgbm_baseline_model.get_params(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Plots\n",
    "plot_pr_curve(y_val,  proba_val_lgb,  f\"{EXP_NAME_LGB} — PR (val)\",  OUT_FIGS_L / f\"{EXP_NAME_LGB}_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_lgb, f\"{EXP_NAME_LGB} — PR (test)\", OUT_FIGS_L / f\"{EXP_NAME_LGB}_pr_test.png\")\n",
    "plot_roc_curve(y_val,  proba_val_lgb,  f\"{EXP_NAME_LGB} — ROC (val)\",  OUT_FIGS_L / f\"{EXP_NAME_LGB}_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_lgb, f\"{EXP_NAME_LGB} — ROC (test)\", OUT_FIGS_L / f\"{EXP_NAME_LGB}_roc_test.png\")\n",
    "plot_confusion(y_test, y_pred_test_lgb,\n",
    "               f\"{EXP_NAME_LGB} — Confusion (test @thr_used={thr_val_lgb:.3f})\",\n",
    "               OUT_FIGS_L / f\"{EXP_NAME_LGB}_cm_test.png\")\n",
    "\n",
    "# Importancias\n",
    "try:\n",
    "    imp_gain_l = lgbm_adapter.booster_.feature_importance(importance_type=\"gain\")\n",
    "except Exception:\n",
    "    imp_gain_l = np.zeros(len(feature_names_used))\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\"feature\": feature_names_used[:len(imp_gain_l)], \"importance_gain\": imp_gain_l}\n",
    ").sort_values(\"importance_gain\", ascending=False)\\\n",
    " .to_csv(OUT_RESULTS_L / f\"{EXP_NAME_LGB}_feature_importances.csv\", index=False)\n",
    "\n",
    "# Preds test\n",
    "pd.DataFrame({\"proba\": proba_test_lgb, \"y_true\": y_test})\\\n",
    "  .to_parquet(OUT_PREDS_L / f\"preds_test_{EXP_NAME_LGB}.parquet\", index=False)\n",
    "\n",
    "# Registro baselines.csv (LGBM)\n",
    "row_base_lgb = {\n",
    "    \"model\": EXP_NAME_LGB,\n",
    "    \"thr_val\": thr_val_lgb,\n",
    "    \"thr_oof\": np.nan, \"thr_used\": thr_val_lgb,\n",
    "    \"val_pr_auc\": val_metrics_lgb[\"pr_auc\"],\n",
    "    \"val_roc_auc\": val_metrics_lgb[\"roc_auc\"],\n",
    "    \"val_precision\": val_metrics_lgb[\"precision\"],\n",
    "    \"val_f1\": val_metrics_lgb[\"f1\"],\n",
    "    \"val_recall\": val_metrics_lgb[\"recall\"],\n",
    "    \"val_bal_acc\": val_metrics_lgb[\"bal_acc\"],\n",
    "    \"test_pr_auc\": test_metrics_lgb[\"pr_auc\"],\n",
    "    \"test_roc_auc\": test_metrics_lgb[\"roc_auc\"],\n",
    "    \"test_precision\": test_metrics_lgb[\"precision\"],\n",
    "    \"test_f1\": test_metrics_lgb[\"f1\"],\n",
    "    \"test_recall\": test_metrics_lgb[\"recall\"],\n",
    "    \"test_bal_acc\": test_metrics_lgb[\"bal_acc\"],\n",
    "    \"best_iteration\": lgbm_adapter.best_iteration if hasattr(lgbm_adapter, \"best_iteration\") else np.nan\n",
    "}\n",
    "csv_l = OUT_RESULTS_L / \"baselines.csv\"\n",
    "pd.DataFrame([row_base_lgb]).to_csv(\n",
    "    csv_l, mode=(\"a\" if csv_l.exists() else \"w\"), index=False, header=not csv_l.exists()\n",
    ")\n",
    "\n",
    "# === EXPORT: LGBM BASE ===\n",
    "export_lgbm(\n",
    "    adapter=lgbm_adapter,\n",
    "    out_dir=OUT_EXPORT_L,\n",
    "    exp_name=f\"{EXP_NAME_LGB}_BASE\",\n",
    "    feature_names=feature_names_used,\n",
    "    threshold=thr_val_lgb,\n",
    "    val_metrics=val_metrics_lgb,\n",
    "    test_metrics=test_metrics_lgb\n",
    ")\n",
    "print(\"[OK][LGBM BASE] Guardados en\", bl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ce5f1",
   "metadata": {},
   "source": [
    "8 — Optuna incremental (XGB y LGBM), re-entreno TUNED + guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6491c887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 18:15:00,251] A new study created in memory with name: XGB_FULL_SMOTENC_AP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA][XGB] Enqueue BEST anterior.\n",
      "[OPTUNA][XGB] Iniciando 40 pruebas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 18:15:04,849] Trial 0 finished with value: 0.6982587963425562 and parameters: {'learning_rate': 0.013570972114707725, 'n_estimators': 2450, 'max_depth': 4, 'min_child_weight': 1.0024202070808959, 'subsample': 0.8895969662409436, 'colsample_bytree': 0.6179844014785916, 'gamma': 0.005567832464942176, 'reg_alpha': 5.0781078798822983e-05, 'reg_lambda': 0.24267702102911723}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:09,991] Trial 1 finished with value: 0.6896356488001449 and parameters: {'learning_rate': 0.008468008575248327, 'n_estimators': 2900, 'max_depth': 8, 'min_child_weight': 4.550475813202184, 'subsample': 0.6624074561769746, 'colsample_bytree': 0.662397808134481, 'gamma': 3.200866785899844e-08, 'reg_alpha': 1.156732719914599, 'reg_lambda': 0.016136341713591334}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:10,969] Trial 2 finished with value: 0.6804528020112258 and parameters: {'learning_rate': 0.05675206026988748, 'n_estimators': 800, 'max_depth': 10, 'min_child_weight': 10.779361932748845, 'subsample': 0.6849356442713105, 'colsample_bytree': 0.6727299868828402, 'gamma': 3.939402261362697e-07, 'reg_alpha': 0.000134801802908908, 'reg_lambda': 0.004712973756110786}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:13,933] Trial 3 finished with value: 0.6899644014400148 and parameters: {'learning_rate': 0.01174843954800703, 'n_estimators': 1450, 'max_depth': 7, 'min_child_weight': 0.8364645453054501, 'subsample': 0.7168578594140873, 'colsample_bytree': 0.7465447373174767, 'gamma': 9.275538076980542e-05, 'reg_alpha': 0.3134958021096912, 'reg_lambda': 2.498713568466947e-05}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:17,857] Trial 4 finished with value: 0.6784763018831735 and parameters: {'learning_rate': 0.018785426399210624, 'n_estimators': 2100, 'max_depth': 3, 'min_child_weight': 4.702115628087815, 'subsample': 0.6682096494749166, 'colsample_bytree': 0.6260206371941118, 'gamma': 1.7960847528705854, 'reg_alpha': 5.746775499181867, 'reg_lambda': 0.45580746840273345}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:20,925] Trial 5 finished with value: 0.6818573511580819 and parameters: {'learning_rate': 0.0056828375585122656, 'n_estimators': 1000, 'max_depth': 8, 'min_child_weight': 2.5358333235759627, 'subsample': 0.6488152939379115, 'colsample_bytree': 0.798070764044508, 'gamma': 1.9913367728263115e-08, 'reg_alpha': 2.318690670290199, 'reg_lambda': 6.478282331897332e-05}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:22,347] Trial 6 finished with value: 0.6876376985441253 and parameters: {'learning_rate': 0.043767126303409544, 'n_estimators': 1500, 'max_depth': 7, 'min_child_weight': 3.7569262495760847, 'subsample': 0.6739417822102108, 'colsample_bytree': 0.9878338511058234, 'gamma': 0.05531681668096113, 'reg_alpha': 3.7713131110779936, 'reg_lambda': 1.8356566544355097}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:25,381] Trial 7 finished with value: 0.6921430236631655 and parameters: {'learning_rate': 0.03027406546455652, 'n_estimators': 2850, 'max_depth': 3, 'min_child_weight': 1.0302587393796305, 'subsample': 0.6180909155642152, 'colsample_bytree': 0.7301321323053057, 'gamma': 2.4048726561760165e-05, 'reg_alpha': 7.933105363733024e-05, 'reg_lambda': 0.6326486185661588}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:28,820] Trial 8 finished with value: 0.6878849334398204 and parameters: {'learning_rate': 0.0076510536667541975, 'n_estimators': 1400, 'max_depth': 7, 'min_child_weight': 0.8408897660399112, 'subsample': 0.9208787923016158, 'colsample_bytree': 0.6298202574719083, 'gamma': 3.845031120156871, 'reg_alpha': 0.2545150013091294, 'reg_lambda': 2.4604229580184192e-05}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:36,806] Trial 9 finished with value: 0.6766338851359995 and parameters: {'learning_rate': 0.0010319982330247674, 'n_estimators': 2600, 'max_depth': 8, 'min_child_weight': 7.360091638366141, 'subsample': 0.9085081386743783, 'colsample_bytree': 0.6296178606936361, 'gamma': 1.3130541002425655e-05, 'reg_alpha': 6.472669269538641e-06, 'reg_lambda': 1.100839441018132}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:41,091] Trial 10 finished with value: 0.6907373715675573 and parameters: {'learning_rate': 0.006118505699313028, 'n_estimators': 2250, 'max_depth': 4, 'min_child_weight': 0.7977020823390241, 'subsample': 0.9542625333911195, 'colsample_bytree': 0.7245381339258192, 'gamma': 2.619369299453708e-05, 'reg_alpha': 0.00026788983814604486, 'reg_lambda': 0.01645660271000659}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:45,163] Trial 11 finished with value: 0.6926121815736644 and parameters: {'learning_rate': 0.016956996325753846, 'n_estimators': 2600, 'max_depth': 3, 'min_child_weight': 2.7238715685124095, 'subsample': 0.6793460058873315, 'colsample_bytree': 0.7136591296334357, 'gamma': 4.057149495900247e-05, 'reg_alpha': 9.544410558118765e-05, 'reg_lambda': 1.5115019717177225}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:49,444] Trial 12 finished with value: 0.6850772567555833 and parameters: {'learning_rate': 0.008366691144345278, 'n_estimators': 2400, 'max_depth': 3, 'min_child_weight': 1.6761543986464826, 'subsample': 0.8129593753912534, 'colsample_bytree': 0.6548235843522595, 'gamma': 1.6060750503364434, 'reg_alpha': 3.450704096248998e-05, 'reg_lambda': 4.401080716087103}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:51,221] Trial 13 finished with value: 0.6953243121219732 and parameters: {'learning_rate': 0.01813500575754202, 'n_estimators': 2100, 'max_depth': 5, 'min_child_weight': 2.439849593998407, 'subsample': 0.761220657845894, 'colsample_bytree': 0.8366767370901538, 'gamma': 0.00014554309989592144, 'reg_alpha': 2.2980511082048736e-05, 'reg_lambda': 0.04106254615949753}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:55,229] Trial 14 finished with value: 0.683741802109216 and parameters: {'learning_rate': 0.0037899624710656227, 'n_estimators': 1550, 'max_depth': 5, 'min_child_weight': 0.8536387114535015, 'subsample': 0.6591992444914919, 'colsample_bytree': 0.8303165674375249, 'gamma': 0.00013593896783406286, 'reg_alpha': 1.0312719932522045e-05, 'reg_lambda': 0.1448032375111787}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:15:56,868] Trial 15 finished with value: 0.6931378073252701 and parameters: {'learning_rate': 0.02210344980505765, 'n_estimators': 2050, 'max_depth': 6, 'min_child_weight': 2.858359852511454, 'subsample': 0.6041216942523567, 'colsample_bytree': 0.8675980306290558, 'gamma': 1.7145986686935467e-07, 'reg_alpha': 2.2724153991404095e-05, 'reg_lambda': 3.151829301350133e-05}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:00,154] Trial 16 finished with value: 0.6928263572962577 and parameters: {'learning_rate': 0.007442856095087516, 'n_estimators': 1300, 'max_depth': 6, 'min_child_weight': 6.579087047556015, 'subsample': 0.8528642088210454, 'colsample_bytree': 0.8965670582087952, 'gamma': 0.00012024636712926683, 'reg_alpha': 1.8359417631541833e-06, 'reg_lambda': 0.3066451930779317}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:00,954] Trial 17 finished with value: 0.6935476511841041 and parameters: {'learning_rate': 0.1352842529313607, 'n_estimators': 2700, 'max_depth': 3, 'min_child_weight': 0.9933010180587216, 'subsample': 0.8846731666160083, 'colsample_bytree': 0.6960866999565944, 'gamma': 0.00449850658083204, 'reg_alpha': 0.006162902741080436, 'reg_lambda': 0.2099631178454744}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:05,582] Trial 18 finished with value: 0.6840086289854884 and parameters: {'learning_rate': 0.004237408677787972, 'n_estimators': 2050, 'max_depth': 4, 'min_child_weight': 2.6007626886997532, 'subsample': 0.7306333661078984, 'colsample_bytree': 0.8521206490132587, 'gamma': 0.002840525182925867, 'reg_alpha': 0.17027999014644718, 'reg_lambda': 0.03488167023318773}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:06,677] Trial 19 finished with value: 0.6973329516203808 and parameters: {'learning_rate': 0.07251204415446552, 'n_estimators': 1950, 'max_depth': 5, 'min_child_weight': 1.8636923105955239, 'subsample': 0.8431109504016984, 'colsample_bytree': 0.9597602466600178, 'gamma': 0.08053900375752708, 'reg_alpha': 1.6828091476181458e-06, 'reg_lambda': 1.1990533918588877}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:07,863] Trial 20 finished with value: 0.6891793179488678 and parameters: {'learning_rate': 0.1329208122268729, 'n_estimators': 1150, 'max_depth': 7, 'min_child_weight': 1.2509645445273425, 'subsample': 0.9428271459455707, 'colsample_bytree': 0.980738432325191, 'gamma': 0.004136449633692612, 'reg_alpha': 1.588748909497046e-05, 'reg_lambda': 0.2487788365332988}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:11,501] Trial 21 finished with value: 0.6941019318958719 and parameters: {'learning_rate': 0.03578151027222242, 'n_estimators': 1550, 'max_depth': 3, 'min_child_weight': 1.7563783385256062, 'subsample': 0.7673549780932457, 'colsample_bytree': 0.949964519013934, 'gamma': 0.08473638900263361, 'reg_alpha': 2.4584106289099494e-05, 'reg_lambda': 5.655237224488996}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:12,484] Trial 22 finished with value: 0.6937845716839118 and parameters: {'learning_rate': 0.10475354743793541, 'n_estimators': 2800, 'max_depth': 7, 'min_child_weight': 1.478541252998876, 'subsample': 0.8630599210102786, 'colsample_bytree': 0.9774598816223097, 'gamma': 0.08950379082147418, 'reg_alpha': 1.5248811624610642e-06, 'reg_lambda': 1.2636823929193672}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:14,944] Trial 23 finished with value: 0.6939083851312354 and parameters: {'learning_rate': 0.0531099251485005, 'n_estimators': 1900, 'max_depth': 4, 'min_child_weight': 2.7622423604192616, 'subsample': 0.9392687363743124, 'colsample_bytree': 0.9195069220292107, 'gamma': 0.042696980540687664, 'reg_alpha': 2.7386278892723297e-06, 'reg_lambda': 0.02089579493960811}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:15,758] Trial 24 finished with value: 0.6946945485579641 and parameters: {'learning_rate': 0.09157653100311154, 'n_estimators': 1900, 'max_depth': 6, 'min_child_weight': 1.8890487159714195, 'subsample': 0.735708610896555, 'colsample_bytree': 0.7986826855272642, 'gamma': 0.003296458387969272, 'reg_alpha': 7.20288609610767e-06, 'reg_lambda': 0.00043427780227942606}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:17,556] Trial 25 finished with value: 0.6923924570128791 and parameters: {'learning_rate': 0.026861496343146857, 'n_estimators': 1350, 'max_depth': 7, 'min_child_weight': 0.9254844368183801, 'subsample': 0.8542284202580062, 'colsample_bytree': 0.6296774978601992, 'gamma': 0.011327659219169894, 'reg_alpha': 6.231992280617114e-06, 'reg_lambda': 0.28736307132411226}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:21,082] Trial 26 finished with value: 0.688678195422363 and parameters: {'learning_rate': 0.010264547350543126, 'n_estimators': 1950, 'max_depth': 3, 'min_child_weight': 1.435603225525352, 'subsample': 0.8210070279923911, 'colsample_bytree': 0.9381489096778655, 'gamma': 1.7847523027511959e-07, 'reg_alpha': 0.00015881004874525694, 'reg_lambda': 0.3441995947944141}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:27,475] Trial 27 finished with value: 0.6871835924830453 and parameters: {'learning_rate': 0.003042095804582088, 'n_estimators': 2850, 'max_depth': 5, 'min_child_weight': 0.5671474260959256, 'subsample': 0.7736396148855684, 'colsample_bytree': 0.6107548785669453, 'gamma': 0.0007583151697677437, 'reg_alpha': 5.634260783069499e-05, 'reg_lambda': 0.7913572152517976}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:29,043] Trial 28 finished with value: 0.6922122417007053 and parameters: {'learning_rate': 0.029729029898525907, 'n_estimators': 1750, 'max_depth': 5, 'min_child_weight': 2.0679736771169916, 'subsample': 0.7962986760171894, 'colsample_bytree': 0.7757882291590072, 'gamma': 9.61639003529842e-07, 'reg_alpha': 8.204074325795264e-05, 'reg_lambda': 0.004055496416895541}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:30,216] Trial 29 finished with value: 0.6852092172215426 and parameters: {'learning_rate': 0.1094798938934013, 'n_estimators': 1850, 'max_depth': 7, 'min_child_weight': 8.866425601238063, 'subsample': 0.8041713707044185, 'colsample_bytree': 0.9074000746596798, 'gamma': 0.037687120128171825, 'reg_alpha': 1.1962623829618531e-06, 'reg_lambda': 0.2221424645295304}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:32,718] Trial 30 finished with value: 0.6951608621865485 and parameters: {'learning_rate': 0.02071792422418538, 'n_estimators': 2650, 'max_depth': 6, 'min_child_weight': 2.59636174844634, 'subsample': 0.9530261320910559, 'colsample_bytree': 0.6199399767749275, 'gamma': 0.0013943931994375147, 'reg_alpha': 0.0037710734036491, 'reg_lambda': 3.1176416122419006}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:35,600] Trial 31 finished with value: 0.6927982424211199 and parameters: {'learning_rate': 0.01971088526791437, 'n_estimators': 2350, 'max_depth': 4, 'min_child_weight': 10.71607461811125, 'subsample': 0.973905493757132, 'colsample_bytree': 0.6013662011983554, 'gamma': 1.9218153972780104e-05, 'reg_alpha': 0.027684830337974288, 'reg_lambda': 1.9261638048254128}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:40,777] Trial 32 finished with value: 0.693754738200919 and parameters: {'learning_rate': 0.00781913272001791, 'n_estimators': 2300, 'max_depth': 6, 'min_child_weight': 1.9031189065124492, 'subsample': 0.9576987506121128, 'colsample_bytree': 0.6309334092507308, 'gamma': 0.0018548974656662945, 'reg_alpha': 0.003921476786349356, 'reg_lambda': 6.521705969057053}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:42,837] Trial 33 finished with value: 0.6953730001955866 and parameters: {'learning_rate': 0.022136730691049808, 'n_estimators': 2650, 'max_depth': 6, 'min_child_weight': 0.820006488810565, 'subsample': 0.8957329491482297, 'colsample_bytree': 0.6148255942120923, 'gamma': 0.017925571799655905, 'reg_alpha': 0.00013210500103218907, 'reg_lambda': 0.042365602178806666}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:48,680] Trial 34 finished with value: 0.694144941191633 and parameters: {'learning_rate': 0.005745765587153656, 'n_estimators': 2550, 'max_depth': 7, 'min_child_weight': 1.3770002820310256, 'subsample': 0.9002757728205751, 'colsample_bytree': 0.6547942247685861, 'gamma': 1.109356621963633, 'reg_alpha': 2.3161238936817903e-05, 'reg_lambda': 0.019506242127304528}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:50,912] Trial 35 finished with value: 0.6946596183896918 and parameters: {'learning_rate': 0.029557018399138456, 'n_estimators': 2300, 'max_depth': 5, 'min_child_weight': 1.8922006819151935, 'subsample': 0.9477296530648248, 'colsample_bytree': 0.6175461163043205, 'gamma': 0.001078813667446152, 'reg_alpha': 1.7806499050311243e-05, 'reg_lambda': 0.007054607423443389}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:51,516] Trial 36 finished with value: 0.6929203772115742 and parameters: {'learning_rate': 0.23216656247624592, 'n_estimators': 1900, 'max_depth': 3, 'min_child_weight': 1.1367782450535313, 'subsample': 0.8705261390695435, 'colsample_bytree': 0.9781278279359957, 'gamma': 0.04047451244182866, 'reg_alpha': 1.2763547100877981e-06, 'reg_lambda': 8.014783824696488}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:16:53,775] Trial 37 finished with value: 0.6914669332707332 and parameters: {'learning_rate': 0.01614033250027869, 'n_estimators': 2450, 'max_depth': 7, 'min_child_weight': 1.8548996423179522, 'subsample': 0.789424382124167, 'colsample_bytree': 0.8084392268528902, 'gamma': 0.011223006945211481, 'reg_alpha': 4.274663840646861e-06, 'reg_lambda': 0.7271454715899379}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:17:00,141] Trial 38 finished with value: 0.6950722831020181 and parameters: {'learning_rate': 0.008310362558005843, 'n_estimators': 2950, 'max_depth': 4, 'min_child_weight': 0.7245718181106635, 'subsample': 0.9535473058562212, 'colsample_bytree': 0.6448164006680167, 'gamma': 0.00015501498991682264, 'reg_alpha': 3.221146877299224e-05, 'reg_lambda': 3.444381087539486}. Best is trial 0 with value: 0.6982587963425562.\n",
      "[I 2025-12-11 18:17:01,445] Trial 39 finished with value: 0.6948854293063601 and parameters: {'learning_rate': 0.061200406123572915, 'n_estimators': 2300, 'max_depth': 5, 'min_child_weight': 0.5111913982664795, 'subsample': 0.8061507533481154, 'colsample_bytree': 0.6382946724637566, 'gamma': 0.0017220247932638426, 'reg_alpha': 4.785992785728568e-05, 'reg_lambda': 0.07382124910687085}. Best is trial 0 with value: 0.6982587963425562.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA][XGB] Mejor AP(val): 0.698259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 18:17:09,369] A new study created in memory with name: LGBM_FULL_SMOTENC_AP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPORT][XGB] XGB_FULL_SMOTENC_TUNED -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_FULL_SMOTENC/export\n",
      "[OPTUNA][LGBM] Enqueue BEST anterior.\n",
      "[OPTUNA][LGBM] Iniciando 40 pruebas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 18:17:14,285] Trial 0 finished with value: 0.6992533795477553 and parameters: {'learning_rate': 0.01410084212512224, 'n_estimators': 2600, 'num_leaves': 20, 'max_depth': 5, 'min_child_samples': 34, 'subsample': 0.7161156117749173, 'colsample_bytree': 0.985100043834859, 'reg_alpha': 0.002809197614512053, 'reg_lambda': 0.007006506306737809}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:17:23,929] Trial 1 finished with value: 0.6918091739274744 and parameters: {'learning_rate': 0.008468008575248327, 'n_estimators': 2900, 'num_leaves': 192, 'max_depth': 7, 'min_child_samples': 35, 'subsample': 0.662397808134481, 'colsample_bytree': 0.6232334448672797, 'reg_alpha': 1.156732719914599, 'reg_lambda': 0.016136341713591334}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:17:26,472] Trial 2 finished with value: 0.6842666605734523 and parameters: {'learning_rate': 0.05675206026988748, 'n_estimators': 800, 'num_leaves': 249, 'max_depth': 10, 'min_child_samples': 46, 'subsample': 0.6727299868828402, 'colsample_bytree': 0.6733618039413735, 'reg_alpha': 0.000134801802908908, 'reg_lambda': 0.004712973756110786}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:17:34,902] Trial 3 finished with value: 0.6852734880621091 and parameters: {'learning_rate': 0.01174843954800703, 'n_estimators': 1450, 'num_leaves': 163, 'max_depth': 0, 'min_child_samples': 62, 'subsample': 0.7465447373174767, 'colsample_bytree': 0.7824279936868144, 'reg_alpha': 0.3134958021096912, 'reg_lambda': 2.498713568466947e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:17:38,475] Trial 4 finished with value: 0.6823594798573636 and parameters: {'learning_rate': 0.018785426399210624, 'n_estimators': 2100, 'num_leaves': 27, 'max_depth': 7, 'min_child_samples': 38, 'subsample': 0.6260206371941118, 'colsample_bytree': 0.9795542149013333, 'reg_alpha': 5.746775499181867, 'reg_lambda': 0.45580746840273345}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:17:42,053] Trial 5 finished with value: 0.6734013307594136 and parameters: {'learning_rate': 0.0056828375585122656, 'n_estimators': 1000, 'num_leaves': 180, 'max_depth': 5, 'min_child_samples': 28, 'subsample': 0.798070764044508, 'colsample_bytree': 0.6137554084460873, 'reg_alpha': 2.318690670290199, 'reg_lambda': 6.478282331897332e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:17:44,163] Trial 6 finished with value: 0.685298122697496 and parameters: {'learning_rate': 0.043767126303409544, 'n_estimators': 1500, 'num_leaves': 141, 'max_depth': 6, 'min_child_samples': 41, 'subsample': 0.9878338511058234, 'colsample_bytree': 0.9100531293444458, 'reg_alpha': 3.7713131110779936, 'reg_lambda': 1.8356566544355097}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:17:45,075] Trial 7 finished with value: 0.6306059515173923 and parameters: {'learning_rate': 0.03027406546455652, 'n_estimators': 2850, 'num_leaves': 37, 'max_depth': 1, 'min_child_samples': 13, 'subsample': 0.7301321323053057, 'colsample_bytree': 0.7554709158757928, 'reg_alpha': 7.933105363733024e-05, 'reg_lambda': 0.6326486185661588}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:17:50,224] Trial 8 finished with value: 0.6739866608700927 and parameters: {'learning_rate': 0.0076510536667541975, 'n_estimators': 1400, 'num_leaves': 146, 'max_depth': 0, 'min_child_samples': 162, 'subsample': 0.6298202574719083, 'colsample_bytree': 0.9947547746402069, 'reg_alpha': 0.2545150013091294, 'reg_lambda': 2.4604229580184192e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:00,244] Trial 9 finished with value: 0.6639211072589041 and parameters: {'learning_rate': 0.0010319982330247674, 'n_estimators': 2600, 'num_leaves': 186, 'max_depth': 9, 'min_child_samples': 156, 'subsample': 0.6296178606936361, 'colsample_bytree': 0.7433862914177091, 'reg_alpha': 6.472669269538641e-06, 'reg_lambda': 1.100839441018132}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:01,677] Trial 10 finished with value: 0.6828241410255373 and parameters: {'learning_rate': 0.05935845486683995, 'n_estimators': 3000, 'num_leaves': 76, 'max_depth': 7, 'min_child_samples': 121, 'subsample': 0.8118868232958342, 'colsample_bytree': 0.983526060994001, 'reg_alpha': 0.005568880137370509, 'reg_lambda': 0.052544479432145054}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:07,219] Trial 11 finished with value: 0.6913935957211846 and parameters: {'learning_rate': 0.009968480456979933, 'n_estimators': 2850, 'num_leaves': 182, 'max_depth': 6, 'min_child_samples': 12, 'subsample': 0.7018196690481528, 'colsample_bytree': 0.65652501406895, 'reg_alpha': 0.158450516311406, 'reg_lambda': 0.2051395698775704}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:11,508] Trial 12 finished with value: 0.6944271331324028 and parameters: {'learning_rate': 0.008878240006044237, 'n_estimators': 2150, 'num_leaves': 42, 'max_depth': 4, 'min_child_samples': 37, 'subsample': 0.7118852047721561, 'colsample_bytree': 0.9055080708566403, 'reg_alpha': 6.035089212563596e-05, 'reg_lambda': 0.00020910376679315922}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:14,717] Trial 13 finished with value: 0.6937197394369514 and parameters: {'learning_rate': 0.013136887848709474, 'n_estimators': 2050, 'num_leaves': 40, 'max_depth': 4, 'min_child_samples': 66, 'subsample': 0.7861150271405694, 'colsample_bytree': 0.9655221159643753, 'reg_alpha': 1.7759741320508232e-05, 'reg_lambda': 3.904986695784336e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:16,128] Trial 14 finished with value: 0.6289975428358396 and parameters: {'learning_rate': 0.0052070635286817915, 'n_estimators': 2350, 'num_leaves': 48, 'max_depth': 1, 'min_child_samples': 49, 'subsample': 0.7962895837138877, 'colsample_bytree': 0.9558596210962261, 'reg_alpha': 0.02765336504224308, 'reg_lambda': 0.01982582827764617}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:20,871] Trial 15 finished with value: 0.6888319255834604 and parameters: {'learning_rate': 0.019498894988268584, 'n_estimators': 2650, 'num_leaves': 95, 'max_depth': 7, 'min_child_samples': 13, 'subsample': 0.6613030423794282, 'colsample_bytree': 0.7826434104330384, 'reg_alpha': 1.3299756574033045e-05, 'reg_lambda': 0.00043674420231023386}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:22,034] Trial 16 finished with value: 0.6884870002897155 and parameters: {'learning_rate': 0.04826498450536064, 'n_estimators': 2550, 'num_leaves': 21, 'max_depth': 4, 'min_child_samples': 92, 'subsample': 0.6562238614545011, 'colsample_bytree': 0.9084147239970881, 'reg_alpha': 0.04049405991126566, 'reg_lambda': 3.7617282614896515e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:30,361] Trial 17 finished with value: 0.6923124380364228 and parameters: {'learning_rate': 0.005081355503726294, 'n_estimators': 1900, 'num_leaves': 20, 'max_depth': 11, 'min_child_samples': 30, 'subsample': 0.6966647850249827, 'colsample_bytree': 0.913232134742366, 'reg_alpha': 2.0807883744584744e-05, 'reg_lambda': 0.003365367295332745}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:32,787] Trial 18 finished with value: 0.6939559158268017 and parameters: {'learning_rate': 0.035947283304032907, 'n_estimators': 1450, 'num_leaves': 42, 'max_depth': 6, 'min_child_samples': 70, 'subsample': 0.8269886692999311, 'colsample_bytree': 0.7894951950025773, 'reg_alpha': 0.10873123560905236, 'reg_lambda': 1.0637726689480285e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:45,212] Trial 19 finished with value: 0.6792194845097258 and parameters: {'learning_rate': 0.00270915192176569, 'n_estimators': 1150, 'num_leaves': 51, 'max_depth': 0, 'min_child_samples': 15, 'subsample': 0.7036765764879205, 'colsample_bytree': 0.9342995467900507, 'reg_alpha': 1.6401832089647914e-05, 'reg_lambda': 7.613728206611868e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:48,995] Trial 20 finished with value: 0.6908974443137487 and parameters: {'learning_rate': 0.010119595292601797, 'n_estimators': 2700, 'num_leaves': 20, 'max_depth': 3, 'min_child_samples': 64, 'subsample': 0.7825193036269266, 'colsample_bytree': 0.9664655233388606, 'reg_alpha': 1.358344522917136e-05, 'reg_lambda': 0.24669467398625536}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:51,589] Trial 21 finished with value: 0.6962110311483385 and parameters: {'learning_rate': 0.03616162132183676, 'n_estimators': 1850, 'num_leaves': 31, 'max_depth': 8, 'min_child_samples': 21, 'subsample': 0.7553133482119524, 'colsample_bytree': 0.772753264315744, 'reg_alpha': 0.2658676383447176, 'reg_lambda': 2.8573446345140887e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:53,214] Trial 22 finished with value: 0.6900273109517808 and parameters: {'learning_rate': 0.07403902238807089, 'n_estimators': 2150, 'num_leaves': 29, 'max_depth': 12, 'min_child_samples': 42, 'subsample': 0.802112618593719, 'colsample_bytree': 0.7386524679634476, 'reg_alpha': 0.44714053113524793, 'reg_lambda': 9.484595856658244e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:18:56,818] Trial 23 finished with value: 0.6784321315535069 and parameters: {'learning_rate': 0.006659938859328871, 'n_estimators': 1850, 'num_leaves': 35, 'max_depth': 4, 'min_child_samples': 10, 'subsample': 0.7842648207932795, 'colsample_bytree': 0.8002623793079308, 'reg_alpha': 6.7353834882080585, 'reg_lambda': 0.000648652062322818}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:19:00,880] Trial 24 finished with value: 0.6918542002729692 and parameters: {'learning_rate': 0.008234271361363024, 'n_estimators': 1750, 'num_leaves': 127, 'max_depth': 4, 'min_child_samples': 20, 'subsample': 0.7061079397175543, 'colsample_bytree': 0.9870392185950929, 'reg_alpha': 0.0003536153829987422, 'reg_lambda': 0.001351262932854882}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:19:09,987] Trial 25 finished with value: 0.6939780661488587 and parameters: {'learning_rate': 0.004718910664980987, 'n_estimators': 2600, 'num_leaves': 34, 'max_depth': 5, 'min_child_samples': 31, 'subsample': 0.6862200466669891, 'colsample_bytree': 0.9354925770816529, 'reg_alpha': 0.0021177080891036197, 'reg_lambda': 0.004827765407906431}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:19:11,600] Trial 26 finished with value: 0.6898277603126107 and parameters: {'learning_rate': 0.09383281093251866, 'n_estimators': 1100, 'num_leaves': 32, 'max_depth': 10, 'min_child_samples': 31, 'subsample': 0.7068300997045969, 'colsample_bytree': 0.7036867169965233, 'reg_alpha': 0.023543471400866572, 'reg_lambda': 9.134281924014848e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:19:16,056] Trial 27 finished with value: 0.6892159974112423 and parameters: {'learning_rate': 0.019042717171184855, 'n_estimators': 2300, 'num_leaves': 62, 'max_depth': 9, 'min_child_samples': 44, 'subsample': 0.6775994596835578, 'colsample_bytree': 0.7322017630627324, 'reg_alpha': 0.2846153479260884, 'reg_lambda': 4.282576600305326e-06}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:19:18,806] Trial 28 finished with value: 0.6851349055931086 and parameters: {'learning_rate': 0.0653557659633067, 'n_estimators': 2050, 'num_leaves': 65, 'max_depth': 0, 'min_child_samples': 12, 'subsample': 0.7123445610914511, 'colsample_bytree': 0.9413657288718607, 'reg_alpha': 0.0005975869146098432, 'reg_lambda': 4.013270379601583e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:19:20,832] Trial 29 finished with value: 0.6876181660767354 and parameters: {'learning_rate': 0.04727375382207466, 'n_estimators': 2350, 'num_leaves': 58, 'max_depth': 7, 'min_child_samples': 60, 'subsample': 0.722145633940952, 'colsample_bytree': 0.9705133867950101, 'reg_alpha': 0.017807344689698593, 'reg_lambda': 0.009169497639786058}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:19:28,912] Trial 30 finished with value: 0.6797749647423764 and parameters: {'learning_rate': 0.002805220568103408, 'n_estimators': 2000, 'num_leaves': 82, 'max_depth': 0, 'min_child_samples': 34, 'subsample': 0.8291938854703227, 'colsample_bytree': 0.813280147708319, 'reg_alpha': 0.00015735789227971293, 'reg_lambda': 2.3787308861046742e-05}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:19:33,135] Trial 31 finished with value: 0.6705144625614712 and parameters: {'learning_rate': 0.003411849500087014, 'n_estimators': 2800, 'num_leaves': 36, 'max_depth': 3, 'min_child_samples': 20, 'subsample': 0.6329610829415775, 'colsample_bytree': 0.9077514091160288, 'reg_alpha': 0.0007313005237020366, 'reg_lambda': 0.11186144060478694}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:19:53,261] Trial 32 finished with value: 0.6874315987818386 and parameters: {'learning_rate': 0.003166280924793566, 'n_estimators': 2900, 'num_leaves': 72, 'max_depth': 10, 'min_child_samples': 41, 'subsample': 0.6148657228662617, 'colsample_bytree': 0.9792766384487686, 'reg_alpha': 0.0011187260467032472, 'reg_lambda': 0.02047596524453592}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:19:57,378] Trial 33 finished with value: 0.6932084347933215 and parameters: {'learning_rate': 0.012765437642311528, 'n_estimators': 2700, 'num_leaves': 29, 'max_depth': 5, 'min_child_samples': 20, 'subsample': 0.6429656869911553, 'colsample_bytree': 0.924185747694989, 'reg_alpha': 0.14529124568199647, 'reg_lambda': 0.00042521776328019803}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:20:02,539] Trial 34 finished with value: 0.675896276535427 and parameters: {'learning_rate': 0.0028642458384316714, 'n_estimators': 2450, 'num_leaves': 77, 'max_depth': 4, 'min_child_samples': 63, 'subsample': 0.6499031421211942, 'colsample_bytree': 0.9879830912702902, 'reg_alpha': 0.00029861771643458184, 'reg_lambda': 0.00011637567322168598}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:20:10,680] Trial 35 finished with value: 0.6922737021588954 and parameters: {'learning_rate': 0.009156604658555605, 'n_estimators': 2800, 'num_leaves': 38, 'max_depth': 7, 'min_child_samples': 59, 'subsample': 0.7910931425610509, 'colsample_bytree': 0.8902741344127775, 'reg_alpha': 0.0006078979284038216, 'reg_lambda': 0.00024491589535369966}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:20:13,152] Trial 36 finished with value: 0.6879400581729044 and parameters: {'learning_rate': 0.06386830691709665, 'n_estimators': 2000, 'num_leaves': 140, 'max_depth': 8, 'min_child_samples': 22, 'subsample': 0.7634508330905897, 'colsample_bytree': 0.7898067391002757, 'reg_alpha': 1.0451833184278345, 'reg_lambda': 0.00021531602961597025}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:20:14,493] Trial 37 finished with value: 0.6883427179890856 and parameters: {'learning_rate': 0.09436190466923815, 'n_estimators': 1950, 'num_leaves': 28, 'max_depth': 2, 'min_child_samples': 21, 'subsample': 0.730559168296573, 'colsample_bytree': 0.8354993239668392, 'reg_alpha': 2.2300390511983776, 'reg_lambda': 1.9934231520192962e-06}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:20:15,967] Trial 38 finished with value: 0.6856595032373625 and parameters: {'learning_rate': 0.04566598711712238, 'n_estimators': 2300, 'num_leaves': 28, 'max_depth': 4, 'min_child_samples': 87, 'subsample': 0.6042533443093878, 'colsample_bytree': 0.8899953776446395, 'reg_alpha': 2.7472083692378644e-05, 'reg_lambda': 0.018995945689181885}. Best is trial 0 with value: 0.6992533795477553.\n",
      "[I 2025-12-11 18:20:21,787] Trial 39 finished with value: 0.6915823367821509 and parameters: {'learning_rate': 0.005845496298781776, 'n_estimators': 1750, 'num_leaves': 41, 'max_depth': 5, 'min_child_samples': 44, 'subsample': 0.7054317676908851, 'colsample_bytree': 0.8344503098781375, 'reg_alpha': 0.00016578725503441744, 'reg_lambda': 0.005222024698498307}. Best is trial 0 with value: 0.6992533795477553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA][LGBM] Mejor AP(val): 0.699253\n",
      "[EXPORT][LGBM] LGBM_FULL_SMOTENC_TUNED -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_FULL_SMOTENC/export\n"
     ]
    }
   ],
   "source": [
    "def tune_xgb_with_optuna(seed_params, X_train_final, y_train_final, X_val_fit, y_val, feature_names_used):\n",
    "    \"\"\"\n",
    "    Tuning de XGBoost maximizando AP(val) con Optuna.\n",
    "    Devuelve:\n",
    "      - adapter entrenado (XGBAdapter)\n",
    "      - best_params (dict con hiperparámetros finales)\n",
    "    \"\"\"\n",
    "    N_TRIALS = 40\n",
    "    STUDY_NAME = f\"XGB_{VIEW_TAG}_{BAL_TAG}_AP\"\n",
    "    sampler = TPESampler(seed=RANDOM_STATE, multivariate=True, group=False)\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                study_name=STUDY_NAME,\n",
    "                                sampler=sampler)\n",
    "\n",
    "    SEARCH_KEYS = [\n",
    "        \"learning_rate\", \"n_estimators\", \"max_depth\", \"min_child_weight\",\n",
    "        \"subsample\", \"colsample_bytree\", \"gamma\", \"reg_alpha\", \"reg_lambda\"\n",
    "    ]\n",
    "\n",
    "    best_fp = _best_file(\"xgb\")\n",
    "    if best_fp.exists():\n",
    "        try:\n",
    "            prev = json.loads(best_fp.read_text())\n",
    "            warm = {k: prev[k] for k in SEARCH_KEYS if k in prev}\n",
    "            if warm:\n",
    "                print(\"[OPTUNA][XGB] Enqueue BEST anterior.\")\n",
    "                study.enqueue_trial(warm)\n",
    "        except Exception as e:\n",
    "            print(\"[OPTUNA][XGB] Aviso warm-start:\", e)\n",
    "\n",
    "    def suggest(trial):\n",
    "        p = {}\n",
    "        p[\"learning_rate\"]    = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "        p[\"n_estimators\"]     = trial.suggest_int(\"n_estimators\", 800, 3000, step=50)\n",
    "        p[\"max_depth\"]        = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "        p[\"min_child_weight\"] = trial.suggest_float(\"min_child_weight\", 0.5, 20.0, log=True)\n",
    "        p[\"subsample\"]        = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        p[\"colsample_bytree\"] = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
    "        p[\"gamma\"]            = trial.suggest_float(\"gamma\", 1e-8, 5.0, log=True)\n",
    "        p[\"reg_alpha\"]        = trial.suggest_float(\"reg_alpha\", 1e-6, 10.0, log=True)\n",
    "        p[\"reg_lambda\"]       = trial.suggest_float(\"reg_lambda\", 1e-6, 10.0, log=True)\n",
    "        # Fijos\n",
    "        p[\"random_state\"]     = RANDOM_STATE\n",
    "        p[\"n_jobs\"]           = -1\n",
    "        p[\"eval_metric\"]      = \"aucpr\"\n",
    "        p[\"tree_method\"]      = \"hist\"\n",
    "        p[\"verbosity\"]        = 0\n",
    "        return p\n",
    "\n",
    "    def objective(trial):\n",
    "        hp = suggest(trial)\n",
    "        mdl = XGBClassifier(**{**seed_params, **hp})\n",
    "        mdl = xgb_fit_with_es(\n",
    "            mdl,\n",
    "            X_train_final, y_train_final,\n",
    "            X_val_fit, y_val,\n",
    "            feature_names=feature_names_used,\n",
    "            rounds=200,\n",
    "            verbose=False\n",
    "        )\n",
    "        proba_val = mdl.predict_proba(X_val_fit)[:, 1]\n",
    "        ap = average_precision_score(y_val, proba_val)\n",
    "        trial.set_user_attr(\"best_iteration\", getattr(mdl, \"best_iteration\", None))\n",
    "        return ap\n",
    "\n",
    "    print(f\"[OPTUNA][XGB] Iniciando {N_TRIALS} pruebas...\")\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "    best = study.best_trial\n",
    "    print(f\"[OPTUNA][XGB] Mejor AP(val): {best.value:.6f}\")\n",
    "\n",
    "    best_params = dict(best.params)\n",
    "    best_params.update({\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"n_jobs\": -1,\n",
    "        \"eval_metric\": \"aucpr\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"verbosity\": 0,\n",
    "    })\n",
    "\n",
    "    # Guardamos BEST global\n",
    "    with open(best_fp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Re-entrenamos con los mejores hiperparámetros\n",
    "    tuned_clf = XGBClassifier(**{**seed_params, **best_params})\n",
    "    tuned_adapter = xgb_fit_with_es(\n",
    "        tuned_clf,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names=feature_names_used,\n",
    "        rounds=200,\n",
    "        verbose=False\n",
    "    )\n",
    "    return tuned_adapter, best_params\n",
    "\n",
    "\n",
    "def tune_lgbm_with_optuna(seed_params, X_train_final, y_train_final, X_val_fit, y_val, feature_names_used):\n",
    "    N_TRIALS = 40\n",
    "    STUDY_NAME = f\"LGBM_{VIEW_TAG}_{BAL_TAG}_AP\"\n",
    "    sampler = TPESampler(seed=RANDOM_STATE, multivariate=True, group=False)\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                study_name=STUDY_NAME,\n",
    "                                sampler=sampler)\n",
    "\n",
    "    SEARCH_KEYS = [\n",
    "        \"learning_rate\", \"n_estimators\", \"num_leaves\", \"max_depth\",\n",
    "        \"min_child_samples\", \"subsample\", \"colsample_bytree\",\n",
    "        \"reg_alpha\", \"reg_lambda\"\n",
    "    ]\n",
    "\n",
    "    best_fp = _best_file(\"lgbm\")\n",
    "    if best_fp.exists():\n",
    "        try:\n",
    "            prev = json.loads(best_fp.read_text())\n",
    "            warm = {k: prev[k] for k in SEARCH_KEYS if k in prev}\n",
    "            if warm:\n",
    "                print(\"[OPTUNA][LGBM] Enqueue BEST anterior.\")\n",
    "                study.enqueue_trial(warm)\n",
    "        except Exception as e:\n",
    "            print(\"[OPTUNA][LGBM] Aviso warm-start:\", e)\n",
    "\n",
    "    def suggest(trial):\n",
    "        p = {}\n",
    "        p[\"learning_rate\"]     = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "        p[\"n_estimators\"]      = trial.suggest_int(\"n_estimators\", 800, 3000, step=50)\n",
    "        p[\"num_leaves\"]        = trial.suggest_int(\"num_leaves\", 16, 256)\n",
    "        p[\"max_depth\"]         = trial.suggest_int(\"max_depth\", -1, 12)\n",
    "        p[\"min_child_samples\"] = trial.suggest_int(\"min_child_samples\", 5, 200)\n",
    "        p[\"subsample\"]         = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        p[\"colsample_bytree\"]  = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
    "        p[\"reg_alpha\"]         = trial.suggest_float(\"reg_alpha\", 1e-6, 10.0, log=True)\n",
    "        p[\"reg_lambda\"]        = trial.suggest_float(\"reg_lambda\", 1e-6, 10.0, log=True)\n",
    "        return p\n",
    "\n",
    "    def objective(trial):\n",
    "        hp = suggest(trial)\n",
    "        params = {**seed_params, **hp}\n",
    "        params.setdefault(\"objective\", \"binary\")\n",
    "        params.setdefault(\"metric\", \"average_precision\")\n",
    "        params.setdefault(\"random_state\", RANDOM_STATE)\n",
    "        params.setdefault(\"n_jobs\", -1)\n",
    "\n",
    "        base_clf = LGBMClassifier(**params)\n",
    "        adapter = lgbm_fit_with_es(\n",
    "            base_clf,\n",
    "            X_train_final, y_train_final,\n",
    "            X_val_fit, y_val,\n",
    "            feature_names=feature_names_used,\n",
    "            rounds=200,\n",
    "            verbose=False\n",
    "        )\n",
    "        proba_val = adapter.predict_proba(X_val_fit)[:, 1]\n",
    "        ap = average_precision_score(y_val, proba_val)\n",
    "        trial.set_user_attr(\"best_iteration\", adapter.best_iteration)\n",
    "        return ap\n",
    "\n",
    "    print(f\"[OPTUNA][LGBM] Iniciando {N_TRIALS} pruebas...\")\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "    best = study.best_trial\n",
    "    print(f\"[OPTUNA][LGBM] Mejor AP(val): {best.value:.6f}\")\n",
    "\n",
    "    best_params = dict(best.params)\n",
    "    best_params.update({\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"average_precision\",\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"n_jobs\": -1,\n",
    "    })\n",
    "\n",
    "    # Guardamos BEST global\n",
    "    with open(best_fp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    tuned_base = LGBMClassifier(**{**seed_params, **best_params})\n",
    "    tuned_adapter = lgbm_fit_with_es(\n",
    "        tuned_base,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names=feature_names_used,\n",
    "        rounds=200,\n",
    "        verbose=False\n",
    "    )\n",
    "    return tuned_adapter, best_params\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#   EJECUCIÓN DEL TUNING + RE-ENTRENO + GUARDADO DE preds_val_*\n",
    "# ============================================================\n",
    "\n",
    "# Por si acaso, definimos nombres base\n",
    "EXP_NAME_XGB = f\"XGB_{VIEW_TAG}_{BAL_TAG}\"\n",
    "EXP_NAME_LGB = f\"LGBM_{VIEW_TAG}_{BAL_TAG}\"\n",
    "\n",
    "# Directorios para XGB\n",
    "bx = DIRS[\"xgb\"]\n",
    "OUT_RESULTS_X = bx / \"results\"\n",
    "OUT_FIGS_X    = bx / \"figs\"\n",
    "OUT_PREDS_X   = bx / \"preds\"\n",
    "OUT_PARAMS_X  = bx / \"best_params\"\n",
    "OUT_EXPORT_X  = bx / \"export\"\n",
    "\n",
    "# Directorios para LGBM\n",
    "bl = DIRS[\"lgbm\"]\n",
    "OUT_RESULTS_L = bl / \"results\"\n",
    "OUT_FIGS_L    = bl / \"figs\"\n",
    "OUT_PREDS_L   = bl / \"preds\"\n",
    "OUT_PARAMS_L  = bl / \"best_params\"\n",
    "OUT_EXPORT_L  = bl / \"export\"\n",
    "\n",
    "# === XGB TUNED ===\n",
    "if DO_TUNE_XGB:\n",
    "    xgb_tuned_adapter, xgb_best_params = tune_xgb_with_optuna(\n",
    "        xgb_params_seed,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names_used\n",
    "    )\n",
    "    xgb_tuned_model = xgb_tuned_adapter  \n",
    "\n",
    "    # Predicciones y métricas en VAL\n",
    "    proba_val_xgb_tuned = xgb_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "    thr_val_xgb_tuned, _ = find_best_threshold(y_val, proba_val_xgb_tuned, metric=\"f1\")\n",
    "    val_metrics_xgb_tuned = compute_all_metrics(y_val, proba_val_xgb_tuned, thr_val_xgb_tuned)\n",
    "\n",
    "    # Predicciones y métricas en TEST\n",
    "    proba_test_xgb_tuned = xgb_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "    test_metrics_xgb_tuned = compute_all_metrics(y_test, proba_test_xgb_tuned, thr_val_xgb_tuned)\n",
    "    y_pred_test_xgb_tuned = (proba_test_xgb_tuned >= thr_val_xgb_tuned).astype(int)\n",
    "\n",
    "    # --- GUARDAMOS preds_val_* y preds_test_* ---\n",
    "    EXP_NAME_XGB_TUNED = f\"{EXP_NAME_XGB}_TUNED\"\n",
    "\n",
    "    pd.DataFrame({\"proba\": proba_val_xgb_tuned, \"y_true\": y_val}).to_parquet(\n",
    "        OUT_PREDS_X / f\"preds_val_{EXP_NAME_XGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame({\"proba\": proba_test_xgb_tuned, \"y_true\": y_test}).to_parquet(\n",
    "        OUT_PREDS_X / f\"preds_test_{EXP_NAME_XGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Guardamos best_params específicos de TUNED\n",
    "    with open(OUT_PARAMS_X / f\"{EXP_NAME_XGB_TUNED}_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(xgb_best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Plots TUNED\n",
    "    plot_pr_curve(y_val,  proba_val_xgb_tuned,\n",
    "                  f\"{EXP_NAME_XGB_TUNED} — PR (val)\",\n",
    "                  OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_pr_val.png\")\n",
    "    plot_pr_curve(y_test, proba_test_xgb_tuned,\n",
    "                  f\"{EXP_NAME_XGB_TUNED} — PR (test)\",\n",
    "                  OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_pr_test.png\")\n",
    "    plot_roc_curve(y_val,  proba_val_xgb_tuned,\n",
    "                   f\"{EXP_NAME_XGB_TUNED} — ROC (val)\",\n",
    "                   OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_roc_val.png\")\n",
    "    plot_roc_curve(y_test, proba_test_xgb_tuned,\n",
    "                   f\"{EXP_NAME_XGB_TUNED} — ROC (test)\",\n",
    "                   OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_roc_test.png\")\n",
    "    plot_confusion(\n",
    "        y_test,\n",
    "        y_pred_test_xgb_tuned,\n",
    "        f\"{EXP_NAME_XGB_TUNED} — Confusion (test @thr={thr_val_xgb_tuned:.3f})\",\n",
    "        OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_cm_test.png\"\n",
    "    )\n",
    "\n",
    "    # Registro tuned.csv\n",
    "    best_iter_tuned_xgb = getattr(xgb_tuned_adapter, \"best_iteration\", None)\n",
    "    row_tuned_xgb = {\n",
    "        \"model\": EXP_NAME_XGB_TUNED,\n",
    "        \"thr_val\": thr_val_xgb_tuned,\n",
    "        \"val_pr_auc\": val_metrics_xgb_tuned[\"pr_auc\"],\n",
    "        \"val_roc_auc\": val_metrics_xgb_tuned[\"roc_auc\"],\n",
    "        \"val_precision\": val_metrics_xgb_tuned[\"precision\"],\n",
    "        \"val_f1\": val_metrics_xgb_tuned[\"f1\"],\n",
    "        \"val_recall\": val_metrics_xgb_tuned[\"recall\"],\n",
    "        \"val_bal_acc\": val_metrics_xgb_tuned[\"bal_acc\"],\n",
    "        \"test_pr_auc\": test_metrics_xgb_tuned[\"pr_auc\"],\n",
    "        \"test_roc_auc\": test_metrics_xgb_tuned[\"roc_auc\"],\n",
    "        \"test_precision\": test_metrics_xgb_tuned[\"precision\"],\n",
    "        \"test_f1\": test_metrics_xgb_tuned[\"f1\"],\n",
    "        \"test_recall\": test_metrics_xgb_tuned[\"recall\"],\n",
    "        \"test_bal_acc\": test_metrics_xgb_tuned[\"bal_acc\"],\n",
    "        \"best_iteration\": best_iter_tuned_xgb if best_iter_tuned_xgb is not None else np.nan,\n",
    "    }\n",
    "    csv_x_tuned = OUT_RESULTS_X / \"tuned.csv\"\n",
    "    pd.DataFrame([row_tuned_xgb]).to_csv(\n",
    "        csv_x_tuned,\n",
    "        mode=(\"a\" if csv_x_tuned.exists() else \"w\"),\n",
    "        index=False,\n",
    "        header=not csv_x_tuned.exists()\n",
    "    )\n",
    "\n",
    "    # Export XGB TUNED\n",
    "    export_xgb(\n",
    "        adapter=xgb_tuned_adapter,\n",
    "        out_dir=OUT_EXPORT_X,\n",
    "        exp_name=EXP_NAME_XGB_TUNED,\n",
    "        feature_names=feature_names_used,\n",
    "        threshold=thr_val_xgb_tuned,\n",
    "        val_metrics=val_metrics_xgb_tuned,\n",
    "        test_metrics=test_metrics_xgb_tuned,\n",
    "    )\n",
    "\n",
    "\n",
    "# === LGBM TUNED ===\n",
    "if DO_TUNE_LGBM:\n",
    "    lgbm_tuned_adapter, lgbm_best_params = tune_lgbm_with_optuna(\n",
    "        lgbm_params_seed,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names_used\n",
    "    )\n",
    "    # Alias de compatibilidad\n",
    "    lgbm_tuned_model = lgbm_tuned_adapter  \n",
    "\n",
    "    # Predicciones y métricas en VAL\n",
    "    proba_val_lgb_tuned = lgbm_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "    thr_val_lgb_tuned, _ = find_best_threshold(y_val, proba_val_lgb_tuned, metric=\"f1\")\n",
    "    val_metrics_lgb_tuned = compute_all_metrics(y_val, proba_val_lgb_tuned, thr_val_lgb_tuned)\n",
    "\n",
    "    # Predicciones y métricas en TEST\n",
    "    proba_test_lgb_tuned = lgbm_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "    test_metrics_lgb_tuned = compute_all_metrics(y_test, proba_test_lgb_tuned, thr_val_lgb_tuned)\n",
    "    y_pred_test_lgb_tuned = (proba_test_lgb_tuned >= thr_val_lgb_tuned).astype(int)\n",
    "\n",
    "    EXP_NAME_LGB_TUNED = f\"{EXP_NAME_LGB}_TUNED\"\n",
    "\n",
    "    pd.DataFrame({\"proba\": proba_val_lgb_tuned, \"y_true\": y_val}).to_parquet(\n",
    "        OUT_PREDS_L / f\"preds_val_{EXP_NAME_LGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame({\"proba\": proba_test_lgb_tuned, \"y_true\": y_test}).to_parquet(\n",
    "        OUT_PREDS_L / f\"preds_test_{EXP_NAME_LGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    with open(OUT_PARAMS_L / f\"{EXP_NAME_LGB_TUNED}_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(lgbm_best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Plots TUNED\n",
    "    plot_pr_curve(y_val,  proba_val_lgb_tuned,\n",
    "                  f\"{EXP_NAME_LGB_TUNED} — PR (val)\",\n",
    "                  OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_pr_val.png\")\n",
    "    plot_pr_curve(y_test, proba_test_lgb_tuned,\n",
    "                  f\"{EXP_NAME_LGB_TUNED} — PR (test)\",\n",
    "                  OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_pr_test.png\")\n",
    "    plot_roc_curve(y_val,  proba_val_lgb_tuned,\n",
    "                   f\"{EXP_NAME_LGB_TUNED} — ROC (val)\",\n",
    "                   OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_roc_val.png\")\n",
    "    plot_roc_curve(y_test, proba_test_lgb_tuned,\n",
    "                   f\"{EXP_NAME_LGB_TUNED} — ROC (test)\",\n",
    "                   OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_roc_test.png\")\n",
    "    plot_confusion(\n",
    "        y_test,\n",
    "        y_pred_test_lgb_tuned,\n",
    "        f\"{EXP_NAME_LGB_TUNED} — Confusion (test @thr={thr_val_lgb_tuned:.3f})\",\n",
    "        OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_cm_test.png\"\n",
    "    )\n",
    "\n",
    "    # Registro tuned.csv\n",
    "    row_tuned_lgb = {\n",
    "        \"model\": EXP_NAME_LGB_TUNED,\n",
    "        \"thr_val\": thr_val_lgb_tuned,\n",
    "        \"val_pr_auc\": val_metrics_lgb_tuned[\"pr_auc\"],\n",
    "        \"val_roc_auc\": val_metrics_lgb_tuned[\"roc_auc\"],\n",
    "        \"val_precision\": val_metrics_lgb_tuned[\"precision\"],\n",
    "        \"val_f1\": val_metrics_lgb_tuned[\"f1\"],\n",
    "        \"val_recall\": val_metrics_lgb_tuned[\"recall\"],\n",
    "        \"val_bal_acc\": val_metrics_lgb_tuned[\"bal_acc\"],\n",
    "        \"test_pr_auc\": test_metrics_lgb_tuned[\"pr_auc\"],\n",
    "        \"test_roc_auc\": test_metrics_lgb_tuned[\"roc_auc\"],\n",
    "        \"test_precision\": test_metrics_lgb_tuned[\"precision\"],\n",
    "        \"test_f1\": test_metrics_lgb_tuned[\"f1\"],\n",
    "        \"test_recall\": test_metrics_lgb_tuned[\"recall\"],\n",
    "        \"test_bal_acc\": test_metrics_lgb_tuned[\"bal_acc\"],\n",
    "        \"best_iteration\": lgbm_tuned_adapter.best_iteration if hasattr(lgbm_tuned_adapter, \"best_iteration\") else np.nan,\n",
    "    }\n",
    "    csv_l_tuned = OUT_RESULTS_L / \"tuned.csv\"\n",
    "    pd.DataFrame([row_tuned_lgb]).to_csv(\n",
    "        csv_l_tuned,\n",
    "        mode=(\"a\" if csv_l_tuned.exists() else \"w\"),\n",
    "        index=False,\n",
    "        header=not csv_l_tuned.exists()\n",
    "    )\n",
    "\n",
    "    # Export LGBM TUNED\n",
    "    export_lgbm(\n",
    "        adapter=lgbm_tuned_adapter,\n",
    "        out_dir=OUT_EXPORT_L,\n",
    "        exp_name=EXP_NAME_LGB_TUNED,\n",
    "        feature_names=feature_names_used,\n",
    "        threshold=thr_val_lgb_tuned,\n",
    "        val_metrics=val_metrics_lgb_tuned,\n",
    "        test_metrics=test_metrics_lgb_tuned,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94dfcfe",
   "metadata": {},
   "source": [
    "9 — Ensemble híbrido (stacking + soft voting XGB + LGBM + DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cc70a583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SOFT VOTING (val) ===\n",
      "{'pr_auc': 0.7002532353715107, 'roc_auc': 0.8610922170244204, 'precision': 0.6143497757847534, 'f1': 0.6424384525205158, 'recall': 0.6732186732186732, 'bal_acc': 0.7826231470299267}\n",
      "\n",
      "=== SOFT VOTING (test) ===\n",
      "{'pr_auc': 0.7096893186534421, 'roc_auc': 0.860865487984132, 'precision': 0.5907079646017699, 'f1': 0.6216530849825378, 'recall': 0.6560196560196561, 'bal_acc': 0.7699432868924394}\n",
      "\n",
      "=== STACKING (val) ===\n",
      "{'pr_auc': 0.7013753952350565, 'roc_auc': 0.862559015101388, 'precision': 0.640661938534279, 'f1': 0.653012048192771, 'recall': 0.6658476658476659, 'bal_acc': 0.7852151072490056}\n",
      "\n",
      "=== STACKING (test) ===\n",
      "{'pr_auc': 0.7091312422680297, 'roc_auc': 0.8612001832340814, 'precision': 0.5944700460829493, 'f1': 0.6135552913198573, 'recall': 0.6339066339066339, 'bal_acc': 0.7617116345929905}\n",
      "\n",
      "[OK][ENSEMBLE] Métricas, preds y pesos del modelo híbrido guardados en:\n",
      "  RESULTS: /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_FULL_SMOTENC/results\n",
      "  PREDS:   /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_FULL_SMOTENC/preds\n",
      "  PARAMS:  /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_FULL_SMOTENC/best_params\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "EXP_NAME_ENS = f\"ENS_{VIEW_TAG}_{BAL_TAG}\"\n",
    "be = DIRS[\"ens\"]\n",
    "OUT_RESULTS_E = be / \"results\"\n",
    "OUT_FIGS_E    = be / \"figs\"\n",
    "OUT_PREDS_E   = be / \"preds\"\n",
    "OUT_PARAMS_E  = be / \"best_params\"\n",
    "\n",
    "OUT_RESULTS_E.mkdir(parents=True, exist_ok=True)\n",
    "OUT_FIGS_E.mkdir(parents=True, exist_ok=True)\n",
    "OUT_PREDS_E.mkdir(parents=True, exist_ok=True)\n",
    "OUT_PARAMS_E.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "proba_val_xgb_tuned  = xgb_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "proba_test_xgb_tuned = xgb_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "\n",
    "proba_val_lgb_tuned  = lgbm_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "proba_test_lgb_tuned = lgbm_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "\n",
    "dl_preds_val_path  = DIRS[\"dl\"] / \"preds\" / f\"preds_val_DL_{VIEW_TAG}_{BAL_TAG}.parquet\"\n",
    "dl_preds_test_path = DIRS[\"dl\"] / \"preds\" / f\"preds_test_DL_{VIEW_TAG}_{BAL_TAG}.parquet\"\n",
    "\n",
    "df_dl_val  = pd.read_parquet(dl_preds_val_path)\n",
    "df_dl_test = pd.read_parquet(dl_preds_test_path)\n",
    "\n",
    "proba_val_dl  = df_dl_val[\"proba\"].to_numpy()\n",
    "proba_test_dl = df_dl_test[\"proba\"].to_numpy()\n",
    "\n",
    "assert len(proba_val_dl)  == len(y_val),  \"[DL] Longitud de preds val != y_val\"\n",
    "assert len(proba_test_dl) == len(y_test), \"[DL] Longitud de preds test != y_test\"\n",
    "\n",
    "\n",
    "proba_val_soft = (proba_val_xgb_tuned + proba_val_lgb_tuned + proba_val_dl) / 3.0\n",
    "thr_val_soft, f1_val_soft = find_best_threshold(y_val, proba_val_soft, metric=\"f1\")\n",
    "metrics_val_soft  = compute_all_metrics(y_val,  proba_val_soft,  thr_val_soft)\n",
    "\n",
    "proba_test_soft = (proba_test_xgb_tuned + proba_test_lgb_tuned + proba_test_dl) / 3.0\n",
    "metrics_test_soft = compute_all_metrics(y_test, proba_test_soft, thr_val_soft)\n",
    "y_pred_test_soft  = (proba_test_soft >= thr_val_soft).astype(int)\n",
    "\n",
    "Z_val  = np.column_stack([proba_val_xgb_tuned,  proba_val_lgb_tuned,  proba_val_dl])\n",
    "Z_test = np.column_stack([proba_test_xgb_tuned, proba_test_lgb_tuned, proba_test_dl])\n",
    "\n",
    "stack_clf = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=1.0,\n",
    "    solver=\"liblinear\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "stack_clf.fit(Z_val, y_val)\n",
    "\n",
    "proba_val_stack = stack_clf.predict_proba(Z_val)[:, 1]\n",
    "thr_val_stack, f1_val_stack = find_best_threshold(y_val, proba_val_stack, metric=\"f1\")\n",
    "metrics_val_stack  = compute_all_metrics(y_val,  proba_val_stack,  thr_val_stack)\n",
    "\n",
    "proba_test_stack = stack_clf.predict_proba(Z_test)[:, 1]\n",
    "metrics_test_stack = compute_all_metrics(y_test, proba_test_stack, thr_val_stack)\n",
    "y_pred_test_stack  = (proba_test_stack >= thr_val_stack).astype(int)\n",
    "\n",
    "\n",
    "plot_pr_curve(y_val,  proba_val_soft,\n",
    "              f\"{EXP_NAME_ENS}_SOFT — PR (val)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_soft,\n",
    "              f\"{EXP_NAME_ENS}_SOFT — PR (test)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_pr_test.png\")\n",
    "\n",
    "plot_pr_curve(y_val,  proba_val_stack,\n",
    "              f\"{EXP_NAME_ENS}_STACK — PR (val)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_stack,\n",
    "              f\"{EXP_NAME_ENS}_STACK — PR (test)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_pr_test.png\")\n",
    "\n",
    "plot_roc_curve(y_val,  proba_val_soft,\n",
    "               f\"{EXP_NAME_ENS}_SOFT — ROC (val)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_soft,\n",
    "               f\"{EXP_NAME_ENS}_SOFT — ROC (test)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_roc_test.png\")\n",
    "\n",
    "plot_roc_curve(y_val,  proba_val_stack,\n",
    "               f\"{EXP_NAME_ENS}_STACK — ROC (val)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_stack,\n",
    "               f\"{EXP_NAME_ENS}_STACK — ROC (test)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_roc_test.png\")\n",
    "\n",
    "plot_confusion(y_test, y_pred_test_soft,\n",
    "               f\"{EXP_NAME_ENS}_SOFT — Confusion (test @thr={thr_val_soft:.3f})\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_cm_test.png\")\n",
    "\n",
    "plot_confusion(y_test, y_pred_test_stack,\n",
    "               f\"{EXP_NAME_ENS}_STACK — Confusion (test @thr={thr_val_stack:.3f})\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_cm_test.png\")\n",
    "\n",
    "ens_summary = {\n",
    "    \"model\": EXP_NAME_ENS,\n",
    "    \"stack\": {\n",
    "        \"thr_val\": float(thr_val_stack),\n",
    "        \"metrics_val\":  _floatify_metrics(metrics_val_stack),\n",
    "        \"metrics_test\": _floatify_metrics(metrics_test_stack),\n",
    "    },\n",
    "    \"soft_voting\": {\n",
    "        \"thr_val\": float(thr_val_soft),\n",
    "        \"metrics_val\":  _floatify_metrics(metrics_val_soft),\n",
    "        \"metrics_test\": _floatify_metrics(metrics_test_soft),\n",
    "    }\n",
    "}\n",
    "_json_dump(OUT_RESULTS_E / f\"{EXP_NAME_ENS}_metrics.json\", ens_summary)\n",
    "\n",
    "row_stack = {\n",
    "    \"model\": f\"{EXP_NAME_ENS}_STACK\",\n",
    "    \"thr_val\":        thr_val_stack,\n",
    "    \"val_pr_auc\":     metrics_val_stack[\"pr_auc\"],\n",
    "    \"val_roc_auc\":    metrics_val_stack[\"roc_auc\"],\n",
    "    \"val_precision\":  metrics_val_stack[\"precision\"],\n",
    "    \"val_f1\":         metrics_val_stack[\"f1\"],\n",
    "    \"val_recall\":     metrics_val_stack[\"recall\"],\n",
    "    \"val_bal_acc\":    metrics_val_stack[\"bal_acc\"],\n",
    "    \"test_pr_auc\":    metrics_test_stack[\"pr_auc\"],\n",
    "    \"test_roc_auc\":   metrics_test_stack[\"roc_auc\"],\n",
    "    \"test_precision\": metrics_test_stack[\"precision\"],\n",
    "    \"test_f1\":        metrics_test_stack[\"f1\"],\n",
    "    \"test_recall\":    metrics_test_stack[\"recall\"],\n",
    "    \"test_bal_acc\":   metrics_test_stack[\"bal_acc\"],\n",
    "}\n",
    "\n",
    "row_soft = {\n",
    "    \"model\": f\"{EXP_NAME_ENS}_SOFT\",\n",
    "    \"thr_val\":        thr_val_soft,\n",
    "    \"val_pr_auc\":     metrics_val_soft[\"pr_auc\"],\n",
    "    \"val_roc_auc\":    metrics_val_soft[\"roc_auc\"],\n",
    "    \"val_precision\":  metrics_val_soft[\"precision\"],\n",
    "    \"val_f1\":         metrics_val_soft[\"f1\"],\n",
    "    \"val_recall\":     metrics_val_soft[\"recall\"],\n",
    "    \"val_bal_acc\":    metrics_val_soft[\"bal_acc\"],\n",
    "    \"test_pr_auc\":    metrics_test_soft[\"pr_auc\"],\n",
    "    \"test_roc_auc\":   metrics_test_soft[\"roc_auc\"],\n",
    "    \"test_precision\": metrics_test_soft[\"precision\"],\n",
    "    \"test_f1\":        metrics_test_soft[\"f1\"],\n",
    "    \"test_recall\":    metrics_test_soft[\"recall\"],\n",
    "    \"test_bal_acc\":   metrics_test_soft[\"bal_acc\"],\n",
    "}\n",
    "\n",
    "csv_e = OUT_RESULTS_E / \"ensembles.csv\"\n",
    "pd.DataFrame([row_stack, row_soft]).to_csv(\n",
    "    csv_e,\n",
    "    mode=(\"a\" if csv_e.exists() else \"w\"),\n",
    "    index=False,\n",
    "    header=not csv_e.exists()\n",
    ")\n",
    "\n",
    "stack_params = {\n",
    "    \"type\": \"LogisticRegression\",\n",
    "    \"penalty\": stack_clf.penalty,\n",
    "    \"C\": float(stack_clf.C),\n",
    "    \"solver\": stack_clf.solver,\n",
    "    \"feature_names_level1\": [\"xgb_tuned_proba\", \"lgbm_tuned_proba\", \"dl_proba\"],\n",
    "    \"coef_\": stack_clf.coef_[0].tolist(),\n",
    "    \"intercept_\": float(stack_clf.intercept_[0]),\n",
    "}\n",
    "_json_dump(OUT_PARAMS_E / f\"BEST_{EXP_NAME_ENS}.json\", stack_params)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"p_xgb\":   proba_val_xgb_tuned,\n",
    "    \"p_lgbm\":  proba_val_lgb_tuned,\n",
    "    \"p_dl\":    proba_val_dl,\n",
    "    \"p_soft\":  proba_val_soft,\n",
    "    \"p_stack\": proba_val_stack,\n",
    "    \"y_true\":  y_val,\n",
    "}).to_parquet(OUT_PREDS_E / f\"preds_val_{EXP_NAME_ENS}.parquet\", index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"p_xgb\":   proba_test_xgb_tuned,\n",
    "    \"p_lgbm\":  proba_test_lgb_tuned,\n",
    "    \"p_dl\":    proba_test_dl,\n",
    "    \"p_soft\":  proba_test_soft,\n",
    "    \"p_stack\": proba_test_stack,\n",
    "    \"y_true\":  y_test,\n",
    "}).to_parquet(OUT_PREDS_E / f\"preds_test_{EXP_NAME_ENS}.parquet\", index=False)\n",
    "\n",
    "print(\"\\n=== SOFT VOTING (val) ===\")\n",
    "print(metrics_val_soft)\n",
    "print(\"\\n=== SOFT VOTING (test) ===\")\n",
    "print(metrics_test_soft)\n",
    "\n",
    "print(\"\\n=== STACKING (val) ===\")\n",
    "print(metrics_val_stack)\n",
    "print(\"\\n=== STACKING (test) ===\")\n",
    "print(metrics_test_stack)\n",
    "\n",
    "print(\"\\n[OK][ENSEMBLE] Métricas, preds y pesos del modelo híbrido guardados en:\")\n",
    "print(\"  RESULTS:\", OUT_RESULTS_E)\n",
    "print(\"  PREDS:  \", OUT_PREDS_E)\n",
    "print(\"  PARAMS: \", OUT_PARAMS_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f7b15",
   "metadata": {},
   "source": [
    "10 - Stacking con OOF (CV=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4ead0c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[DL OOF] Columnas encontradas en el parquet: Index(['oof_proba', 'y_true'], dtype='object')\n",
      "[DL OOF] Usando columna 'oof_proba' como probabilidad.\n",
      "[STACK OOF] Usando SOLO TRAIN para meta-learner (len = 6000 ).\n",
      "[STACK OOF] Mejor C para meta-learner: 0.1000 | AP OOF: 0.70209\n",
      "[STACK OOF] OK — resultados de VAL y TEST guardados.\n"
     ]
    }
   ],
   "source": [
    "DO_FULL_STACKING_OOF = True \n",
    "\n",
    "if DO_FULL_STACKING_OOF:\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    def oof_preds_tree(model_builder, X, y, name):\n",
    "        oof = np.zeros(len(y), dtype=float)\n",
    "        models = []\n",
    "        for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "            X_tr, y_tr = X[tr], y[tr]\n",
    "            X_va, y_va = X[va], y[va]\n",
    "            if USE_BALANCED_TRAIN:\n",
    "                X_tr, y_tr = maybe_smote(X_tr, y_tr, keep_idx=keep_idx_global)\n",
    "            mdl = model_builder()\n",
    "            if name == \"xgb\":\n",
    "                mdl = xgb_fit_with_es(\n",
    "                    mdl,\n",
    "                    X_tr, y_tr,\n",
    "                    X_va, y_va,\n",
    "                    feature_names=feature_names_used,\n",
    "                    rounds=200,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                mdl = lgbm_fit_with_es(\n",
    "                    mdl,\n",
    "                    X_tr, y_tr,\n",
    "                    X_va, y_va,\n",
    "                    feature_names=feature_names_used,\n",
    "                    rounds=200,\n",
    "                    verbose=False\n",
    "                )\n",
    "            oof[va] = mdl.predict_proba(X_va)[:, 1]\n",
    "            models.append(mdl)\n",
    "        return oof, models\n",
    "\n",
    "    # --- Builders de los modelos base ---\n",
    "    xgb_hp = xgb_best_params if ('xgb_best_params' in locals() and xgb_best_params is not None) else xgb_params_seed\n",
    "    def build_xgb():\n",
    "        return XGBClassifier(\n",
    "            **{\n",
    "                **xgb_hp,\n",
    "                \"n_jobs\": -1,\n",
    "                \"eval_metric\": \"aucpr\",\n",
    "                \"tree_method\": \"hist\",\n",
    "                \"verbosity\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    lgbm_hp = lgbm_best_params if ('lgbm_best_params' in locals() and lgbm_best_params is not None) else lgbm_params_seed\n",
    "    def build_lgb():\n",
    "        return LGBMClassifier(\n",
    "            **{\n",
    "                **lgbm_hp,\n",
    "                \"metric\": \"average_precision\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # --- OOF XGB / LGBM sobre TRAIN+VAL ---\n",
    "    X_all = np.vstack([X_train, X_val])\n",
    "    y_all = np.concatenate([y_train, y_val])\n",
    "\n",
    "    oof_xgb, xgb_models = oof_preds_tree(build_xgb, X_all, y_all, \"xgb\")\n",
    "    oof_lgb, lgb_models = oof_preds_tree(build_lgb, X_all, y_all, \"lgbm\")\n",
    "\n",
    "    # --- OOF DL ---\n",
    "    dl_oof_fp = _find_one(\"preds/oof_DL_*_CV5.parquet\") or _find_one(\"oof_DL_*_CV5.parquet\")\n",
    "    if dl_oof_fp is None:\n",
    "        raise FileNotFoundError(\"[DL] No se encontró oof del DL (oof_DL_*_CV5.parquet).\")\n",
    "\n",
    "    df_oof_dl = pd.read_parquet(dl_oof_fp)\n",
    "    print(\"[DL OOF] Columnas encontradas en el parquet:\", df_oof_dl.columns)\n",
    "\n",
    "    # Elegimos la columna de probas ignorando columnas típicas de label\n",
    "    candidate_cols = [c for c in df_oof_dl.columns if c.lower() not in (\"y_true\", \"y\", \"target\", \"label\")]\n",
    "    if len(candidate_cols) != 1:\n",
    "        raise ValueError(f\"[DL] No puedo identificar de forma única la columna de probas. Columnas: {df_oof_dl.columns}\")\n",
    "    proba_col_dl = candidate_cols[0]\n",
    "    print(f\"[DL OOF] Usando columna '{proba_col_dl}' como probabilidad.\")\n",
    "\n",
    "    oof_dl = df_oof_dl[proba_col_dl].to_numpy()\n",
    "\n",
    "    # --- Alinear longitudes ---\n",
    "    n_dl = len(oof_dl)\n",
    "    n_all = len(y_all)\n",
    "    n_train = len(y_train)\n",
    "\n",
    "    if n_dl == n_all:\n",
    "        # Caso ideal: el DL tiene OOF sobre TRAIN+VAL\n",
    "        oof_xgb_meta = oof_xgb\n",
    "        oof_lgb_meta = oof_lgb\n",
    "        y_meta = y_all\n",
    "        print(\"[STACK OOF] Usando TRAIN+VAL para meta-learner (len =\", n_dl, \").\")\n",
    "    elif n_dl == n_train:\n",
    "        # Caso actual: el DL solo tiene OOF para TRAIN\n",
    "        oof_xgb_meta = oof_xgb[:n_dl]\n",
    "        oof_lgb_meta = oof_lgb[:n_dl]\n",
    "        y_meta = y_train\n",
    "        print(\"[STACK OOF] Usando SOLO TRAIN para meta-learner (len =\", n_dl, \").\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"[DL] Longitud OOF DL {n_dl} no coincide ni con TRAIN ({n_train}) ni con TRAIN+VAL ({n_all}).\"\n",
    "        )\n",
    "\n",
    "    # --- Meta-learner con OOF alineado ---\n",
    "    X_meta_oof = np.column_stack([oof_xgb_meta, oof_lgb_meta, oof_dl])\n",
    "    meta_oof = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        C=1.0,\n",
    "        solver=\"liblinear\"\n",
    "    )\n",
    "\n",
    "    # Pequeño search en C por AP OOF\n",
    "    best_c, best_ap = None, -1.0\n",
    "    for c in [0.01, 0.1, 1.0, 3.0, 10.0]:\n",
    "        tmp = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight=\"balanced\",\n",
    "            C=c,\n",
    "            solver=\"liblinear\"\n",
    "        )\n",
    "        tmp.fit(X_meta_oof, y_meta)\n",
    "        ap = average_precision_score(y_meta, tmp.predict_proba(X_meta_oof)[:, 1])\n",
    "        if ap > best_ap:\n",
    "            best_ap, best_c = ap, c\n",
    "\n",
    "    meta_oof.set_params(C=best_c)\n",
    "    meta_oof.fit(X_meta_oof, y_meta)\n",
    "    print(f\"[STACK OOF] Mejor C para meta-learner: {best_c:.4f} | AP OOF: {best_ap:.5f}\")\n",
    "\n",
    "    # --- Predicciones TEST con modelos reentrenados ---\n",
    "    def fit_full_and_pred(models_builder, name):\n",
    "        mdl = models_builder()\n",
    "        if name == \"xgb\":\n",
    "            mdl = xgb_fit_with_es(\n",
    "                mdl,\n",
    "                X_all, y_all,\n",
    "                X_val_fit, y_val,\n",
    "                feature_names=feature_names_used,\n",
    "                rounds=200,\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            mdl = lgbm_fit_with_es(\n",
    "                mdl,\n",
    "                X_all, y_all,\n",
    "                X_val_fit, y_val,\n",
    "                feature_names=feature_names_used,\n",
    "                rounds=200,\n",
    "                verbose=False\n",
    "            )\n",
    "        return mdl.predict_proba(X_test_fit)[:, 1]\n",
    "\n",
    "    ptest_xgb_full = fit_full_and_pred(build_xgb, \"xgb\")\n",
    "    ptest_lgb_full = fit_full_and_pred(build_lgb, \"lgbm\")\n",
    "\n",
    "    # --- DL TEST ---\n",
    "    if proba_test_dl is None:\n",
    "        raise FileNotFoundError(\"[DL] Falta preds TEST del DL para completar stacking canónico.\")\n",
    "\n",
    "    X_meta_test = np.column_stack([ptest_xgb_full, ptest_lgb_full, proba_test_dl])\n",
    "\n",
    "    # --- PREDICCIONES STACK EN VALIDACIÓN ---\n",
    "    X_meta_val = np.column_stack([proba_val_xgb_best, proba_val_lgb_best, proba_val_dl])\n",
    "    p_val_stack_oof = meta_oof.predict_proba(X_meta_val)[:, 1]\n",
    "\n",
    "    thr_stack_oof, _ = find_best_threshold(y_val, p_val_stack_oof, metric=\"f1\")\n",
    "    metrics_val_stack_oof = compute_all_metrics(y_val, p_val_stack_oof, thr_stack_oof)\n",
    "\n",
    "    # --- PREDICCIONES STACK EN TEST ---\n",
    "    p_test_stack_oof = meta_oof.predict_proba(X_meta_test)[:, 1]\n",
    "    metrics_test_stack_oof = compute_all_metrics(y_test, p_test_stack_oof, thr_stack_oof)\n",
    "\n",
    "    # --- GUARDAR PREDS VAL Y TEST ---\n",
    "    pd.DataFrame({\"proba\": p_val_stack_oof, \"y_true\": y_val}).to_parquet(\n",
    "        OUT_PREDS_E / \"preds_val_ENS_STACK_OOF.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    pd.DataFrame({\"proba\": p_test_stack_oof, \"y_true\": y_test}).to_parquet(\n",
    "        OUT_PREDS_E / \"preds_test_ENS_STACK_OOF.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    with open(OUT_RESULTS_E / \"ensemble_stack_oof_summary.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"thr\": float(thr_stack_oof),\n",
    "                \"val\": {k: float(v) for k, v in metrics_val_stack_oof.items()},\n",
    "                \"test\": {k: float(v) for k, v in metrics_test_stack_oof.items()},\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"[STACK OOF] OK — resultados de VAL y TEST guardados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
