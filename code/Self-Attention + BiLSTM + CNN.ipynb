{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "sec1",
      "metadata": {},
      "source": [
        "1 — Imports, configuración y rutas (DL híbrido sin reducción)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "sec1code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exp: DL_FULL_SMOTENC\n",
            "DATA_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/preproc_datasets/full\n",
            "ARTIF_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/DL_FULL_SMOTENC\n",
            "DEVICE: cpu\n"
          ]
        }
      ],
      "source": [
        "import json, os, warnings, time, re, glob, math, random\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score, precision_recall_curve, roc_auc_score, roc_curve,\n",
        "    f1_score, recall_score, balanced_accuracy_score, confusion_matrix, precision_score\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Balanceo\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTENC\n",
        "    _HAS_IMBLEARN = True\n",
        "except Exception:\n",
        "    _HAS_IMBLEARN = False\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# === Toggles de experimento ===\n",
        "USE_REDUCED = False               \n",
        "USE_BALANCED_TRAIN = True       \n",
        "BALANCE_IN_CV = True              \n",
        "RANDOM_STATE = 42\n",
        "DO_TUNE = True\n",
        "DO_CV_BASELINE = True\n",
        "DO_CV_TUNED = True\n",
        "CV_FOLDS = 5\n",
        "\n",
        "# === Nombres y rutas ===\n",
        "ROOT = Path.cwd().parent\n",
        "EXP_NAME = f\"DL_{'REDUCED' if USE_REDUCED else 'FULL'}_{'SMOTENC' if USE_BALANCED_TRAIN else 'IMB'}\"\n",
        "ARTIF_DIR = ROOT / \"artifacts\" / EXP_NAME\n",
        "OUT_RESULTS = ARTIF_DIR / \"results\"\n",
        "OUT_FIGS    = ARTIF_DIR / \"figs\"\n",
        "OUT_PREDS   = ARTIF_DIR / \"preds\"\n",
        "OUT_PARAMS  = ARTIF_DIR / \"best_params\"\n",
        "for p in [OUT_RESULTS, OUT_FIGS, OUT_PREDS, OUT_PARAMS]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Dataset preprocesado\n",
        "DATA_DIR = ROOT / \"preproc_datasets\" / \"full\"\n",
        "\n",
        "print(\"Exp:\", EXP_NAME)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"ARTIF_DIR:\", ARTIF_DIR)\n",
        "\n",
        "# Seeds globales\n",
        "def set_seeds(seed=RANDOM_STATE):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seeds(RANDOM_STATE)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"DEVICE:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec2",
      "metadata": {},
      "source": [
        "2 — Carga de artefactos (X, y, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "sec2code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes: (6000, 15) (2000, 15) (2000, 15)\n",
            "y train/val/test: (6000,) (2000,) (2000,)\n",
            "n features: 15\n",
            "[SMOTENC] 5 grupos OHE; 10 dims categóricas; 5 numéricas\n"
          ]
        }
      ],
      "source": [
        "def load_xy_full(dir_full: Path):\n",
        "    X_train = np.load(dir_full / \"X_train_full.npy\")\n",
        "    X_val   = np.load(dir_full / \"X_val_full.npy\")\n",
        "    X_test  = np.load(dir_full / \"X_test_full.npy\")\n",
        "\n",
        "    y_train = pd.read_parquet(dir_full / \"y_train.parquet\")[\"Exited\"].to_numpy()\n",
        "    y_val   = pd.read_parquet(dir_full / \"y_val.parquet\")[\"Exited\"].to_numpy()\n",
        "    y_test  = pd.read_parquet(dir_full / \"y_test.parquet\")[\"Exited\"].to_numpy()\n",
        "\n",
        "    feat = pd.read_parquet(dir_full / \"feature_names_full.parquet\")[\"feature\"].tolist()\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, feat\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_xy_full(DATA_DIR)\n",
        "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "print(\"y train/val/test:\", y_train.shape, y_val.shape, y_test.shape)\n",
        "print(\"n features:\", len(feature_names))\n",
        "\n",
        "# Tipos consistentes\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_val   = X_val.astype(np.float32)\n",
        "X_test  = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_val   = y_val.astype(np.int64)\n",
        "y_test  = y_test.astype(np.int64)\n",
        "\n",
        "PREPROC_META = DATA_DIR / \"preprocessor_meta.json\"\n",
        "cat_cols_order = None\n",
        "if PREPROC_META.exists():\n",
        "    try:\n",
        "        with open(PREPROC_META, \"r\", encoding=\"utf-8\") as f:\n",
        "            _meta = json.load(f)\n",
        "        cat_cols_order = _meta.get(\"cat_cols\", None)\n",
        "    except Exception:\n",
        "        cat_cols_order = None\n",
        "\n",
        "# Índices numéricos (prefijo 'num__') y OHE (resto)\n",
        "num_idx = [i for i, n in enumerate(feature_names) if str(n).startswith(\"num__\")]\n",
        "ohe_idx = [i for i in range(len(feature_names)) if i not in num_idx]\n",
        "\n",
        "from collections import defaultdict\n",
        "base_to_idx = defaultdict(list)\n",
        "for i in ohe_idx:\n",
        "    base = str(feature_names[i]).split(\"_\", 1)[0]  # <col>_<cat> -> <col>\n",
        "    base_to_idx[base].append(i)\n",
        "\n",
        "# Orden estable\n",
        "for k in base_to_idx:\n",
        "    base_to_idx[k] = sorted(base_to_idx[k])\n",
        "\n",
        "if cat_cols_order:\n",
        "    ONEHOT_GROUPS = [base_to_idx[c] for c in cat_cols_order if c in base_to_idx]\n",
        "else:\n",
        "    ONEHOT_GROUPS = [base_to_idx[k] for k in sorted(base_to_idx.keys(), key=lambda k: min(base_to_idx[k]))]\n",
        "\n",
        "CAT_IDX = sorted([j for grp in ONEHOT_GROUPS for j in grp])\n",
        "\n",
        "print(f\"[SMOTENC] {len(ONEHOT_GROUPS)} grupos OHE; {len(CAT_IDX)} dims categóricas; {len(num_idx)} numéricas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec3",
      "metadata": {},
      "source": [
        "3 — Métricas, threshold y plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "sec3code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pr_auc(y_true, y_proba):\n",
        "    return float(average_precision_score(y_true, y_proba))\n",
        "\n",
        "def roc_auc(y_true, y_proba):\n",
        "    return float(roc_auc_score(y_true, y_proba))\n",
        "\n",
        "def find_best_threshold(y_true, y_proba, metric=\"f1\"):\n",
        "    thr_grid = np.linspace(0.0, 1.0, 1001)\n",
        "    best_thr, best_score = 0.5, -1.0\n",
        "    for thr in thr_grid:\n",
        "        y_pred = (y_proba >= thr).astype(int)\n",
        "        if metric == \"f1\":\n",
        "            score = f1_score(y_true, y_pred, zero_division=0)\n",
        "        elif metric == \"recall\":\n",
        "            score = recall_score(y_true, y_pred, zero_division=0)\n",
        "        else:\n",
        "            raise ValueError(\"metric no soportada\")\n",
        "        if score > best_score:\n",
        "            best_score, best_thr = score, thr\n",
        "    return float(best_thr), float(best_score)\n",
        "\n",
        "def compute_all_metrics(y_true, y_proba, thr):\n",
        "    y_pred = (y_proba >= thr).astype(int)\n",
        "    return {\n",
        "        \"pr_auc\": pr_auc(y_true, y_proba),\n",
        "        \"roc_auc\": roc_auc(y_true, y_proba),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"bal_acc\": balanced_accuracy_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "def plot_pr_curve(y_true, y_proba, title, out_path):\n",
        "    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n",
        "    ap = average_precision_score(y_true, y_proba)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.step(rec, prec, where='post')\n",
        "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
        "    plt.title(f'{title} (AP={ap:.4f})')\n",
        "    plt.grid(True, linestyle='--', alpha=.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150); plt.close()\n",
        "\n",
        "def plot_roc_curve(y_true, y_proba, title, out_path):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    auc = roc_auc_score(y_true, y_proba)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(fpr, tpr, lw=2)\n",
        "    plt.plot([0,1],[0,1], 'k--', lw=1)\n",
        "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{title} (AUC={auc:.4f})')\n",
        "    plt.grid(True, linestyle='--', alpha=.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150); plt.close()\n",
        "\n",
        "def plot_confusion(y_true, y_pred, title, out_path, normalize=False):\n",
        "    norm = 'true' if normalize else None\n",
        "    cm = confusion_matrix(y_true, y_pred, normalize=norm)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    im = plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    ticks = np.arange(2)\n",
        "    plt.xticks(ticks, ['0','1']); plt.yticks(ticks, ['0','1'])\n",
        "    thresh = cm.max()/2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            txt = f'{cm[i,j]:.2f}' if normalize else str(cm[i,j])\n",
        "            plt.text(j, i, txt, ha='center', va='center',\n",
        "                     color='white' if cm[i,j] > thresh else 'black')\n",
        "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=150); plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec4",
      "metadata": {},
      "source": [
        "4 — Helpers: SMOTE, Dataset y utilidades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "sec4code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _repair_onehot_blocks(X, groups):\n",
        "    X = X.copy()\n",
        "    if not groups:\n",
        "        return X.astype(np.float32)\n",
        "    rows = np.arange(X.shape[0])\n",
        "    for grp in groups:\n",
        "        if len(grp) == 1:\n",
        "            c = grp[0]\n",
        "            X[:, c] = (X[:, c] >= 0.5).astype(np.float32)\n",
        "        else:\n",
        "            block = X[:, grp]\n",
        "            winners = np.argmax(block, axis=1)\n",
        "            X[:, grp] = 0.0\n",
        "            X[rows, np.array(grp)[winners]] = 1.0\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "def maybe_resample(X, y, seed=RANDOM_STATE):\n",
        "\n",
        "    if not _HAS_IMBLEARN or X.shape[0] != y.shape[0] or len(CAT_IDX) == 0:\n",
        "        return X, y\n",
        "    try:\n",
        "        sm = SMOTENC(categorical_features=CAT_IDX, random_state=seed)\n",
        "        Xb, yb = sm.fit_resample(X, y)\n",
        "        Xb = _repair_onehot_blocks(Xb, ONEHOT_GROUPS)\n",
        "        return Xb.astype(np.float32), yb.astype(np.int64)\n",
        "    except Exception as e:\n",
        "        print(\"[SMOTENC] Aviso: se usará dataset original por error:\", e)\n",
        "        return X, y\n",
        "\n",
        "class TabDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.X[idx]\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def class_pos_weight(y):\n",
        "    # pos_weight = N_neg / N_pos\n",
        "    y = np.asarray(y)\n",
        "    n_pos = (y == 1).sum()\n",
        "    n_neg = (y == 0).sum()\n",
        "    if n_pos == 0:\n",
        "        return 1.0\n",
        "    return float(n_neg / max(1, n_pos))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec5",
      "metadata": {},
      "source": [
        "5 — Modelo DL híbrido (Self-Attention + BiLSTM + CNN) y entrenamiento con early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "sec5code",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScalarFeatureTokenizer(nn.Module):\n",
        "    \"\"\"\n",
        "    Proyecta cada feature escalar a un token de dimensión d_model:\n",
        "    token_i = x_i * W_i + b_i  (W_i y b_i aprendibles por feature)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, d_model):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(n_features, d_model) * 0.02)\n",
        "        self.bias   = nn.Parameter(torch.zeros(n_features, d_model))\n",
        "    def forward(self, x):  # x: (B, F)\n",
        "        return x.unsqueeze(-1) * self.weight + self.bias  # (B, F, d_model)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout, batch_first=True)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ff  = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):  # (B, F, d)\n",
        "        attn_out, _ = self.mha(x, x, x, need_weights=False)\n",
        "        x = self.ln1(x + self.dropout(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "class SELayer1D(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation sobre canales (dim de canales tras Conv1d).\"\"\"\n",
        "    def __init__(self, channels, reduction=8):\n",
        "        super().__init__()\n",
        "        hidden = max(1, channels // reduction)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, hidden), nn.ReLU(inplace=True), nn.Linear(hidden, channels), nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):  \n",
        "        s = x.mean(dim=-1)  \n",
        "        w = self.fc(s)      \n",
        "        return x * w.unsqueeze(-1)\n",
        "\n",
        "class CCPNetLite(nn.Module):\n",
        "    \"\"\"\n",
        "    Híbrido simple: Tokenizer -> N bloques Transformer -> BiLSTM -> Conv1d + SE -> Pool -> Head\n",
        "    Pensado para tabular (tratando cada feature como token).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, d_model=48, n_heads=4, n_layers=2, lstm_hidden=64,\n",
        "                 cnn_channels=64, kernel_size=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.n_features = n_features\n",
        "        self.tokenizer = ScalarFeatureTokenizer(n_features, d_model)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model=d_model, n_heads=n_heads, d_ff=4*d_model, dropout=dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.bi_lstm = nn.LSTM(input_size=d_model, hidden_size=lstm_hidden, batch_first=True, bidirectional=True)\n",
        "        conv_in = lstm_hidden * 2\n",
        "        pad = kernel_size // 2\n",
        "        self.conv = nn.Conv1d(conv_in, cnn_channels, kernel_size=kernel_size, padding=pad)\n",
        "        self.se   = SELayer1D(cnn_channels)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(cnn_channels*2, 128), nn.ReLU(inplace=True), nn.Dropout(dropout), nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x): \n",
        "        t = self.tokenizer(x)                \n",
        "        for blk in self.blocks:\n",
        "            t = blk(t)                        \n",
        "        lstm_out, _ = self.bi_lstm(t)         \n",
        "        z = lstm_out.transpose(1, 2)          \n",
        "        z = self.conv(z)                      \n",
        "        z = F.gelu(z)\n",
        "        z = self.se(z)                     \n",
        "        # Global avg + max pooling\n",
        "        gap = z.mean(dim=-1)\n",
        "        gmp, _ = z.max(dim=-1)\n",
        "        g = torch.cat([gap, gmp], dim=1)     \n",
        "        logit = self.head(g).squeeze(1)      \n",
        "        return logit\n",
        "    \n",
        "    def feature_importance(self, feature_names):\n",
        "        with torch.no_grad():\n",
        "            w = self.tokenizer.weight.detach().cpu().numpy() \n",
        "            imp = np.linalg.norm(w, axis=1)\n",
        "        return pd.DataFrame({\"feature\": feature_names, \"importance_proxy\": imp}).sort_values(\"importance_proxy\", ascending=False)\n",
        "\n",
        "def get_dl_defaults(seed=RANDOM_STATE):\n",
        "    return {\n",
        "        \"d_model\": 48,\n",
        "        \"n_heads\": 4,\n",
        "        \"n_layers\": 2,\n",
        "        \"lstm_hidden\": 64,\n",
        "        \"cnn_channels\": 64,\n",
        "        \"kernel_size\": 3,\n",
        "        \"dropout\": 0.2,\n",
        "        \"lr\": 1e-3,\n",
        "        \"weight_decay\": 1e-4,\n",
        "        \"batch_size\": 256,\n",
        "        \"epochs\": 100,\n",
        "        \"patience\": 12,\n",
        "        \"random_state\": seed\n",
        "    }\n",
        "\n",
        "def make_model(n_features, hp):\n",
        "    mdl = CCPNetLite(\n",
        "        n_features=n_features,\n",
        "        d_model=int(hp[\"d_model\"]),\n",
        "        n_heads=int(hp[\"n_heads\"]),\n",
        "        n_layers=int(hp[\"n_layers\"]),\n",
        "        lstm_hidden=int(hp[\"lstm_hidden\"]),\n",
        "        cnn_channels=int(hp[\"cnn_channels\"]),\n",
        "        kernel_size=int(hp[\"kernel_size\"]),\n",
        "        dropout=float(hp[\"dropout\"]) \n",
        "    ).to(DEVICE)\n",
        "    return mdl\n",
        "\n",
        "def train_one(model, X_tr, y_tr, X_va, y_va, hp, verbose=False):\n",
        "    set_seeds(RANDOM_STATE)\n",
        "    bs = int(hp[\"batch_size\"]) if \"batch_size\" in hp else 256\n",
        "    epochs = int(hp.get(\"epochs\", 100))\n",
        "    patience = int(hp.get(\"patience\", 12))\n",
        "    lr = float(hp.get(\"lr\", 1e-3))\n",
        "    wd = float(hp.get(\"weight_decay\", 1e-4))\n",
        "\n",
        "    did_smote = hp.get(\"_did_smote\", False)\n",
        "    pw = 1.0 if did_smote else class_pos_weight(y_tr)\n",
        "    pos_w = torch.tensor([pw], dtype=torch.float32, device=DEVICE)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_w)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    dl_tr = DataLoader(TabDataset(X_tr, y_tr), batch_size=bs, shuffle=True, num_workers=0, pin_memory=False)\n",
        "    dl_va = DataLoader(TabDataset(X_va, y_va), batch_size=bs, shuffle=False, num_workers=0, pin_memory=False)\n",
        "\n",
        "    best_ap = -1.0\n",
        "    best_epoch = -1\n",
        "    best_state = None\n",
        "    wait = 0\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running = 0.0\n",
        "        for xb, yb in dl_tr:\n",
        "            xb = xb.to(DEVICE)\n",
        "            yb = yb.to(DEVICE)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            logit = model(xb)\n",
        "            loss = criterion(logit, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running += float(loss.item())\n",
        "\n",
        "        # Validación\n",
        "        model.eval()\n",
        "        all_probs = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dl_va:\n",
        "                xb = xb.to(DEVICE)\n",
        "                logit = model(xb)\n",
        "                prob = torch.sigmoid(logit).detach().cpu().numpy()\n",
        "                all_probs.append(prob)\n",
        "        va_proba = np.concatenate(all_probs, axis=0)\n",
        "        ap = average_precision_score(y_va, va_proba)\n",
        "\n",
        "        if verbose and ep % 10 == 0:\n",
        "            print(f\"[EP {ep:03d}] loss={running/len(dl_tr):.4f} | AP(val)={ap:.4f}\")\n",
        "\n",
        "        # Early stopping por AP\n",
        "        if ap > best_ap + 1e-6:\n",
        "            best_ap = ap\n",
        "            best_epoch = ep\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                break\n",
        "\n",
        "    # Restaurar mejor estado\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, best_epoch, best_ap\n",
        "\n",
        "def predict_proba(model, X, batch_size=512):\n",
        "    model.eval()\n",
        "    dl = DataLoader(TabDataset(X, None), batch_size=batch_size, shuffle=False)\n",
        "    probs = []\n",
        "    with torch.no_grad():\n",
        "        for xb in dl:\n",
        "            xb = xb.to(DEVICE)\n",
        "            logit = model(xb)\n",
        "            prob = torch.sigmoid(logit).detach().cpu().numpy()\n",
        "            probs.append(prob)\n",
        "    return np.concatenate(probs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec6",
      "metadata": {},
      "source": [
        "6 — Hiperparámetros persistentes (carga/guardado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "sec6code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HP] Cargando mejores hiperparámetros previos: BEST_DL_FULL_SMOTENC.json\n"
          ]
        }
      ],
      "source": [
        "VIEW_TAG = \"REDUCED\" if USE_REDUCED else \"FULL\"\n",
        "BAL_TAG  = \"SMOTENC\" if USE_BALANCED_TRAIN else \"IMB\"\n",
        "BEST_HP_FILE = OUT_PARAMS / f\"BEST_DL_{VIEW_TAG}_{BAL_TAG}.json\"\n",
        "\n",
        "def load_best_or_default():\n",
        "    base = get_dl_defaults()\n",
        "    if BEST_HP_FILE.exists():\n",
        "        try:\n",
        "            best = json.loads(BEST_HP_FILE.read_text())\n",
        "            print(\"[HP] Cargando mejores hiperparámetros previos:\", BEST_HP_FILE.name)\n",
        "            base.update(best)\n",
        "            return base, True\n",
        "        except Exception as e:\n",
        "            print(\"[HP] Aviso: no se pudo leer BEST (uso defaults).\", e)\n",
        "    print(\"[HP] Usando hiperparámetros DEFAULT de DL.\")\n",
        "    return base, False\n",
        "\n",
        "seed_params, loaded_best_flag = load_best_or_default()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec7",
      "metadata": {},
      "source": [
        "7 — Entrenamiento BASELINE + umbral (DL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "sec7code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BASELINE] best_epoch: 21\n",
            "[BASELINE] Mejor umbral (val) por F1: 0.814 | F1(val)=0.6270\n",
            "[BASELINE] Métricas val: {'pr_auc': 0.6795, 'roc_auc': 0.8473, 'precision': 0.6967, 'f1': 0.627, 'recall': 0.57, 'bal_acc': 0.7533}\n"
          ]
        }
      ],
      "source": [
        "set_seeds(RANDOM_STATE)\n",
        "\n",
        "feature_names_used = feature_names\n",
        "X_train_fit, X_val_fit, X_test_fit = X_train, X_val, X_test\n",
        "\n",
        "X_train_final, y_train_final = X_train_fit, y_train\n",
        "did_smote_flag = False\n",
        "if USE_BALANCED_TRAIN:\n",
        "    X_train_final, y_train_final = maybe_resample(X_train_fit, y_train)\n",
        "    did_smote_flag = True\n",
        "\n",
        "base_hp = dict(seed_params)\n",
        "base_hp[\"_did_smote\"] = did_smote_flag\n",
        "model = make_model(n_features=X_train_final.shape[1], hp=base_hp)\n",
        "model, best_epoch = train_one(model, X_train_final, y_train_final, X_val_fit, y_val, base_hp, verbose=False)[:2]\n",
        "print(f\"[BASELINE] best_epoch: {best_epoch}\")\n",
        "\n",
        "proba_val = predict_proba(model, X_val_fit)\n",
        "thr_val, best_f1_val = find_best_threshold(y_val, proba_val, metric=\"f1\")\n",
        "# Guardar predicciones de validación (baseline)\n",
        "val_preds_path = OUT_PREDS / f\"preds_val_{EXP_NAME}.parquet\"\n",
        "pd.DataFrame({\n",
        "    \"proba\": proba_val,\n",
        "    \"y_true\": y_val,\n",
        "    \"y_pred\": (proba_val >= thr_val).astype(int)\n",
        "}).to_parquet(val_preds_path, index=False)\n",
        "print(f\"[BASELINE] Mejor umbral (val) por F1: {thr_val:.3f} | F1(val)={best_f1_val:.4f}\")\n",
        "\n",
        "val_metrics = compute_all_metrics(y_val, proba_val, thr_val)\n",
        "print(\"[BASELINE] Métricas val:\", {k: (round(v,4) if isinstance(v,float) else v) for k,v in val_metrics.items()})\n",
        "\n",
        "baseline = model\n",
        "base_best_it = best_epoch\n",
        "tuned_model = None\n",
        "best_params = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec8",
      "metadata": {},
      "source": [
        "8 — Optimización incremental (Optuna) sobre AP(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "sec8code",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-10 22:54:23,769] A new study created in memory with name: DL_FULL_SMOTENC_AP\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OPTUNA] Enqueuing previous BEST as a trial seed.\n",
            "[OPTUNA] Iniciando estudio 'DL_FULL_SMOTENC_AP' con 40 pruebas...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-10 22:55:49,687] Trial 0 finished with value: 0.6757454775056062 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.19430284891752264, 'lr': 0.000810001430602943, 'weight_decay': 1.0315635803757857e-05, 'batch_size': 128}. Best is trial 0 with value: 0.6757454775056062.\n",
            "[I 2025-12-10 22:56:53,611] Trial 1 finished with value: 0.6758043511711931 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.2623782158161189, 'lr': 0.0008110848199986004, 'weight_decay': 1.461896279370496e-05, 'batch_size': 128}. Best is trial 1 with value: 0.6758043511711931.\n",
            "[I 2025-12-10 22:58:33,050] Trial 2 finished with value: 0.6622112017235411 and parameters: {'d_model': 64, 'n_heads': 8, 'n_layers': 1, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.06101911742238941, 'lr': 0.0009382059110341113, 'weight_decay': 1.3726318898045876e-06, 'batch_size': 128}. Best is trial 1 with value: 0.6758043511711931.\n",
            "[I 2025-12-10 23:03:10,471] Trial 3 finished with value: 0.672476601401931 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 128, 'kernel_size': 5, 'dropout': 0.17837666334679464, 'lr': 0.0005728695840313613, 'weight_decay': 0.00014817820606039095, 'batch_size': 256}. Best is trial 1 with value: 0.6758043511711931.\n",
            "[I 2025-12-10 23:05:39,610] Trial 4 finished with value: 0.6700230890159271 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.36480308916903204, 'lr': 0.0013022031035668192, 'weight_decay': 0.0035387588647792408, 'batch_size': 512}. Best is trial 1 with value: 0.6758043511711931.\n",
            "[I 2025-12-10 23:07:37,586] Trial 5 finished with value: 0.669011425577782 and parameters: {'d_model': 64, 'n_heads': 4, 'n_layers': 1, 'lstm_hidden': 96, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.038489954914396496, 'lr': 0.0005846187064070543, 'weight_decay': 4.414536876494481e-06, 'batch_size': 128}. Best is trial 1 with value: 0.6758043511711931.\n",
            "[I 2025-12-10 23:11:20,492] Trial 6 finished with value: 0.6737795183486313 and parameters: {'d_model': 32, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 128, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.0599326836668414, 'lr': 0.0006527342871386315, 'weight_decay': 0.005910698619088547, 'batch_size': 512}. Best is trial 1 with value: 0.6758043511711931.\n",
            "[I 2025-12-10 23:12:36,321] Trial 7 finished with value: 0.6664894255851543 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 1, 'lstm_hidden': 64, 'cnn_channels': 64, 'kernel_size': 5, 'dropout': 0.1210276357557502, 'lr': 0.0014101223696388783, 'weight_decay': 0.0011129571947046015, 'batch_size': 256}. Best is trial 1 with value: 0.6758043511711931.\n",
            "[I 2025-12-10 23:15:10,839] Trial 8 finished with value: 0.6677736800381915 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 1, 'lstm_hidden': 96, 'cnn_channels': 96, 'kernel_size': 3, 'dropout': 0.46836499436836726, 'lr': 0.0004117581451393256, 'weight_decay': 2.31347816122256e-05, 'batch_size': 256}. Best is trial 1 with value: 0.6758043511711931.\n",
            "[I 2025-12-10 23:16:33,242] Trial 9 finished with value: 0.6646676977248959 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 64, 'cnn_channels': 96, 'kernel_size': 3, 'dropout': 0.042069982497524416, 'lr': 0.00043526121378702535, 'weight_decay': 0.00392840951347923, 'batch_size': 128}. Best is trial 1 with value: 0.6758043511711931.\n",
            "[I 2025-12-10 23:17:57,187] Trial 10 finished with value: 0.6676587142435256 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.4241181385706499, 'lr': 0.0005437093597192922, 'weight_decay': 3.225961288100114e-05, 'batch_size': 128}. Best is trial 1 with value: 0.6758043511711931.\n",
            "[I 2025-12-10 23:20:05,573] Trial 11 finished with value: 0.6826222503227435 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 2, 'lstm_hidden': 96, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.3119109910179524, 'lr': 0.0010215759578799664, 'weight_decay': 5.540535748737848e-05, 'batch_size': 128}. Best is trial 11 with value: 0.6826222503227435.\n",
            "[I 2025-12-10 23:21:37,430] Trial 12 finished with value: 0.6751120143003946 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.1943340682871702, 'lr': 0.001897627751658619, 'weight_decay': 9.26973627695836e-06, 'batch_size': 128}. Best is trial 11 with value: 0.6826222503227435.\n",
            "[I 2025-12-10 23:25:35,456] Trial 13 finished with value: 0.6857900390064231 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.330222144734749, 'lr': 0.0006202506844188844, 'weight_decay': 0.0007018196903507625, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-10 23:29:34,015] Trial 14 finished with value: 0.6694929880137649 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.28721765919853254, 'lr': 0.001039477271273094, 'weight_decay': 0.0009827693994966526, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-10 23:32:01,349] Trial 15 finished with value: 0.6694797549163495 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 2, 'lstm_hidden': 96, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.33268473365772594, 'lr': 0.0006911221145775565, 'weight_decay': 0.00015554059536542362, 'batch_size': 512}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-10 23:36:51,981] Trial 16 finished with value: 0.6777859071908063 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.47466849225553887, 'lr': 0.0003311709039415936, 'weight_decay': 0.002345780119505676, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-10 23:39:24,289] Trial 17 finished with value: 0.6589913956052604 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.20218773812605495, 'lr': 0.0017551075228393978, 'weight_decay': 7.355437621318777e-06, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-10 23:47:46,262] Trial 18 finished with value: 0.6766099624920376 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.24948570596821137, 'lr': 0.0003984280196386759, 'weight_decay': 0.00045868917975757127, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-10 23:55:28,022] Trial 19 finished with value: 0.6735857135808596 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.4053486115388624, 'lr': 0.0007473024782180796, 'weight_decay': 2.164963140027022e-06, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:00:05,992] Trial 20 finished with value: 0.6637009388887244 and parameters: {'d_model': 32, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 32, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.2965034680645032, 'lr': 0.0004394292041892153, 'weight_decay': 4.367106602293923e-05, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:07:04,937] Trial 21 finished with value: 0.6798906073473858 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.46005623059576545, 'lr': 0.0003576637225413206, 'weight_decay': 0.0030034906288510335, 'batch_size': 256}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:11:03,124] Trial 22 finished with value: 0.6856056131227504 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 2, 'lstm_hidden': 64, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.4840547873043678, 'lr': 0.0005706995207067927, 'weight_decay': 0.005325649003495777, 'batch_size': 256}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:12:37,831] Trial 23 finished with value: 0.6645924164890843 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 64, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.318612872840622, 'lr': 0.0004315287854515368, 'weight_decay': 0.0014976090981990058, 'batch_size': 256}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:14:46,862] Trial 24 finished with value: 0.6744492105123089 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 2, 'lstm_hidden': 64, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.45107342272127865, 'lr': 0.0007300241525284566, 'weight_decay': 0.004855557041878007, 'batch_size': 256}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:16:23,881] Trial 25 finished with value: 0.665999174894506 and parameters: {'d_model': 48, 'n_heads': 8, 'n_layers': 1, 'lstm_hidden': 64, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.3859497996633121, 'lr': 0.0010179305699748664, 'weight_decay': 0.0002736341709230032, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:19:07,605] Trial 26 finished with value: 0.6680962638692618 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 64, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.4867898178429928, 'lr': 0.0006707802338869175, 'weight_decay': 3.9274274619581e-05, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:22:31,033] Trial 27 finished with value: 0.6731606395442783 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 2, 'lstm_hidden': 96, 'cnn_channels': 128, 'kernel_size': 3, 'dropout': 0.20648513890930537, 'lr': 0.0006464865982727852, 'weight_decay': 3.390546760034409e-05, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:25:11,549] Trial 28 finished with value: 0.6703425671950388 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 2, 'lstm_hidden': 96, 'cnn_channels': 96, 'kernel_size': 3, 'dropout': 0.3411949210187995, 'lr': 0.0004544209998747041, 'weight_decay': 0.0004366258809730242, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:28:49,958] Trial 29 finished with value: 0.680642374386065 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 2, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.48996725778000966, 'lr': 0.00079383859074352, 'weight_decay': 0.002307676973242855, 'batch_size': 256}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:31:24,655] Trial 30 finished with value: 0.6698838845355819 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 2, 'lstm_hidden': 96, 'cnn_channels': 96, 'kernel_size': 3, 'dropout': 0.3878933503809101, 'lr': 0.0012793294654139804, 'weight_decay': 0.00014140846520098942, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:35:16,083] Trial 31 finished with value: 0.6761126081822931 and parameters: {'d_model': 32, 'n_heads': 4, 'n_layers': 2, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.459532165504561, 'lr': 0.0007477392173755737, 'weight_decay': 0.00086104597049602, 'batch_size': 256}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:39:24,953] Trial 32 finished with value: 0.6675416616618272 and parameters: {'d_model': 48, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.4411402335082784, 'lr': 0.0010592138348894423, 'weight_decay': 0.001985568502301077, 'batch_size': 256}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:42:29,244] Trial 33 finished with value: 0.6682550455830512 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 2, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.4518952900153888, 'lr': 0.0011779594465075836, 'weight_decay': 0.0013749531325635965, 'batch_size': 512}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:44:01,752] Trial 34 finished with value: 0.6677728535248566 and parameters: {'d_model': 64, 'n_heads': 2, 'n_layers': 1, 'lstm_hidden': 96, 'cnn_channels': 64, 'kernel_size': 3, 'dropout': 0.38570854009719524, 'lr': 0.0015407988220404094, 'weight_decay': 2.057167974215237e-05, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:47:10,649] Trial 35 finished with value: 0.670673829177869 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 2, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.31933471967695715, 'lr': 0.0007687218853371378, 'weight_decay': 0.0072912398853154205, 'batch_size': 256}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:50:30,913] Trial 36 finished with value: 0.6801760159016365 and parameters: {'d_model': 48, 'n_heads': 8, 'n_layers': 2, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.4663013147945007, 'lr': 0.0006920053958128826, 'weight_decay': 0.0029834313508013076, 'batch_size': 128}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:52:36,095] Trial 37 finished with value: 0.662859996855045 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 1, 'lstm_hidden': 128, 'cnn_channels': 64, 'kernel_size': 5, 'dropout': 0.4861125407049148, 'lr': 0.0010384339418925152, 'weight_decay': 0.00018154963757712427, 'batch_size': 256}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 00:56:16,135] Trial 38 finished with value: 0.6772801556299606 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 2, 'lstm_hidden': 128, 'cnn_channels': 32, 'kernel_size': 5, 'dropout': 0.4900813467540524, 'lr': 0.0005586733217719798, 'weight_decay': 0.000137924460381591, 'batch_size': 256}. Best is trial 13 with value: 0.6857900390064231.\n",
            "[I 2025-12-11 01:01:33,542] Trial 39 finished with value: 0.6694532127289623 and parameters: {'d_model': 48, 'n_heads': 4, 'n_layers': 2, 'lstm_hidden': 128, 'cnn_channels': 128, 'kernel_size': 5, 'dropout': 0.47779889126006553, 'lr': 0.0006865635430584955, 'weight_decay': 0.00286347296284085, 'batch_size': 512}. Best is trial 13 with value: 0.6857900390064231.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OPTUNA] Mejor AP(val): 0.685790\n",
            "[OPTUNA] Params ganadores: {'d_model': 48, 'n_heads': 2, 'n_layers': 3, 'lstm_hidden': 96, 'cnn_channels': 32, 'kernel_size': 3, 'dropout': 0.330222144734749, 'lr': 0.0006202506844188844, 'weight_decay': 0.0007018196903507625, 'batch_size': 128}\n",
            "[OPTUNA] best_epoch (del trial): 30\n",
            "[OPTUNA] Guardado BEST en: BEST_DL_FULL_SMOTENC.json\n",
            "[OPTUNA] Reentreno final completado. best_epoch = 31\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "N_TRIALS = 40\n",
        "STUDY_NAME = f\"DL_{VIEW_TAG}_{BAL_TAG}_AP\"\n",
        "SAMPLER = TPESampler(seed=RANDOM_STATE, multivariate=True, group=False)\n",
        "study = optuna.create_study(direction=\"maximize\", study_name=STUDY_NAME, sampler=SAMPLER)\n",
        "\n",
        "def suggest_heads_for_dim(trial, d_model):\n",
        "\n",
        "    candidates = [2, 4, 8]\n",
        "    opts = [h for h in candidates if d_model % h == 0 and h <= d_model]\n",
        "    if not opts:\n",
        "        opts = [1]\n",
        "    return trial.suggest_categorical(\"n_heads\", opts)\n",
        "\n",
        "SEARCH_KEYS = [\n",
        "    \"d_model\",\"n_heads\",\"n_layers\",\"lstm_hidden\",\"cnn_channels\",\"kernel_size\",\n",
        "    \"dropout\",\"lr\",\"weight_decay\",\"batch_size\"\n",
        "]\n",
        "\n",
        "def suggest_dl_params(trial):\n",
        "    hp = {}\n",
        "    d_model = trial.suggest_categorical(\"d_model\", [32, 48, 64])\n",
        "    hp[\"d_model\"] = d_model\n",
        "    hp[\"n_heads\"] = suggest_heads_for_dim(trial, d_model)\n",
        "    hp[\"n_layers\"] = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "    hp[\"lstm_hidden\"] = trial.suggest_categorical(\"lstm_hidden\", [32, 64, 96, 128])\n",
        "    hp[\"cnn_channels\"] = trial.suggest_categorical(\"cnn_channels\", [32, 64, 96, 128])\n",
        "    hp[\"kernel_size\"] = trial.suggest_categorical(\"kernel_size\", [3, 5])\n",
        "    hp[\"dropout\"] = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
        "    hp[\"lr\"] = trial.suggest_float(\"lr\", 3e-4, 3e-3, log=True)\n",
        "    hp[\"weight_decay\"] = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
        "    hp[\"batch_size\"] = trial.suggest_categorical(\"batch_size\", [128, 256, 512])\n",
        "    hp[\"epochs\"] = seed_params.get(\"epochs\", 100)\n",
        "    hp[\"patience\"] = seed_params.get(\"patience\", 12)\n",
        "    hp[\"random_state\"] = RANDOM_STATE\n",
        "    hp[\"_did_smote\"] = did_smote_flag\n",
        "    return hp\n",
        "\n",
        "def objective(trial):\n",
        "    hp = suggest_dl_params(trial)\n",
        "    mdl = make_model(n_features=X_train_final.shape[1], hp=hp)\n",
        "    mdl, best_ep, best_ap = train_one(mdl, X_train_final, y_train_final, X_val_fit, y_val, hp, verbose=False)\n",
        "    proba_val_t = predict_proba(mdl, X_val_fit)\n",
        "    ap = average_precision_score(y_val, proba_val_t)\n",
        "    trial.set_user_attr(\"best_epoch\", best_ep)\n",
        "    return ap\n",
        "\n",
        "# Warm-start con BEST\n",
        "if BEST_HP_FILE.exists():\n",
        "    try:\n",
        "        prev = json.loads(BEST_HP_FILE.read_text())\n",
        "        warm = {k: prev[k] for k in SEARCH_KEYS if k in prev}\n",
        "        if warm:\n",
        "            print(\"[OPTUNA] Enqueuing previous BEST as a trial seed.\")\n",
        "            study.enqueue_trial(warm)\n",
        "    except Exception as e:\n",
        "        print(\"[OPTUNA] Aviso: no se pudo usar BEST para warm-start:\", e)\n",
        "\n",
        "print(f\"[OPTUNA] Iniciando estudio '{STUDY_NAME}' con {N_TRIALS} pruebas...\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
        "\n",
        "best = study.best_trial\n",
        "print(f\"[OPTUNA] Mejor AP(val): {best.value:.6f}\")\n",
        "print(f\"[OPTUNA] Params ganadores:\", best.params)\n",
        "print(f\"[OPTUNA] best_epoch (del trial):\", best.user_attrs.get(\"best_epoch\"))\n",
        "\n",
        "best_params = dict(best.params)\n",
        "best_params.update({\n",
        "    \"epochs\": seed_params.get(\"epochs\", 100),\n",
        "    \"patience\": seed_params.get(\"patience\", 12),\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "    \"_did_smote\": did_smote_flag\n",
        "})\n",
        "with open(BEST_HP_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
        "print(\"[OPTUNA] Guardado BEST en:\", BEST_HP_FILE.name)\n",
        "\n",
        "tuned_model = make_model(n_features=X_train_final.shape[1], hp=best_params)\n",
        "tuned_model, best_ep = train_one(tuned_model, X_train_final, y_train_final, X_val_fit, y_val, best_params, verbose=False)[:2]\n",
        "print(\"[OPTUNA] Reentreno final completado. best_epoch =\", best_ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec9",
      "metadata": {},
      "source": [
        "9 — Cross-Validation (OOF) para baseline y tuned (DL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "sec9code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV-BASELINE] Guardados: cv_summary_DL_FULL_SMOTENC_BASELINE_CV5.csv | oof_DL_FULL_SMOTENC_BASELINE_CV5.parquet\n",
            "[CV-TUNED] Guardados: cv_summary_DL_FULL_SMOTENC_TUNED_CV5.csv | oof_DL_FULL_SMOTENC_TUNED_CV5.parquet\n"
          ]
        }
      ],
      "source": [
        "def run_oof_cv_dl(model_hp, X, y, k_folds=CV_FOLDS, seed=RANDOM_STATE, exp_suffix=\"BASELINE\"):\n",
        "    cv_tag = f\"{EXP_NAME}_{exp_suffix}_CV{k_folds}\"\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
        "    oof_proba = np.zeros_like(y, dtype=float)\n",
        "    fold_rows = []\n",
        "\n",
        "    for f, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        X_tr0, X_va0 = X[tr_idx], X[va_idx]\n",
        "        y_tr0, y_va0 = y[tr_idx], y[va_idx]\n",
        "\n",
        "        X_tr, y_tr = X_tr0, y_tr0\n",
        "        did_smote = False\n",
        "        if BALANCE_IN_CV and USE_BALANCED_TRAIN:\n",
        "            X_tr, y_tr = maybe_resample(X_tr0, y_tr0)\n",
        "            did_smote = True\n",
        "\n",
        "        hp = dict(model_hp)\n",
        "        hp[\"_did_smote\"] = did_smote\n",
        "        mdl = make_model(n_features=X.shape[1], hp=hp)\n",
        "        adapter, best_ep, _ = train_one(mdl, X_tr, y_tr, X_va0, y_va0, hp, verbose=False)\n",
        "        proba_va = predict_proba(adapter, X_va0)\n",
        "        oof_proba[va_idx] = proba_va\n",
        "\n",
        "        fold_tag = f\"preds_val_fold{f}_{cv_tag}.parquet\"\n",
        "        pd.DataFrame({\n",
        "            \"idx\": va_idx,\n",
        "            \"proba\": proba_va,\n",
        "            \"y_true\": y_va0\n",
        "        }).to_parquet(OUT_PREDS / fold_tag, index=False)\n",
        "\n",
        "        fold_rows.append({\n",
        "            \"fold\": f,\n",
        "            \"pr_auc\": average_precision_score(y_va0, proba_va),\n",
        "            \"roc_auc\": roc_auc_score(y_va0, proba_va),\n",
        "            \"best_iteration\": best_ep if best_ep is not None else np.nan\n",
        "        })\n",
        "\n",
        "    oof_pr = average_precision_score(y, oof_proba)\n",
        "    oof_roc = roc_auc_score(y, oof_proba)\n",
        "    thr_oof, _ = find_best_threshold(y, oof_proba, metric=\"f1\")\n",
        "    y_oof_pred = (oof_proba >= thr_oof).astype(int)\n",
        "    oof_f1  = f1_score(y, y_oof_pred, zero_division=0)\n",
        "    oof_rec = recall_score(y, y_oof_pred, zero_division=0)\n",
        "    oof_bal = balanced_accuracy_score(y, y_oof_pred)\n",
        "\n",
        "    cv_csv = OUT_RESULTS / f\"cv_summary_{cv_tag}.csv\"\n",
        "    folds_df = pd.DataFrame(fold_rows)\n",
        "    agg_row = pd.DataFrame([{\n",
        "        \"fold\": \"OOF\", \"pr_auc\": oof_pr, \"roc_auc\": oof_roc,\n",
        "        \"thr\": thr_oof, \"f1\": oof_f1, \"recall\": oof_rec, \"bal_acc\": oof_bal\n",
        "    }])\n",
        "    pd.concat([folds_df, agg_row], ignore_index=True).to_csv(cv_csv, index=False)\n",
        "\n",
        "    oof_path = OUT_PREDS / f\"oof_{cv_tag}.parquet\"\n",
        "    pd.DataFrame({\"oof_proba\": oof_proba, \"y_true\": y}).to_parquet(oof_path, index=False)\n",
        "    print(f\"[CV-{exp_suffix}] Guardados: {cv_csv.name} | {oof_path.name}\")\n",
        "\n",
        "    return {\n",
        "        \"oof_pr_auc\": oof_pr,\n",
        "        \"oof_roc_auc\": oof_roc,\n",
        "        \"thr\": thr_oof,\n",
        "        \"oof_f1\": oof_f1,\n",
        "        \"oof_recall\": oof_rec,\n",
        "        \"oof_bal_acc\": oof_bal\n",
        "    }\n",
        "\n",
        "cv_baseline = None\n",
        "cv_tuned = None\n",
        "\n",
        "if DO_CV_BASELINE:\n",
        "    cv_baseline = run_oof_cv_dl(base_hp, X_train_fit, y_train, exp_suffix=\"BASELINE\")\n",
        "\n",
        "if DO_CV_TUNED and \"d_model\" in (best_params or {}):\n",
        "    cv_tuned = run_oof_cv_dl(best_params, X_train_fit, y_train, exp_suffix=\"TUNED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec10",
      "metadata": {},
      "source": [
        "10 — Evaluación en test + guardados (curvas, importancias proxy, preds, baselines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "sec10code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK][BASE] Guardados:\n",
            "  - Seed HPs   : DL_FULL_SMOTENC_BASE_seed_params.json \n",
            "  - Fitted HPs : DL_FULL_SMOTENC_BASE_fitted_params.json \n",
            "  - Importancias: DL_FULL_SMOTENC_feature_importances.csv \n",
            "  - Preds test  : preds_test_DL_FULL_SMOTENC.parquet \n",
            "  - Baselines   : baselines.csv\n",
            "[OK][TUNED] Guardados:\n",
            "  - Fitted HPs : DL_FULL_SMOTENC_TUNED_fitted_params.json \n",
            "  - Importancias: DL_FULL_SMOTENC_TUNED_feature_importances.csv \n",
            "  - Preds test  : preds_test_DL_FULL_SMOTENC_TUNED.parquet \n",
            "  - Baselines   : baselines.csv\n"
          ]
        }
      ],
      "source": [
        "base = EXP_NAME\n",
        "\n",
        "# BASELINE\n",
        "proba_test = predict_proba(model, X_test_fit)\n",
        "y_pred_test = (proba_test >= thr_val).astype(int)\n",
        "test_metrics = compute_all_metrics(y_test, proba_test, thr_val)\n",
        "\n",
        "# Guardar HP baseline\n",
        "params_seed_path = OUT_PARAMS / f\"{base}_BASE_seed_params.json\"\n",
        "with open(params_seed_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(base_hp, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "params_fitted_path = OUT_PARAMS / f\"{base}_BASE_fitted_params.json\"\n",
        "with open(params_fitted_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(base_hp, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Figuras baseline\n",
        "plot_pr_curve(y_val,  proba_val,  f\"{base} — PR (val)\",  OUT_FIGS / f\"{base}_pr_val.png\")\n",
        "plot_pr_curve(y_test, proba_test, f\"{base} — PR (test)\", OUT_FIGS / f\"{base}_pr_test.png\")\n",
        "plot_roc_curve(y_val,  proba_val,  f\"{base} — ROC (val)\",  OUT_FIGS / f\"{base}_roc_val.png\")\n",
        "plot_roc_curve(y_test, proba_test, f\"{base} — ROC (test)\", OUT_FIGS / f\"{base}_roc_test.png\")\n",
        "plot_confusion(y_test, y_pred_test, f\"{base} — Confusion (test @thr={thr_val:.3f})\", OUT_FIGS / f\"{base}_cm_test.png\")\n",
        "\n",
        "# Importancias proxy\n",
        "try:\n",
        "    imp_df = model.feature_importance(feature_names_used)\n",
        "except Exception:\n",
        "    imp_df = pd.DataFrame({\"feature\": feature_names_used, \"importance_proxy\": np.zeros(len(feature_names_used))})\n",
        "imp_path = OUT_RESULTS / f\"{base}_feature_importances.csv\"\n",
        "imp_df.to_csv(imp_path, index=False)\n",
        "\n",
        "# Preds test baseline\n",
        "preds_path = OUT_PREDS / f\"preds_test_{base}.parquet\"\n",
        "pd.DataFrame({\"proba\": proba_test, \"y_true\": y_test}).to_parquet(preds_path, index=False)\n",
        "\n",
        "row_base = {\n",
        "    \"model\": base,\n",
        "    \"thr_val\": thr_val,\n",
        "    \"val_pr_auc\": val_metrics[\"pr_auc\"],\n",
        "    \"val_roc_auc\": val_metrics[\"roc_auc\"],\n",
        "    \"val_precision\": val_metrics[\"precision\"],\n",
        "    \"val_f1\": val_metrics[\"f1\"],\n",
        "    \"val_recall\": val_metrics[\"recall\"],\n",
        "    \"val_bal_acc\": val_metrics[\"bal_acc\"],\n",
        "    \"test_pr_auc\": test_metrics[\"pr_auc\"],\n",
        "    \"test_roc_auc\": test_metrics[\"roc_auc\"],\n",
        "    \"test_precision\": test_metrics[\"precision\"],\n",
        "    \"test_f1\": test_metrics[\"f1\"],\n",
        "    \"test_recall\": test_metrics[\"recall\"],\n",
        "    \"test_bal_acc\": test_metrics[\"bal_acc\"],\n",
        "    \"best_iteration\": base_best_it if base_best_it is not None else np.nan\n",
        "}\n",
        "res_csv = OUT_RESULTS / \"baselines.csv\"\n",
        "pd.DataFrame([row_base]).to_csv(res_csv, mode=(\"a\" if res_csv.exists() else \"w\"), index=False, header=not res_csv.exists())\n",
        "\n",
        "print(\"[OK][BASE] Guardados:\\n  - Seed HPs   :\", params_seed_path.name,\n",
        "      \"\\n  - Fitted HPs :\", params_fitted_path.name,\n",
        "      \"\\n  - Importancias:\", imp_path.name,\n",
        "      \"\\n  - Preds test  :\", preds_path.name,\n",
        "      \"\\n  - Baselines   :\", res_csv.name)\n",
        "\n",
        "# TUNED\n",
        "if tuned_model is not None and best_params is not None:\n",
        "    proba_val_tuned = predict_proba(tuned_model, X_val_fit)\n",
        "    thr_val_tuned, _ = find_best_threshold(y_val, proba_val_tuned, metric=\"f1\")\n",
        "    # Guardar predicciones de validación (tuned)\n",
        "    val_tuned_path = OUT_PREDS / f\"preds_val_{base}_TUNED.parquet\"\n",
        "    pd.DataFrame({\n",
        "        \"proba\": proba_val_tuned,\n",
        "        \"y_true\": y_val,\n",
        "        \"y_pred\": (proba_val_tuned >= thr_val_tuned).astype(int)\n",
        "    }).to_parquet(val_tuned_path, index=False)\n",
        "    val_metrics_tuned = compute_all_metrics(y_val, proba_val_tuned, thr_val_tuned)\n",
        "\n",
        "    proba_test_tuned = predict_proba(tuned_model, X_test_fit)\n",
        "    y_pred_test_tuned = (proba_test_tuned >= thr_val_tuned).astype(int)\n",
        "    test_metrics_tuned = compute_all_metrics(y_test, proba_test_tuned, thr_val_tuned)\n",
        "\n",
        "    tuned_fitted_path = OUT_PARAMS / f\"{base}_TUNED_fitted_params.json\"\n",
        "    with open(tuned_fitted_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    base_t = base + \"_TUNED\"\n",
        "    plot_pr_curve(y_val,  proba_val_tuned,  f\"{base_t} — PR (val)\",  OUT_FIGS / f\"{base_t}_pr_val.png\")\n",
        "    plot_pr_curve(y_test, proba_test_tuned, f\"{base_t} — PR (test)\", OUT_FIGS / f\"{base_t}_pr_test.png\")\n",
        "    plot_roc_curve(y_val,  proba_val_tuned,  f\"{base_t} — ROC (val)\",  OUT_FIGS / f\"{base_t}_roc_val.png\")\n",
        "    plot_roc_curve(y_test, proba_test_tuned, f\"{base_t} — ROC (test)\", OUT_FIGS / f\"{base_t}_roc_test.png\")\n",
        "    plot_confusion(y_test, y_pred_test_tuned, f\"{base_t} — Confusion (test @thr={thr_val_tuned:.3f})\", OUT_FIGS / f\"{base_t}_cm_test.png\")\n",
        "\n",
        "    try:\n",
        "        imp_t_df = tuned_model.feature_importance(feature_names_used)\n",
        "    except Exception:\n",
        "        imp_t_df = pd.DataFrame({\"feature\": feature_names_used, \"importance_proxy\": np.zeros(len(feature_names_used))})\n",
        "    imp_t_path = OUT_RESULTS / f\"{base_t}_feature_importances.csv\"\n",
        "    imp_t_df.to_csv(imp_t_path, index=False)\n",
        "\n",
        "    preds_t_path = OUT_PREDS / f\"preds_test_{base_t}.parquet\"\n",
        "    pd.DataFrame({\"proba\": proba_test_tuned, \"y_true\": y_test}).to_parquet(preds_t_path, index=False)\n",
        "\n",
        "    row_t = {\n",
        "        \"model\": base_t,\n",
        "        \"thr_val\": thr_val_tuned,\n",
        "        \"val_pr_auc\": val_metrics_tuned[\"pr_auc\"],\n",
        "        \"val_roc_auc\": val_metrics_tuned[\"roc_auc\"],\n",
        "        \"val_precision\": val_metrics_tuned[\"precision\"],\n",
        "        \"val_f1\": val_metrics_tuned[\"f1\"],\n",
        "        \"val_recall\": val_metrics_tuned[\"recall\"],\n",
        "        \"val_bal_acc\": val_metrics_tuned[\"bal_acc\"],\n",
        "        \"test_pr_auc\": test_metrics_tuned[\"pr_auc\"],\n",
        "        \"test_roc_auc\": test_metrics_tuned[\"roc_auc\"],\n",
        "        \"test_precision\": test_metrics_tuned[\"precision\"],\n",
        "        \"test_f1\": test_metrics_tuned[\"f1\"],\n",
        "        \"test_recall\": test_metrics_tuned[\"recall\"],\n",
        "        \"test_bal_acc\": test_metrics_tuned[\"bal_acc\"],\n",
        "        \"best_iteration\": best_ep if best_ep is not None else np.nan\n",
        "    }\n",
        "    pd.DataFrame([row_t]).to_csv(res_csv, mode=\"a\", index=False, header=False)\n",
        "\n",
        "    print(\"[OK][TUNED] Guardados:\\n  - Fitted HPs :\", tuned_fitted_path.name,\n",
        "          \"\\n  - Importancias:\", imp_t_path.name,\n",
        "          \"\\n  - Preds test  :\", preds_t_path.name,\n",
        "          \"\\n  - Baselines   :\", res_csv.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec11",
      "metadata": {},
      "source": [
        "11 — Mejores resultados + resumen CV (formato similar a tu XGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "sec11code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MEJORES EN TEST (por métrica) ===\n",
            "- test_pr_auc: DL_FULL_SMOTENC_TUNED | PR-AUC=0.7045 | ROC-AUC=0.8584 | F1=0.6183 | Recall=0.6069 | Precision=0.6301 | thr(val)=0.710 | best_iter=31\n",
            "- test_roc_auc: DL_FULL_SMOTENC_TUNED | PR-AUC=0.7045 | ROC-AUC=0.8584 | F1=0.6183 | Recall=0.6069 | Precision=0.6301 | thr(val)=0.710 | best_iter=31\n",
            "- test_recall: DL_FULL_SMOTENC_TUNED | PR-AUC=0.7045 | ROC-AUC=0.8584 | F1=0.6183 | Recall=0.6069 | Precision=0.6301 | thr(val)=0.710 | best_iter=31\n",
            "- test_f1: DL_FULL_SMOTENC_TUNED | PR-AUC=0.7045 | ROC-AUC=0.8584 | F1=0.6183 | Recall=0.6069 | Precision=0.6301 | thr(val)=0.710 | best_iter=31\n",
            "- test_precision: DL_FULL_SMOTENC | PR-AUC=0.6995 | ROC-AUC=0.8515 | F1=0.6069 | Recall=0.5405 | Precision=0.6918 | thr(val)=0.814 | best_iter=21\n",
            "=== RESUMEN CV-OOF (por experimento) ===\n",
            "                         tag   pr_auc  roc_auc       f1   recall  bal_acc   thr\n",
            "   DL_FULL_SMOTENC_TUNED_CV5 0.676418 0.845732 0.608551 0.599346 0.752258 0.683\n",
            "DL_FULL_SMOTENC_BASELINE_CV5 0.673205 0.848232 0.607318 0.658217 0.763900 0.639\n",
            "[OK] Normalizado. Backup: baselines_legacy_backup.csv\n"
          ]
        }
      ],
      "source": [
        "AGGREGATE_ALL_RUNS = False\n",
        "\n",
        "def safe(v, fmt=\".4f\"):\n",
        "    try:\n",
        "        return f\"{float(v):{fmt}}\"\n",
        "    except Exception:\n",
        "        return \"NA\"\n",
        "\n",
        "base_csv = OUT_RESULTS / \"baselines.csv\"\n",
        "if not base_csv.exists():\n",
        "    raise FileNotFoundError(f\"No existe {base_csv}\")\n",
        "\n",
        "df = pd.read_csv(base_csv)\n",
        "\n",
        "needed = [\n",
        "    \"model\",\"thr_val\",\n",
        "    \"val_pr_auc\",\"val_roc_auc\",\"val_precision\",\"val_f1\",\"val_recall\",\"val_bal_acc\",\n",
        "    \"test_pr_auc\",\"test_roc_auc\",\"test_precision\",\"test_f1\",\"test_recall\",\"test_bal_acc\",\n",
        "    \"best_iteration\"\n",
        "]\n",
        "for c in needed:\n",
        "    if c not in df.columns:\n",
        "        df[c] = pd.NA \n",
        "\n",
        "df = df[needed].copy()\n",
        "\n",
        "num_cols = [c for c in needed if c not in (\"model\",)]\n",
        "for c in num_cols:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "if AGGREGATE_ALL_RUNS:\n",
        "    root_art = ARTIF_DIR.parent\n",
        "    for p in (root_art).glob(\"DL_*/results/baselines.csv\"):\n",
        "        if p == base_csv:\n",
        "            continue\n",
        "        try:\n",
        "            d2 = pd.read_csv(p)\n",
        "            for c in needed:\n",
        "                if c not in d2.columns:\n",
        "                    d2[c] = pd.NA\n",
        "            d2 = d2[needed]\n",
        "            for c in num_cols:\n",
        "                d2[c] = pd.to_numeric(d2[c], errors=\"coerce\")\n",
        "            df = pd.concat([df, d2], ignore_index=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(\"El dataframe de resultados está vacío.\")\n",
        "\n",
        "df = df.drop_duplicates(subset=[\"model\"], keep=\"last\").copy()\n",
        "\n",
        "def best_by(metric):\n",
        "    if metric not in df.columns or df[metric].dropna().empty:\n",
        "        return None\n",
        "    r = df.loc[df[metric].idxmax()]\n",
        "    print(\n",
        "        f\"- {metric}: {r['model']} | \"\n",
        "        f\"PR-AUC={safe(r['test_pr_auc'])} | \"\n",
        "        f\"ROC-AUC={safe(r['test_roc_auc'])} | \"\n",
        "        f\"F1={safe(r['test_f1'])} | \"\n",
        "        f\"Recall={safe(r['test_recall'])} | \"\n",
        "        f\"Precision={safe(r['test_precision'])} | \"\n",
        "        f\"thr(val)={safe(r['thr_val'], '.3f')} | \"\n",
        "        f\"best_iter={int(r['best_iteration']) if pd.notna(r['best_iteration']) else 'NA'}\"\n",
        "    )\n",
        "    return r\n",
        "\n",
        "print(\"=== MEJORES EN TEST (por métrica) ===\")\n",
        "winners = {}\n",
        "for m in [\"test_pr_auc\",\"test_roc_auc\",\"test_recall\",\"test_f1\",\"test_precision\"]:\n",
        "    w = best_by(m)\n",
        "    if w is not None:\n",
        "        winners[m] = w\n",
        "\n",
        "cv_files = list(OUT_RESULTS.glob(\"cv_summary_*_CV*.csv\"))\n",
        "if cv_files:\n",
        "    print(\"=== RESUMEN CV-OOF (por experimento) ===\")\n",
        "    rows = []\n",
        "    for f in cv_files:\n",
        "        tag = re.sub(r\"^cv_summary_|\\.csv$\", \"\", f.name)\n",
        "        cv = pd.read_csv(f)\n",
        "        oof = cv.loc[cv[\"fold\"] == \"OOF\"]\n",
        "        if not oof.empty:\n",
        "            r = oof.iloc[0]\n",
        "            rows.append({\n",
        "                \"tag\": tag,\n",
        "                \"pr_auc\": r.get(\"pr_auc\"),\n",
        "                \"roc_auc\": r.get(\"roc_auc\"),\n",
        "                \"f1\": r.get(\"f1\"),\n",
        "                \"recall\": r.get(\"recall\"),\n",
        "                \"bal_acc\": r.get(\"bal_acc\"),\n",
        "                \"thr\": r.get(\"thr\"),\n",
        "            })\n",
        "    if rows:\n",
        "        print(pd.DataFrame(rows).sort_values([\"pr_auc\",\"roc_auc\"], ascending=False).to_string(index=False))\n",
        "else:\n",
        "    print(\"(No se hallaron archivos de CV para este experimento)\")\n",
        "\n",
        "# Normalización/backup\n",
        "backup = OUT_RESULTS / \"baselines_legacy_backup.csv\"\n",
        "base_csv.replace(backup)\n",
        "df.to_csv(base_csv, index=False)\n",
        "print(\"[OK] Normalizado. Backup:\", backup.name)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
