{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2aec4297",
      "metadata": {},
      "source": [
        "1 — Imports, configuración y rutas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "id": "565a4b55",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exp: XGB_FULL_SMOTENC\n",
            "DATA_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/preproc_datasets/full\n",
            "ARTIF_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_FULL_SMOTENC\n"
          ]
        }
      ],
      "source": [
        "import json, os, warnings, time, re, glob\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    average_precision_score, precision_recall_curve, roc_auc_score, roc_curve,\n",
        "    f1_score, recall_score, balanced_accuracy_score, confusion_matrix, precision_score\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Balanceo (SMOTENC)\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTENC, SMOTE\n",
        "    _HAS_IMBLEARN = True\n",
        "except Exception:\n",
        "    _HAS_IMBLEARN = False\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# === Toggles de experimento ===\n",
        "USE_REDUCED = False              # selección MI top-k\n",
        "USE_BALANCED_TRAIN = True        # SMOTE sobre el train final\n",
        "BALANCE_IN_CV = True             # SMOTE dentro de cada fold de CV\n",
        "RANDOM_STATE = 42\n",
        "DO_TUNE = True\n",
        "DO_CV_BASELINE = True\n",
        "DO_CV_TUNED = True\n",
        "CV_FOLDS = 5\n",
        "MI_TOPK = 30                     # K para selección MI si USE_REDUCED=True\n",
        "\n",
        "# === Nombres y rutas ===\n",
        "ROOT = Path.cwd().parent\n",
        "EXP_NAME = f\"XGB_{'REDUCED' if USE_REDUCED else 'FULL'}_{'SMOTENC' if USE_BALANCED_TRAIN else 'IMB'}\"\n",
        "ARTIF_DIR = ROOT / \"artifacts\" / EXP_NAME\n",
        "OUT_RESULTS = ARTIF_DIR / \"results\"\n",
        "OUT_FIGS    = ARTIF_DIR / \"figs\"\n",
        "OUT_PREDS   = ARTIF_DIR / \"preds\"\n",
        "OUT_PARAMS  = ARTIF_DIR / \"best_params\"\n",
        "for p in [OUT_RESULTS, OUT_FIGS, OUT_PREDS, OUT_PARAMS]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Dataset preprocesado\n",
        "DATA_DIR = ROOT / \"preproc_datasets\" / \"full\"\n",
        "\n",
        "print(\"Exp:\", EXP_NAME)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"ARTIF_DIR:\", ARTIF_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cd89f52",
      "metadata": {},
      "source": [
        "2 — Carga de artefactos (X, y, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "id": "38f0a43f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes: (6000, 15) (2000, 15) (2000, 15)\n",
            "y train/val/test: (6000,) (2000,) (2000,)\n",
            "n features: 15\n",
            "[META] Archivo detectado: N/D | columnas categóricas=0\n",
            "[META] No se detectaron columnas categóricas en metadatos. Caerá en SMOTE estándar.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "def load_xy_full(dir_full: Path):\n",
        "    X_train = np.load(dir_full / \"X_train_full.npy\")\n",
        "    X_val   = np.load(dir_full / \"X_val_full.npy\")\n",
        "    X_test  = np.load(dir_full / \"X_test_full.npy\")\n",
        "\n",
        "    y_train = pd.read_parquet(dir_full / \"y_train.parquet\")[\"Exited\"].to_numpy()\n",
        "    y_val   = pd.read_parquet(dir_full / \"y_val.parquet\")[\"Exited\"].to_numpy()\n",
        "    y_test  = pd.read_parquet(dir_full / \"y_test.parquet\")[\"Exited\"].to_numpy()\n",
        "\n",
        "    feat = pd.read_parquet(dir_full / \"feature_names_full.parquet\")[\"feature\"].tolist()\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, feat\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_xy_full(DATA_DIR)\n",
        "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "print(\"y train/val/test:\", y_train.shape, y_val.shape, y_test.shape)\n",
        "print(\"n features:\", len(feature_names))\n",
        "\n",
        "def _read_feature_roles(dir_full: Path):\n",
        "    candidates = [\n",
        "        dir_full / \"feature_roles_full.parquet\",\n",
        "        dir_full / \"feature_roles.parquet\",\n",
        "        dir_full / \"feature_meta_full.parquet\",\n",
        "        dir_full / \"feature_meta.parquet\",\n",
        "        dir_full / \"feature_types_full.parquet\",\n",
        "        dir_full / \"feature_types.parquet\",\n",
        "        dir_full / \"feature_meta.json\",\n",
        "    ]\n",
        "    for p in candidates:\n",
        "        if p.exists():\n",
        "            if p.suffix == \".parquet\":\n",
        "                df = pd.read_parquet(p)\n",
        "            elif p.suffix == \".json\":\n",
        "                obj = json.loads(p.read_text())\n",
        "                # admitimos lista de dicts o dict con lista\n",
        "                if isinstance(obj, dict) and \"features\" in obj:\n",
        "                    df = pd.DataFrame(obj[\"features\"])\n",
        "                else:\n",
        "                    df = pd.DataFrame(obj)\n",
        "            else:\n",
        "                continue\n",
        "            return df, p.name\n",
        "    return None, None\n",
        "\n",
        "def _build_cat_idx(feature_names, roles_df):\n",
        "\n",
        "    if roles_df is None or len(roles_df) == 0:\n",
        "        return []\n",
        "    df = roles_df.copy()\n",
        "    df.columns = [str(c).lower() for c in df.columns]\n",
        "\n",
        "    if \"feature\" not in df.columns:\n",
        "        if \"name\" in df.columns:\n",
        "            df[\"feature\"] = df[\"name\"]\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    cat_names = set()\n",
        "    if \"role\" in df.columns:\n",
        "        cat_names = set(\n",
        "            df.loc[df[\"role\"].astype(str).str.lower().isin(\n",
        "                [\"cat\", \"categorical\", \"bin\", \"binary\", \"ordinal\"]\n",
        "            ), \"feature\"]\n",
        "        )\n",
        "    elif \"dtype\" in df.columns:\n",
        "        cat_names = set(\n",
        "            df.loc[df[\"dtype\"].astype(str).str.lower().isin(\n",
        "                [\"category\", \"categorical\", \"object\", \"bool\"]\n",
        "            ), \"feature\"]\n",
        "        )\n",
        "    elif \"is_cat\" in df.columns:\n",
        "        cat_names = set(df.loc[df[\"is_cat\"].astype(bool), \"feature\"])\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "    idx = [i for i, f in enumerate(feature_names) if f in cat_names]\n",
        "    return sorted(idx)\n",
        "\n",
        "roles_df, meta_file = _read_feature_roles(DATA_DIR)\n",
        "CAT_IDX_FULL = _build_cat_idx(feature_names, roles_df)\n",
        "\n",
        "print(f\"[META] Archivo detectado: {meta_file or 'N/D'} | columnas categóricas={len(CAT_IDX_FULL)}\")\n",
        "if CAT_IDX_FULL:\n",
        "    preview = CAT_IDX_FULL[:10]\n",
        "    print(f\"[META] Índices categóricos (primeros 10): {preview}{'...' if len(CAT_IDX_FULL) > 10 else ''}\")\n",
        "else:\n",
        "    print(\"[META] No se detectaron columnas categóricas en metadatos. Caerá en SMOTE estándar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e693828",
      "metadata": {},
      "source": [
        "3 — Métricas, threshold y plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "650b59ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pr_auc(y_true, y_proba): \n",
        "    return float(average_precision_score(y_true, y_proba))\n",
        "\n",
        "def roc_auc(y_true, y_proba): \n",
        "    return float(roc_auc_score(y_true, y_proba))\n",
        "\n",
        "def find_best_threshold(y_true, y_proba, metric=\"f1\"):\n",
        "    thr_grid = np.linspace(0.0, 1.0, 1001)\n",
        "    best_thr, best_score = 0.5, -1.0\n",
        "    for thr in thr_grid:\n",
        "        y_pred = (y_proba >= thr).astype(int)\n",
        "        if metric == \"f1\":\n",
        "            score = f1_score(y_true, y_pred, zero_division=0)\n",
        "        elif metric == \"recall\":\n",
        "            score = recall_score(y_true, y_pred, zero_division=0)\n",
        "        else:\n",
        "            raise ValueError(\"metric no soportada\")\n",
        "        if score > best_score:\n",
        "            best_score, best_thr = score, thr\n",
        "    return float(best_thr), float(best_score)\n",
        "\n",
        "def compute_all_metrics(y_true, y_proba, thr):\n",
        "    y_pred = (y_proba >= thr).astype(int)\n",
        "    return {\n",
        "        \"pr_auc\": pr_auc(y_true, y_proba),\n",
        "        \"roc_auc\": roc_auc(y_true, y_proba),\n",
        "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"bal_acc\": balanced_accuracy_score(y_true, y_pred)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7943526",
      "metadata": {},
      "source": [
        "4 — Helpers MI Top-K y balanceo in-memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "id": "95ecc2fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "class _BoosterAdapter:\n",
        "    def __init__(self, booster, params, best_iteration, feature_names=None):\n",
        "        self._booster = booster\n",
        "        self._params = dict(params)\n",
        "        self.best_iteration = best_iteration\n",
        "        self._feature_names = feature_names\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        d = xgb.DMatrix(X, feature_names=self._feature_names)\n",
        "        if self.best_iteration is not None:\n",
        "            pred = self._booster.predict(d, iteration_range=(0, int(self.best_iteration) + 1))\n",
        "        else:\n",
        "            pred = self._booster.predict(d)\n",
        "        return np.column_stack([1.0 - pred, pred])\n",
        "\n",
        "    def get_booster(self):\n",
        "        return self._booster\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return dict(self._params)\n",
        "\n",
        "def xgb_fit_with_es(\n",
        "    sk_model, X_tr, y_tr, X_va, y_va,\n",
        "    feature_names=None, rounds=200, verbose=False\n",
        "):\n",
        "    p = sk_model.get_params()\n",
        "    n_estimators = p.pop(\"n_estimators\", 1000)\n",
        "    n_estimators = 1000 if n_estimators is None else int(n_estimators)\n",
        "\n",
        "    seed = p.pop(\"random_state\", p.pop(\"seed\", 42))\n",
        "    nthread = p.pop(\"n_jobs\", None)\n",
        "    if nthread is not None:\n",
        "        p[\"nthread\"] = nthread  # alias clásico\n",
        "\n",
        "    p.setdefault(\"seed\", seed)\n",
        "    p.setdefault(\"objective\", \"binary:logistic\")\n",
        "    p.setdefault(\"eval_metric\", \"aucpr\")\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_tr, label=y_tr, feature_names=feature_names)\n",
        "    dvalid = xgb.DMatrix(X_va, label=y_va, feature_names=feature_names)\n",
        "\n",
        "    evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
        "    booster = xgb.train(\n",
        "        params=p,\n",
        "        dtrain=dtrain,\n",
        "        num_boost_round=n_estimators,\n",
        "        evals=evals,\n",
        "        early_stopping_rounds=rounds,\n",
        "        verbose_eval=verbose\n",
        "    )\n",
        "\n",
        "    best_iter = getattr(booster, \"best_iteration\", None)\n",
        "    adapter = _BoosterAdapter(\n",
        "        booster=booster,\n",
        "        params={**sk_model.get_params(), \"best_iteration\": best_iter},\n",
        "        best_iteration=best_iter,\n",
        "        feature_names=feature_names\n",
        "    )\n",
        "    return adapter, best_iter\n",
        "\n",
        "def _map_cat_idx_for_keep(keep_idx, cat_idx_full):\n",
        "    if not cat_idx_full:\n",
        "        return []\n",
        "    if keep_idx is None:\n",
        "        return sorted(cat_idx_full)\n",
        "    pos = {old_i: j for j, old_i in enumerate(keep_idx)}\n",
        "    return sorted([pos[i] for i in cat_idx_full if i in pos])\n",
        "\n",
        "try:\n",
        "    apply_keep_idx\n",
        "except NameError:\n",
        "    def apply_keep_idx(X, keep_idx):\n",
        "        return X[:, keep_idx]\n",
        "\n",
        "def maybe_smote(X, y, keep_idx=None, random_state=RANDOM_STATE, k_neighbors=5):\n",
        "\n",
        "    if not _HAS_IMBLEARN:\n",
        "        print(\"[BAL] imbalanced-learn no disponible. Se omite balanceo.\")\n",
        "        return X, y\n",
        "\n",
        "    y_int = y.astype(int)\n",
        "    if y_int.max() == 0:     \n",
        "        print(\"[BAL] Solo 1 clase en y. Se omite balanceo.\")\n",
        "        return X, y\n",
        "    counts = np.bincount(y_int)\n",
        "    if len(counts) < 2 or counts.min() < 2:\n",
        "        print(\"[BAL] Minoría < 2 muestras. Se omite balanceo.\")\n",
        "        return X, y\n",
        "    k = int(max(1, min(k_neighbors, counts.min() - 1)))\n",
        "\n",
        "\n",
        "    cat_idx = _map_cat_idx_for_keep(keep_idx, CAT_IDX_FULL)\n",
        "\n",
        "    if cat_idx:\n",
        "        sm = SMOTENC(categorical_features=cat_idx, k_neighbors=k, random_state=random_state)\n",
        "        kind = \"SMOTENC\"\n",
        "    else:\n",
        "        sm = SMOTE(k_neighbors=k, random_state=random_state)\n",
        "        kind = \"SMOTE\"\n",
        "\n",
        "    X_res, y_res = sm.fit_resample(X, y)\n",
        "    try:\n",
        "        X_res = X_res.astype(X.dtype, copy=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    print(f\"[BAL] {kind} aplicado | k_neighbors={k} | cat_cols={len(cat_idx)}\")\n",
        "    return X_res, y_res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d86df8a",
      "metadata": {},
      "source": [
        "5 — Hiperparámetros persistentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "32cca868",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HP] Cargando mejores hiperparámetros previos: BEST_XGB_FULL_SMOTENC.json\n"
          ]
        }
      ],
      "source": [
        "VIEW_TAG = \"REDUCED\" if USE_REDUCED else \"FULL\"\n",
        "BAL_TAG  = \"SMOTENC\" if USE_BALANCED_TRAIN else \"IMB\"\n",
        "BEST_HP_FILE = OUT_PARAMS / f\"BEST_XGB_{VIEW_TAG}_{BAL_TAG}.json\"\n",
        "\n",
        "def get_xgb_defaults(seed=RANDOM_STATE):\n",
        "    mdl = XGBClassifier(\n",
        "        random_state=seed,\n",
        "        n_jobs=-1,\n",
        "        eval_metric=\"aucpr\",\n",
        "        tree_method=\"hist\",\n",
        "        verbosity=0,\n",
        "    )\n",
        "    params = mdl.get_params()\n",
        "    # normalizamos la clave de verbosity\n",
        "    params.pop(\"verbose\", None)\n",
        "    params.setdefault(\"verbosity\", 0)\n",
        "    return params\n",
        "\n",
        "def load_best_or_default():\n",
        "    if BEST_HP_FILE.exists():\n",
        "        try:\n",
        "            best = json.loads(BEST_HP_FILE.read_text())\n",
        "            print(\"[HP] Cargando mejores hiperparámetros previos:\", BEST_HP_FILE.name)\n",
        "            base = get_xgb_defaults()\n",
        "            base.update(best)\n",
        "            return base, True\n",
        "        except Exception as e:\n",
        "            print(\"[HP] Aviso: no se pudo leer BEST (uso defaults).\", e)\n",
        "    print(\"[HP] Usando hiperparámetros DEFAULT de XGB.\")\n",
        "    return get_xgb_defaults(), False\n",
        "\n",
        "seed_params, loaded_best_flag = load_best_or_default()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfed62da",
      "metadata": {},
      "source": [
        "6 — Entrenamiento BASELINE + umbral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "id": "ea25cb27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[BASELINE] best_iteration: 1984\n",
            "[BASELINE] Mejor umbral (val) por F1: 0.463 | F1(val)=0.6435\n",
            "[BASELINE] Métricas val: {'pr_auc': 0.6963, 'roc_auc': 0.8596, 'precision': 0.6757, 'f1': 0.6435, 'recall': 0.6143, 'bal_acc': 0.7695}\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "seed_params = dict(seed_params)\n",
        "\n",
        "seed_params.setdefault(\"random_state\", RANDOM_STATE)\n",
        "seed_params.setdefault(\"n_jobs\", -1)\n",
        "seed_params.setdefault(\"eval_metric\", \"aucpr\")\n",
        "seed_params.setdefault(\"tree_method\", \"hist\")\n",
        "seed_params.setdefault(\"verbosity\", 0)\n",
        "seed_params.pop(\"verbose\", None)\n",
        "\n",
        "seed_params[\"n_estimators\"] = seed_params.get(\"n_estimators\") or 1000\n",
        "\n",
        "if seed_params.get(\"n_estimators\") is None:\n",
        "    seed_params.pop(\"n_estimators\", None)\n",
        "\n",
        "keep_idx_global = None\n",
        "feature_names_used = feature_names\n",
        "X_train_fit, X_val_fit, X_test_fit = X_train, X_val, X_test\n",
        "\n",
        "if USE_REDUCED:\n",
        "    keep_idx_global, _mi = fit_mi_selector(X_train, y_train, topk=MI_TOPK, seed=RANDOM_STATE)\n",
        "    X_train_fit = apply_keep_idx(X_train, keep_idx_global)\n",
        "    X_val_fit   = apply_keep_idx(X_val,   keep_idx_global)\n",
        "    X_test_fit  = apply_keep_idx(X_test,  keep_idx_global)\n",
        "    feature_names_used = [feature_names[i] for i in keep_idx_global]\n",
        "\n",
        "X_train_final, y_train_final = X_train_fit, y_train\n",
        "if USE_BALANCED_TRAIN:\n",
        "    X_train_final, y_train_final = maybe_smote(X_train_fit, y_train, keep_idx=keep_idx_global)\n",
        "\n",
        "model = XGBClassifier(**seed_params)\n",
        "model, best_iter = xgb_fit_with_es(\n",
        "    model,\n",
        "    X_train_final, y_train_final,\n",
        "    X_val_fit, y_val,\n",
        "    feature_names=feature_names_used,\n",
        "    rounds=200,\n",
        "    verbose=False\n",
        ")\n",
        "print(f\"[BASELINE] best_iteration: {best_iter}\")\n",
        "\n",
        "proba_val = model.predict_proba(X_val_fit)[:, 1]\n",
        "thr_val, best_f1_val = find_best_threshold(y_val, proba_val, metric=\"f1\")\n",
        "print(f\"[BASELINE] Mejor umbral (val) por F1: {thr_val:.3f} | F1(val)={best_f1_val:.4f}\")\n",
        "\n",
        "val_metrics = compute_all_metrics(y_val, proba_val, thr_val)\n",
        "print(\"[BASELINE] Métricas val:\", {k: (round(v,4) if isinstance(v,float) else v) for k,v in val_metrics.items()})\n",
        "\n",
        "\n",
        "baseline = model\n",
        "base_best_it = best_iter\n",
        "tuned_model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94d893d5",
      "metadata": {},
      "source": [
        "7 — Optimización incremental (Optuna)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "id": "03c63f35",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-09 18:18:50,347] A new study created in memory with name: XGB_FULL_SMOTENC_AP\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OPTUNA] Enqueuing previous BEST as a trial seed.\n",
            "[OPTUNA] Iniciando estudio 'XGB_FULL_SMOTENC_AP' con 40 pruebas...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-09 18:18:54,208] Trial 0 finished with value: 0.6962975223192452 and parameters: {'learning_rate': 0.024657174052728027, 'n_estimators': 2350, 'max_depth': 3, 'min_child_weight': 0.8535743139140601, 'subsample': 0.9369195277814497, 'colsample_bytree': 0.6938682402744633, 'gamma': 1.703067625690673e-06, 'reg_alpha': 0.0004414234817891946, 'reg_lambda': 2.355922278034951}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:18:59,270] Trial 1 finished with value: 0.6896356488001449 and parameters: {'learning_rate': 0.008468008575248327, 'n_estimators': 2900, 'max_depth': 8, 'min_child_weight': 4.550475813202184, 'subsample': 0.6624074561769746, 'colsample_bytree': 0.662397808134481, 'gamma': 3.200866785899844e-08, 'reg_alpha': 1.156732719914599, 'reg_lambda': 0.016136341713591334}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:19:00,257] Trial 2 finished with value: 0.6804528020112258 and parameters: {'learning_rate': 0.05675206026988748, 'n_estimators': 800, 'max_depth': 10, 'min_child_weight': 10.779361932748845, 'subsample': 0.6849356442713105, 'colsample_bytree': 0.6727299868828402, 'gamma': 3.939402261362697e-07, 'reg_alpha': 0.000134801802908908, 'reg_lambda': 0.004712973756110786}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:19:03,277] Trial 3 finished with value: 0.6899644014400148 and parameters: {'learning_rate': 0.01174843954800703, 'n_estimators': 1450, 'max_depth': 7, 'min_child_weight': 0.8364645453054501, 'subsample': 0.7168578594140873, 'colsample_bytree': 0.7465447373174767, 'gamma': 9.275538076980542e-05, 'reg_alpha': 0.3134958021096912, 'reg_lambda': 2.498713568466947e-05}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:19:06,586] Trial 4 finished with value: 0.6784763018831735 and parameters: {'learning_rate': 0.018785426399210624, 'n_estimators': 2100, 'max_depth': 3, 'min_child_weight': 4.702115628087815, 'subsample': 0.6682096494749166, 'colsample_bytree': 0.6260206371941118, 'gamma': 1.7960847528705854, 'reg_alpha': 5.746775499181867, 'reg_lambda': 0.45580746840273345}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:19:09,733] Trial 5 finished with value: 0.6818573511580819 and parameters: {'learning_rate': 0.0056828375585122656, 'n_estimators': 1000, 'max_depth': 8, 'min_child_weight': 2.5358333235759627, 'subsample': 0.6488152939379115, 'colsample_bytree': 0.798070764044508, 'gamma': 1.9913367728263115e-08, 'reg_alpha': 2.318690670290199, 'reg_lambda': 6.478282331897332e-05}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:19:11,261] Trial 6 finished with value: 0.6876376985441253 and parameters: {'learning_rate': 0.043767126303409544, 'n_estimators': 1500, 'max_depth': 7, 'min_child_weight': 3.7569262495760847, 'subsample': 0.6739417822102108, 'colsample_bytree': 0.9878338511058234, 'gamma': 0.05531681668096113, 'reg_alpha': 3.7713131110779936, 'reg_lambda': 1.8356566544355097}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:19:14,367] Trial 7 finished with value: 0.6921430236631655 and parameters: {'learning_rate': 0.03027406546455652, 'n_estimators': 2850, 'max_depth': 3, 'min_child_weight': 1.0302587393796305, 'subsample': 0.6180909155642152, 'colsample_bytree': 0.7301321323053057, 'gamma': 2.4048726561760165e-05, 'reg_alpha': 7.933105363733024e-05, 'reg_lambda': 0.6326486185661588}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:19:17,402] Trial 8 finished with value: 0.6878849334398204 and parameters: {'learning_rate': 0.0076510536667541975, 'n_estimators': 1400, 'max_depth': 7, 'min_child_weight': 0.8408897660399112, 'subsample': 0.9208787923016158, 'colsample_bytree': 0.6298202574719083, 'gamma': 3.845031120156871, 'reg_alpha': 0.2545150013091294, 'reg_lambda': 2.4604229580184192e-05}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:19:25,949] Trial 9 finished with value: 0.6766338851359995 and parameters: {'learning_rate': 0.0010319982330247674, 'n_estimators': 2600, 'max_depth': 8, 'min_child_weight': 7.360091638366141, 'subsample': 0.9085081386743783, 'colsample_bytree': 0.6296178606936361, 'gamma': 1.3130541002425655e-05, 'reg_alpha': 6.472669269538641e-06, 'reg_lambda': 1.100839441018132}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:19:28,551] Trial 10 finished with value: 0.6933517445515558 and parameters: {'learning_rate': 0.038760760308848, 'n_estimators': 2550, 'max_depth': 3, 'min_child_weight': 0.9842019745974843, 'subsample': 0.8839650275298712, 'colsample_bytree': 0.7174338180617875, 'gamma': 1.5454663339895539e-06, 'reg_alpha': 0.16925124121843918, 'reg_lambda': 0.6313134808513103}. Best is trial 0 with value: 0.6962975223192452.\n",
            "[I 2025-12-09 18:19:32,613] Trial 11 finished with value: 0.6970040514058609 and parameters: {'learning_rate': 0.02089375893859898, 'n_estimators': 2400, 'max_depth': 3, 'min_child_weight': 2.632210547538243, 'subsample': 0.8748999591202042, 'colsample_bytree': 0.7047939034355872, 'gamma': 4.927785088148004e-06, 'reg_alpha': 0.04616305361092931, 'reg_lambda': 1.5103208718665497}. Best is trial 11 with value: 0.6970040514058609.\n",
            "[I 2025-12-09 18:19:36,591] Trial 12 finished with value: 0.6971735221513646 and parameters: {'learning_rate': 0.0138212197636635, 'n_estimators': 2050, 'max_depth': 4, 'min_child_weight': 0.8149446034987896, 'subsample': 0.8854268158051167, 'colsample_bytree': 0.6909007858803774, 'gamma': 9.72894078750237e-07, 'reg_alpha': 1.6473776934011615e-05, 'reg_lambda': 0.020631524279200154}. Best is trial 12 with value: 0.6971735221513646.\n",
            "[I 2025-12-09 18:19:40,936] Trial 13 finished with value: 0.6937753413756207 and parameters: {'learning_rate': 0.01519987976345532, 'n_estimators': 1650, 'max_depth': 5, 'min_child_weight': 1.0477924631551068, 'subsample': 0.9227846655273051, 'colsample_bytree': 0.8180816644686575, 'gamma': 7.700459445504702e-06, 'reg_alpha': 9.542970776786888e-06, 'reg_lambda': 0.0015495143967160412}. Best is trial 12 with value: 0.6971735221513646.\n",
            "[I 2025-12-09 18:19:41,540] Trial 14 finished with value: 0.6514413665885422 and parameters: {'learning_rate': 0.0031049708570687136, 'n_estimators': 1550, 'max_depth': 4, 'min_child_weight': 0.5868225256395131, 'subsample': 0.7546241705077186, 'colsample_bytree': 0.712795064424257, 'gamma': 2.708061430451299e-06, 'reg_alpha': 8.950290286378726e-06, 'reg_lambda': 0.087330094696101}. Best is trial 12 with value: 0.6971735221513646.\n",
            "[I 2025-12-09 18:19:46,756] Trial 15 finished with value: 0.6903881233157382 and parameters: {'learning_rate': 0.01905653732256378, 'n_estimators': 2100, 'max_depth': 5, 'min_child_weight': 7.255862891143882, 'subsample': 0.8511524670781316, 'colsample_bytree': 0.7012140015930832, 'gamma': 0.00044869075504893105, 'reg_alpha': 5.5983360099233765, 'reg_lambda': 0.07638762588303766}. Best is trial 12 with value: 0.6971735221513646.\n",
            "[I 2025-12-09 18:19:51,990] Trial 16 finished with value: 0.6682655888316245 and parameters: {'learning_rate': 0.0015450199932584095, 'n_estimators': 1750, 'max_depth': 5, 'min_child_weight': 1.458113137542868, 'subsample': 0.8152369618398595, 'colsample_bytree': 0.6164005522563454, 'gamma': 5.370385722136872e-06, 'reg_alpha': 0.04285806968322117, 'reg_lambda': 2.2949386874442412}. Best is trial 12 with value: 0.6971735221513646.\n",
            "[I 2025-12-09 18:19:52,965] Trial 17 finished with value: 0.6873925279320899 and parameters: {'learning_rate': 0.13004932385230458, 'n_estimators': 1650, 'max_depth': 3, 'min_child_weight': 0.6198181676296357, 'subsample': 0.8902480036877934, 'colsample_bytree': 0.6178650595646226, 'gamma': 6.321925725781451e-08, 'reg_alpha': 0.0004388171373024864, 'reg_lambda': 0.000662548519992666}. Best is trial 12 with value: 0.6971735221513646.\n",
            "[I 2025-12-09 18:20:00,367] Trial 18 finished with value: 0.6817193578047139 and parameters: {'learning_rate': 0.001990207065193698, 'n_estimators': 2400, 'max_depth': 7, 'min_child_weight': 0.5650225592140768, 'subsample': 0.9163455510903953, 'colsample_bytree': 0.6247329659311983, 'gamma': 3.7677249539837204e-07, 'reg_alpha': 1.1050633403193854e-05, 'reg_lambda': 8.087230748410267e-05}. Best is trial 12 with value: 0.6971735221513646.\n",
            "[I 2025-12-09 18:20:01,739] Trial 19 finished with value: 0.6957292298305678 and parameters: {'learning_rate': 0.03676011527210172, 'n_estimators': 2700, 'max_depth': 5, 'min_child_weight': 5.691460618592223, 'subsample': 0.7671804362347427, 'colsample_bytree': 0.7810231028921222, 'gamma': 9.625046406183527e-07, 'reg_alpha': 0.07004467027377127, 'reg_lambda': 1.1300394815284514}. Best is trial 12 with value: 0.6971735221513646.\n",
            "[I 2025-12-09 18:20:05,661] Trial 20 finished with value: 0.69493699252079 and parameters: {'learning_rate': 0.009442472385252577, 'n_estimators': 1450, 'max_depth': 5, 'min_child_weight': 4.928223168331497, 'subsample': 0.9103987765453242, 'colsample_bytree': 0.7874577881016902, 'gamma': 2.9511242973892946e-06, 'reg_alpha': 0.00011644239682348432, 'reg_lambda': 3.1123872787211257}. Best is trial 12 with value: 0.6971735221513646.\n",
            "[I 2025-12-09 18:20:10,457] Trial 21 finished with value: 0.6982587963425562 and parameters: {'learning_rate': 0.013570972114707725, 'n_estimators': 2450, 'max_depth': 4, 'min_child_weight': 1.0024202070808959, 'subsample': 0.8895969662409436, 'colsample_bytree': 0.6179844014785916, 'gamma': 0.005567832464942176, 'reg_alpha': 5.0781078798822983e-05, 'reg_lambda': 0.24267702102911723}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:12,743] Trial 22 finished with value: 0.6919869783841779 and parameters: {'learning_rate': 0.024859797863502165, 'n_estimators': 3000, 'max_depth': 6, 'min_child_weight': 0.9392004264301084, 'subsample': 0.9020727033654958, 'colsample_bytree': 0.7256063686885158, 'gamma': 0.011852190680951651, 'reg_alpha': 3.1171436623813046e-06, 'reg_lambda': 0.5292543602823725}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:16,935] Trial 23 finished with value: 0.6913877488949577 and parameters: {'learning_rate': 0.011556393501501123, 'n_estimators': 2350, 'max_depth': 3, 'min_child_weight': 1.6309635014844663, 'subsample': 0.9664969110906851, 'colsample_bytree': 0.6537315719781782, 'gamma': 0.00514652793057072, 'reg_alpha': 9.998245831310838e-06, 'reg_lambda': 0.005759223863956437}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:19,596] Trial 24 finished with value: 0.6945697051650164 and parameters: {'learning_rate': 0.021143550528273453, 'n_estimators': 2250, 'max_depth': 3, 'min_child_weight': 2.614974001903365, 'subsample': 0.8163921856364715, 'colsample_bytree': 0.7245778700371395, 'gamma': 0.014735719486343054, 'reg_alpha': 0.0011040487920488645, 'reg_lambda': 0.7374526436075931}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:21,460] Trial 25 finished with value: 0.6924052615447713 and parameters: {'learning_rate': 0.03185087916587406, 'n_estimators': 2150, 'max_depth': 5, 'min_child_weight': 0.6769812576213815, 'subsample': 0.875011946240383, 'colsample_bytree': 0.6112171925056857, 'gamma': 0.013537072657598858, 'reg_alpha': 0.00026347901857988405, 'reg_lambda': 1.5429201082870914}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:23,050] Trial 26 finished with value: 0.6913484851847371 and parameters: {'learning_rate': 0.03372397950930264, 'n_estimators': 2050, 'max_depth': 5, 'min_child_weight': 3.7338535391113705, 'subsample': 0.7900273023197848, 'colsample_bytree': 0.6128022527580358, 'gamma': 2.641709669595748e-06, 'reg_alpha': 9.520985753680546e-06, 'reg_lambda': 0.01437156201652356}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:28,352] Trial 27 finished with value: 0.6925930579728903 and parameters: {'learning_rate': 0.004971832421410964, 'n_estimators': 2350, 'max_depth': 5, 'min_child_weight': 1.3994919320986536, 'subsample': 0.9866459031618572, 'colsample_bytree': 0.7414954814087882, 'gamma': 4.2307498902163334e-08, 'reg_alpha': 0.00010369685508692242, 'reg_lambda': 0.02543212797878614}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:33,087] Trial 28 finished with value: 0.6947043288275936 and parameters: {'learning_rate': 0.01375970331751477, 'n_estimators': 2350, 'max_depth': 3, 'min_child_weight': 3.065194416614793, 'subsample': 0.8974976952741088, 'colsample_bytree': 0.7340234098086331, 'gamma': 7.051152754777668e-06, 'reg_alpha': 0.02029571853396512, 'reg_lambda': 0.858959633179205}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:35,281] Trial 29 finished with value: 0.6961704829214248 and parameters: {'learning_rate': 0.030032506743454965, 'n_estimators': 1950, 'max_depth': 4, 'min_child_weight': 0.6228939556115503, 'subsample': 0.8881106026424296, 'colsample_bytree': 0.7173194980433776, 'gamma': 1.9397380014965222e-07, 'reg_alpha': 2.2787661915642157e-05, 'reg_lambda': 0.013603356802709073}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:38,601] Trial 30 finished with value: 0.6944819802650138 and parameters: {'learning_rate': 0.030126734113170912, 'n_estimators': 2550, 'max_depth': 3, 'min_child_weight': 0.6085584271628467, 'subsample': 0.8643668672968977, 'colsample_bytree': 0.7267626635152488, 'gamma': 0.04731722742789661, 'reg_alpha': 0.004659309509564332, 'reg_lambda': 7.637100907276255e-06}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:39,502] Trial 31 finished with value: 0.6977894978217721 and parameters: {'learning_rate': 0.09517354898484005, 'n_estimators': 2550, 'max_depth': 4, 'min_child_weight': 0.519029329177843, 'subsample': 0.973538742653246, 'colsample_bytree': 0.7218034815900292, 'gamma': 1.5757579979061067e-06, 'reg_alpha': 0.00010692371668772797, 'reg_lambda': 2.2832706904876603}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:40,052] Trial 32 finished with value: 0.6795314321764235 and parameters: {'learning_rate': 0.27424225470318214, 'n_estimators': 2700, 'max_depth': 4, 'min_child_weight': 1.5017515810419875, 'subsample': 0.8412709399321342, 'colsample_bytree': 0.8134162490278134, 'gamma': 4.290475458543202e-07, 'reg_alpha': 7.036627583827675e-05, 'reg_lambda': 2.000667211660784}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:40,816] Trial 33 finished with value: 0.6974347604219936 and parameters: {'learning_rate': 0.11560034782380502, 'n_estimators': 2800, 'max_depth': 5, 'min_child_weight': 0.5116983426094259, 'subsample': 0.9799808022740185, 'colsample_bytree': 0.6827309717088533, 'gamma': 8.11753674377388e-06, 'reg_alpha': 0.0008915493222496732, 'reg_lambda': 0.028158269692155846}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:41,572] Trial 34 finished with value: 0.6842387042032025 and parameters: {'learning_rate': 0.19882748868765474, 'n_estimators': 2400, 'max_depth': 7, 'min_child_weight': 0.9173556426883173, 'subsample': 0.9436687731664813, 'colsample_bytree': 0.6747846259731066, 'gamma': 2.742163302700198e-06, 'reg_alpha': 0.0035381255062874877, 'reg_lambda': 0.2854778620661761}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:42,700] Trial 35 finished with value: 0.695181043393784 and parameters: {'learning_rate': 0.0434039455458159, 'n_estimators': 2900, 'max_depth': 5, 'min_child_weight': 0.5457341654146447, 'subsample': 0.9900346568249501, 'colsample_bytree': 0.7684061684399253, 'gamma': 1.7771084607139166e-05, 'reg_alpha': 0.20549057537980753, 'reg_lambda': 0.007880262449363193}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:43,950] Trial 36 finished with value: 0.6932484811056335 and parameters: {'learning_rate': 0.0649995108422955, 'n_estimators': 2850, 'max_depth': 4, 'min_child_weight': 0.5174230196797662, 'subsample': 0.978992042830175, 'colsample_bytree': 0.6689760921784921, 'gamma': 3.087029963792386e-05, 'reg_alpha': 2.3727841057678125e-05, 'reg_lambda': 0.6720025280457625}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:44,554] Trial 37 finished with value: 0.693463170005836 and parameters: {'learning_rate': 0.16111300988376123, 'n_estimators': 2900, 'max_depth': 5, 'min_child_weight': 0.6903276372817634, 'subsample': 0.9787403579965042, 'colsample_bytree': 0.7432574435669602, 'gamma': 1.5117220375393094e-05, 'reg_alpha': 2.0826927439133416e-05, 'reg_lambda': 0.001063275493994168}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:45,841] Trial 38 finished with value: 0.6964207310370601 and parameters: {'learning_rate': 0.06777685816871255, 'n_estimators': 2250, 'max_depth': 3, 'min_child_weight': 0.6682107583142813, 'subsample': 0.9368400052268582, 'colsample_bytree': 0.6879761330116377, 'gamma': 4.799427795417538e-07, 'reg_alpha': 0.0003600560646762229, 'reg_lambda': 1.5990924262430637}. Best is trial 21 with value: 0.6982587963425562.\n",
            "[I 2025-12-09 18:20:46,594] Trial 39 finished with value: 0.6925254787858618 and parameters: {'learning_rate': 0.09734924237644511, 'n_estimators': 2250, 'max_depth': 5, 'min_child_weight': 0.6736129807959751, 'subsample': 0.9989707192893464, 'colsample_bytree': 0.7962072662432402, 'gamma': 2.432599613045369e-07, 'reg_alpha': 3.801632232247569e-05, 'reg_lambda': 8.085870277448572}. Best is trial 21 with value: 0.6982587963425562.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OPTUNA] Mejor AP(val): 0.698259\n",
            "[OPTUNA] Params ganadores: {'learning_rate': 0.013570972114707725, 'n_estimators': 2450, 'max_depth': 4, 'min_child_weight': 1.0024202070808959, 'subsample': 0.8895969662409436, 'colsample_bytree': 0.6179844014785916, 'gamma': 0.005567832464942176, 'reg_alpha': 5.0781078798822983e-05, 'reg_lambda': 0.24267702102911723}\n",
            "[OPTUNA] best_iteration (del trial): 1962\n",
            "[OPTUNA] Guardado BEST en: BEST_XGB_FULL_SMOTENC.json\n",
            "[OPTUNA] Reentreno final completado. best_iteration = 1962\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "tuned_model = None\n",
        "N_TRIALS = 40\n",
        "STUDY_NAME = f\"XGB_{VIEW_TAG}_{BAL_TAG}_AP\"\n",
        "SAMPLER = TPESampler(seed=RANDOM_STATE, multivariate=True, group=False)\n",
        "study = optuna.create_study(direction=\"maximize\", study_name=STUDY_NAME, sampler=SAMPLER)\n",
        "\n",
        "SEARCH_KEYS = [\n",
        "    \"learning_rate\", \"n_estimators\", \"max_depth\", \"min_child_weight\",\n",
        "    \"subsample\", \"colsample_bytree\", \"gamma\", \"reg_alpha\", \"reg_lambda\"\n",
        "]\n",
        "\n",
        "def suggest_xgb_params(trial):\n",
        "    p = {}\n",
        "    p[\"learning_rate\"]    = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
        "    p[\"n_estimators\"]     = trial.suggest_int(\"n_estimators\", 800, 3000, step=50)\n",
        "    p[\"max_depth\"]        = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    p[\"min_child_weight\"] = trial.suggest_float(\"min_child_weight\", 0.5, 20.0, log=True)\n",
        "    p[\"subsample\"]        = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
        "    p[\"colsample_bytree\"] = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
        "    p[\"gamma\"]            = trial.suggest_float(\"gamma\", 1e-8, 5.0, log=True)\n",
        "    p[\"reg_alpha\"]        = trial.suggest_float(\"reg_alpha\", 1e-6, 10.0, log=True)\n",
        "    p[\"reg_lambda\"]       = trial.suggest_float(\"reg_lambda\", 1e-6, 10.0, log=True)\n",
        "    p[\"random_state\"]     = RANDOM_STATE\n",
        "    p[\"n_jobs\"]           = -1\n",
        "    p[\"eval_metric\"]      = \"aucpr\"\n",
        "    p[\"tree_method\"]      = \"hist\"\n",
        "    p[\"verbosity\"]        = 0\n",
        "    return p\n",
        "\n",
        "# Warm-start\n",
        "if BEST_HP_FILE.exists():\n",
        "    try:\n",
        "        prev = json.loads(BEST_HP_FILE.read_text())\n",
        "        warm = {k: prev[k] for k in SEARCH_KEYS if k in prev}\n",
        "        if warm:\n",
        "            print(\"[OPTUNA] Enqueuing previous BEST as a trial seed.\")\n",
        "            study.enqueue_trial(warm)\n",
        "    except Exception as e:\n",
        "        print(\"[OPTUNA] Aviso: no se pudo usar BEST para warm-start:\", e)\n",
        "\n",
        "def objective(trial):\n",
        "    hp = suggest_xgb_params(trial)\n",
        "    mdl = XGBClassifier(**{**seed_params, **hp})\n",
        "    mdl, best_it = xgb_fit_with_es(\n",
        "        mdl,\n",
        "        X_train_final, y_train_final,\n",
        "        X_val_fit, y_val,\n",
        "        feature_names=feature_names_used,\n",
        "        rounds=200,\n",
        "        verbose=False\n",
        "    )\n",
        "    proba_val_t = mdl.predict_proba(X_val_fit)[:, 1]\n",
        "    ap = average_precision_score(y_val, proba_val_t)\n",
        "    trial.set_user_attr(\"best_iteration\", best_it)\n",
        "    return ap\n",
        "\n",
        "print(f\"[OPTUNA] Iniciando estudio '{STUDY_NAME}' con {N_TRIALS} pruebas...\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
        "\n",
        "best = study.best_trial\n",
        "print(f\"[OPTUNA] Mejor AP(val): {best.value:.6f}\")\n",
        "print(f\"[OPTUNA] Params ganadores:\", best.params)\n",
        "print(f\"[OPTUNA] best_iteration (del trial):\", best.user_attrs.get(\"best_iteration\"))\n",
        "\n",
        "best_params = dict(best.params)\n",
        "best_params.update({\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "    \"n_jobs\": -1,\n",
        "    \"eval_metric\": \"aucpr\",\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"verbosity\": 0\n",
        "})\n",
        "with open(BEST_HP_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
        "print(\"[OPTUNA] Guardado BEST en:\", BEST_HP_FILE.name)\n",
        "\n",
        "tuned_model = XGBClassifier(**best_params)\n",
        "tuned_model, best_it = xgb_fit_with_es(\n",
        "    tuned_model,\n",
        "    X_train_final, y_train_final,\n",
        "    X_val_fit, y_val,\n",
        "    feature_names=feature_names_used,\n",
        "    rounds=200,\n",
        "    verbose=False\n",
        ")\n",
        "print(\"[OPTUNA] Reentreno final completado. best_iteration =\", best_it)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8797a95",
      "metadata": {},
      "source": [
        "8 — Cross-Validation (OOF) para baseline y tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efb9478a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[CV-BASELINE] Guardados: cv_summary_XGB_FULL_SMOTENC_BASELINE_CV5.csv | oof_XGB_FULL_SMOTENC_BASELINE_CV5.parquet\n",
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
            "[CV-TUNED] Guardados: cv_summary_XGB_FULL_SMOTENC_TUNED_CV5.csv | oof_XGB_FULL_SMOTENC_TUNED_CV5.parquet\n"
          ]
        }
      ],
      "source": [
        "def run_oof_cv_xgb(model_params, X, y, k_folds=CV_FOLDS, seed=RANDOM_STATE, exp_suffix=\"BASELINE\"):\n",
        "    skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
        "    oof_proba = np.zeros_like(y, dtype=float)\n",
        "    fold_rows = []\n",
        "\n",
        "    base = dict(model_params)\n",
        "  \n",
        "    base.pop(\"verbose\", None)\n",
        "    base.setdefault(\"verbosity\", 0)\n",
        "    base.setdefault(\"eval_metric\", \"aucpr\")\n",
        "    base.setdefault(\"tree_method\", \"hist\")\n",
        "    base.setdefault(\"random_state\", seed)\n",
        "    base.setdefault(\"n_jobs\", -1)\n",
        "    base[\"n_estimators\"] = base.get(\"n_estimators\") or 1000\n",
        "\n",
        "    for f, (tr_idx, va_idx) in enumerate(skf.split(X, y), 1):\n",
        "        X_tr0, X_va0 = X[tr_idx], X[va_idx]\n",
        "        y_tr0, y_va0 = y[tr_idx], y[va_idx]\n",
        "\n",
        "\n",
        "        keep_idx = None\n",
        "        feat_names_fold = feature_names\n",
        "        if USE_REDUCED:\n",
        "            keep_idx, _ = fit_mi_selector(X_tr0, y_tr0, topk=MI_TOPK, seed=seed)\n",
        "            X_tr0 = apply_keep_idx(X_tr0, keep_idx)\n",
        "            X_va0 = apply_keep_idx(X_va0, keep_idx)\n",
        "            feat_names_fold = [feature_names[i] for i in keep_idx]\n",
        "\n",
        "        \n",
        "        if BALANCE_IN_CV and USE_BALANCED_TRAIN:\n",
        "            X_tr, y_tr = maybe_smote(X_tr0, y_tr0, keep_idx=keep_idx)\n",
        "        else:\n",
        "            X_tr, y_tr = X_tr0, y_tr0\n",
        "\n",
        "   \n",
        "        mdl = XGBClassifier(**base)\n",
        "        adapter, best_it = xgb_fit_with_es(\n",
        "            mdl, X_tr, y_tr, X_va0, y_va0,\n",
        "            feature_names=feat_names_fold,\n",
        "            rounds=200, verbose=False\n",
        "        )\n",
        "\n",
        "       \n",
        "        proba_va = adapter.predict_proba(X_va0)[:, 1]\n",
        "        oof_proba[va_idx] = proba_va\n",
        "\n",
        "        fold_rows.append({\n",
        "            \"fold\": f,\n",
        "            \"pr_auc\": average_precision_score(y_va0, proba_va),\n",
        "            \"roc_auc\": roc_auc_score(y_va0, proba_va),\n",
        "            \"best_iteration\": best_it if best_it is not None else np.nan\n",
        "        })\n",
        "\n",
        "  \n",
        "    oof_pr = average_precision_score(y, oof_proba)\n",
        "    oof_roc = roc_auc_score(y, oof_proba)\n",
        "    thr_oof, _ = find_best_threshold(y, oof_proba, metric=\"f1\")\n",
        "    y_oof_pred = (oof_proba >= thr_oof).astype(int)\n",
        "    oof_f1  = f1_score(y, y_oof_pred, zero_division=0)\n",
        "    oof_rec = recall_score(y, y_oof_pred, zero_division=0)\n",
        "    oof_bal = balanced_accuracy_score(y, y_oof_pred)\n",
        "\n",
        "    cv_tag = f\"{EXP_NAME}_{exp_suffix}_CV{CV_FOLDS}\"\n",
        "    cv_csv = OUT_RESULTS / f\"cv_summary_{cv_tag}.csv\"\n",
        "    folds_df = pd.DataFrame(fold_rows)\n",
        "    agg_row = pd.DataFrame([{\n",
        "        \"fold\": \"OOF\", \"pr_auc\": oof_pr, \"roc_auc\": oof_roc,\n",
        "        \"thr\": thr_oof, \"f1\": oof_f1, \"recall\": oof_rec, \"bal_acc\": oof_bal\n",
        "    }])\n",
        "    pd.concat([folds_df, agg_row], ignore_index=True).to_csv(cv_csv, index=False)\n",
        "\n",
        "    oof_path = OUT_PREDS / f\"oof_{cv_tag}.parquet\"\n",
        "    pd.DataFrame({\"oof_proba\": oof_proba, \"y_true\": y}).to_parquet(oof_path, index=False)\n",
        "\n",
        "    print(f\"[CV-{exp_suffix}] Guardados: {cv_csv.name} | {oof_path.name}\")\n",
        "    return {\"oof_pr_auc\": oof_pr, \"oof_roc_auc\": oof_roc, \"thr\": thr_oof,\n",
        "            \"oof_f1\": oof_f1, \"oof_recall\": oof_rec, \"oof_bal_acc\": oof_bal}\n",
        "\n",
        "\n",
        "cv_baseline = None\n",
        "cv_tuned = None\n",
        "\n",
        "if DO_CV_BASELINE:\n",
        "    cv_baseline = run_oof_cv_xgb(seed_params, X_train_fit, y_train, exp_suffix=\"BASELINE\")\n",
        "\n",
        "if DO_CV_TUNED and tuned_model is not None:\n",
        "   \n",
        "    cv_tuned = run_oof_cv_xgb(best_params, X_train_fit, y_train, exp_suffix=\"TUNED\")\n",
        "\n",
        "if lgbm_tuned is not None:\n",
        "\n",
        "    (OUT_PARAMS_L / f\"{EXP_NAME_LGB}_TUNED_fitted_params.json\").write_text(\n",
        "        json.dumps(\n",
        "            {\n",
        "                \"best_iteration\": getattr(lgbm_tuned, \"best_iteration\", None),\n",
        "                **(lgbm_best_params or {})\n",
        "            },\n",
        "            indent=2,\n",
        "            ensure_ascii=False\n",
        "        ),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    proba_test_tl = lgbm_tuned.predict_proba(X_test_fit)[:,1]\n",
        "    y_pred_test_tl = (proba_test_tl >= thr_val_tl).astype(int)\n",
        "    val_metrics_tl = compute_all_metrics(y_val,  proba_val_tl, thr_val_tl)\n",
        "    test_metrics_tl= compute_all_metrics(y_test, proba_test_tl, thr_val_tl)\n",
        "\n",
        "    row_tuned_lgb = {\n",
        "        \"model\": f\"{EXP_NAME_LGB}_TUNED\",\n",
        "        \"thr_val\": float(thr_val_tl),\n",
        "        \"thr_oof\": float(\"nan\"),\n",
        "        \"thr_used\": float(thr_val_tl),\n",
        "        \"val_pr_auc\": val_metrics_tl[\"pr_auc\"],\n",
        "        \"val_roc_auc\": val_metrics_tl[\"roc_auc\"],\n",
        "        \"val_precision\": val_metrics_tl[\"precision\"],\n",
        "        \"val_f1\": val_metrics_tl[\"f1\"],\n",
        "        \"val_recall\": val_metrics_tl[\"recall\"],\n",
        "        \"val_bal_acc\": val_metrics_tl[\"bal_acc\"],\n",
        "        \"test_pr_auc\": test_metrics_tl[\"pr_auc\"],\n",
        "        \"test_roc_auc\": test_metrics_tl[\"roc_auc\"],\n",
        "        \"test_precision\": test_metrics_tl[\"precision\"],\n",
        "        \"test_f1\": test_metrics_tl[\"f1\"],\n",
        "        \"test_recall\": test_metrics_tl[\"recall\"],\n",
        "        \"test_bal_acc\": test_metrics_tl[\"bal_acc\"],\n",
        "        \"best_iteration\": getattr(lgbm_tuned, \"best_iteration\", float(\"nan\"))\n",
        "    }\n",
        "    csv_l = OUT_RESULTS_L / \"baselines.csv\"\n",
        "    pd.DataFrame([row_tuned_lgb]).to_csv(\n",
        "        csv_l, mode=(\"a\" if csv_l.exists() else \"w\"), index=False, header=not csv_l.exists()\n",
        "    )\n",
        "\n",
        "    print(f\"[OK][LGBM TUNED] Guardados en {bl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15dae995",
      "metadata": {},
      "source": [
        "9 — Evaluación en test + guardados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "983f541f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK][BASE] Guardados: \n",
            "  - Seed HPs   : XGB_FULL_SMOTENC_BASE_seed_params.json \n",
            "  - Fitted HPs : XGB_FULL_SMOTENC_BASE_fitted_params.json \n",
            "  - Importancias: XGB_FULL_SMOTENC_feature_importances.csv \n",
            "  - Preds test  : preds_test_XGB_FULL_SMOTENC.parquet \n",
            "  - Baselines   : baselines.csv\n",
            "[OK][TUNED] Guardados: \n",
            "  - Fitted HPs : XGB_FULL_SMOTENC_TUNED_fitted_params.json \n",
            "  - Importancias: XGB_FULL_SMOTENC_TUNED_feature_importances.csv \n",
            "  - Preds test  : preds_test_XGB_FULL_SMOTENC_TUNED.parquet \n",
            "  - Baselines   : baselines.csv\n"
          ]
        }
      ],
      "source": [
        "def _ensure_probs():\n",
        "    # XGB: usar TUNED si existe; si no, BASELINE\n",
        "    p_val_xgb  = locals().get(\"proba_val_tx\",  None)\n",
        "    p_test_xgb = locals().get(\"proba_test_tx\", None)\n",
        "    if p_val_xgb is None or p_test_xgb is None:\n",
        "        p_val_xgb  = locals().get(\"proba_val_xgb\",  None)\n",
        "        p_test_xgb = locals().get(\"proba_test_xgb\", None)\n",
        "\n",
        "    # LGBM: usar TUNED si existe; si no, BASELINE\n",
        "    p_val_lgb  = locals().get(\"proba_val_tl\",  None)\n",
        "    p_test_lgb = locals().get(\"proba_test_tl\", None)\n",
        "    if p_val_lgb is None or p_test_lgb is None:\n",
        "        p_val_lgb  = locals().get(\"proba_val_lgb\",  None)\n",
        "        p_test_lgb = locals().get(\"proba_test_lgb\", None)\n",
        "\n",
        "    if any(v is None for v in [p_val_xgb, p_test_xgb, p_val_lgb, p_test_lgb]):\n",
        "        raise RuntimeError(\"No se encontraron todas las probabilidades requeridas (XGB y LGBM val/test).\")\n",
        "\n",
        "    return p_val_xgb, p_test_xgb, p_val_lgb, p_test_lgb\n",
        "\n",
        "def _logit(p, eps=1e-9):\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    return np.log(p / (1 - p))\n",
        "\n",
        "def _sigmoid(z):\n",
        "    return 1. / (1. + np.exp(-z))\n",
        "\n",
        "def ens_probs(p1, p2, w=0.5, method=\"logit\"):\n",
        "    if method == \"logit\":\n",
        "        z = w * _logit(p1) + (1 - w) * _logit(p2)\n",
        "        return _sigmoid(z)\n",
        "    elif method == \"prob\":\n",
        "        return w * p1 + (1 - w) * p2\n",
        "    else:\n",
        "        raise ValueError(\"method debe ser 'logit' o 'prob'.\")\n",
        "\n",
        "# === Preparación ENS ===\n",
        "be = DIRS[\"ens\"]\n",
        "OUT_RESULTS_E = be / \"results\"\n",
        "OUT_FIGS_E    = be / \"figs\"\n",
        "OUT_PREDS_E   = be / \"preds\"\n",
        "OUT_PARAMS_E  = be / \"best_params\"\n",
        "\n",
        "EXP_NAME_ENS = f\"ENS_{VIEW_TAG}_{BAL_TAG}\"\n",
        "\n",
        "p_val_xgb, p_test_xgb, p_val_lgb, p_test_lgb = _ensure_probs()\n",
        "\n",
        "# === Búsqueda de w para maximizar AP en validación ===\n",
        "best = {\"w\": None, \"ap\": -1.0, \"method\": None}\n",
        "for method in [\"logit\", \"prob\"]:\n",
        "    for w in np.linspace(0.0, 1.0, 41):\n",
        "        p_val_ens = ens_probs(p_val_xgb, p_val_lgb, w=w, method=method)\n",
        "        ap = average_precision_score(y_val, p_val_ens)\n",
        "        if ap > best[\"ap\"]:\n",
        "            best = {\"w\": float(w), \"ap\": float(ap), \"method\": method}\n",
        "\n",
        "# Aplicar peso óptimo a test\n",
        "p_val_ens  = ens_probs(p_val_xgb,  p_val_lgb,  w=best[\"w\"], method=best[\"method\"])\n",
        "p_test_ens = ens_probs(p_test_xgb, p_test_lgb, w=best[\"w\"], method=best[\"method\"])\n",
        "\n",
        "# Umbral por F1 en validación y métricas\n",
        "thr_val_ens, _   = find_best_threshold(y_val, p_val_ens, metric=\"f1\")\n",
        "y_pred_test_ens  = (p_test_ens >= thr_val_ens).astype(int)\n",
        "val_metrics_ens  = compute_all_metrics(y_val,  p_val_ens,  thr_val_ens)\n",
        "test_metrics_ens = compute_all_metrics(y_test, p_test_ens, thr_val_ens)\n",
        "\n",
        "# Guardados\n",
        "with open(OUT_PARAMS_E / f\"{EXP_NAME_ENS}_weights.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"w_xgb\": best[\"w\"], \"w_lgbm\": 1 - best[\"w\"], \"method\": best[\"method\"], \"val_ap\": best[\"ap\"]},\n",
        "              f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Plots\n",
        "plot_pr_curve(y_val,  p_val_ens,  f\"{EXP_NAME_ENS} — PR (val)\",  OUT_FIGS_E / f\"{EXP_NAME_ENS}_pr_val.png\")\n",
        "plot_pr_curve(y_test, p_test_ens, f\"{EXP_NAME_ENS} — PR (test)\", OUT_FIGS_E / f\"{EXP_NAME_ENS}_pr_test.png\")\n",
        "plot_roc_curve(y_val,  p_val_ens,  f\"{EXP_NAME_ENS} — ROC (val)\",  OUT_FIGS_E / f\"{EXP_NAME_ENS}_roc_val.png\")\n",
        "plot_roc_curve(y_test, p_test_ens, f\"{EXP_NAME_ENS} — ROC (test)\", OUT_FIGS_E / f\"{EXP_NAME_ENS}_roc_test.png\")\n",
        "\n",
        "# Preds test\n",
        "pd.DataFrame({\"proba\": p_test_ens, \"y_true\": y_test}).to_parquet(\n",
        "    OUT_PREDS_E / f\"preds_test_{EXP_NAME_ENS}.parquet\", index=False\n",
        ")\n",
        "\n",
        "row_ens = {\n",
        "    \"model\": EXP_NAME_ENS,\n",
        "    \"thr_val\": float(thr_val_ens),\n",
        "    \"thr_oof\": float(\"nan\"),\n",
        "    \"thr_used\": float(thr_val_ens),\n",
        "    \"val_pr_auc\": val_metrics_ens[\"pr_auc\"],\n",
        "    \"val_roc_auc\": val_metrics_ens[\"roc_auc\"],\n",
        "    \"val_precision\": val_metrics_ens[\"precision\"],\n",
        "    \"val_f1\": val_metrics_ens[\"f1\"],\n",
        "    \"val_recall\": val_metrics_ens[\"recall\"],\n",
        "    \"val_bal_acc\": val_metrics_ens[\"bal_acc\"],\n",
        "    \"test_pr_auc\": test_metrics_ens[\"pr_auc\"],\n",
        "    \"test_roc_auc\": test_metrics_ens[\"roc_auc\"],\n",
        "    \"test_precision\": test_metrics_ens[\"precision\"],\n",
        "    \"test_f1\": test_metrics_ens[\"f1\"],\n",
        "    \"test_recall\": test_metrics_ens[\"recall\"],\n",
        "    \"test_bal_acc\": test_metrics_ens[\"bal_acc\"],\n",
        "    \"w_xgb\": best[\"w\"],\n",
        "    \"w_lgbm\": 1 - best[\"w\"],\n",
        "    \"comb_method\": best[\"method\"]\n",
        "}\n",
        "csv_e = OUT_RESULTS_E / \"baselines.csv\"\n",
        "pd.DataFrame([row_ens]).to_csv(csv_e, mode=(\"a\" if csv_e.exists() else \"w\"),\n",
        "                               index=False, header=not csv_e.exists())\n",
        "\n",
        "print(f\"[OK][ENS] w*XGB + (1-w)*LGBM con w={best['w']:.3f} (method={best['method']}) | AP(val)={best['ap']:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "928335ce",
      "metadata": {},
      "source": [
        "10 — Mejores resultados + resumen CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a837bea0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MEJORES EN TEST (por métrica) ===\n",
            "- test_pr_auc: XGB_FULL_SMOTENC | PR-AUC=0.7055 | ROC-AUC=0.8588 | F1=0.6108 | Recall=0.5823 | Precision=0.6423 | thr(val)=0.463 | best_iter=1984\n",
            "- test_roc_auc: XGB_FULL_SMOTENC | PR-AUC=0.7055 | ROC-AUC=0.8588 | F1=0.6108 | Recall=0.5823 | Precision=0.6423 | thr(val)=0.463 | best_iter=1984\n",
            "- test_recall: XGB_FULL_SMOTENC_TUNED | PR-AUC=0.7054 | ROC-AUC=0.8583 | F1=0.6075 | Recall=0.6560 | Precision=0.5657 | thr(val)=0.373 | best_iter=1962\n",
            "- test_f1: XGB_FULL_SMOTENC | PR-AUC=0.7055 | ROC-AUC=0.8588 | F1=0.6108 | Recall=0.5823 | Precision=0.6423 | thr(val)=0.463 | best_iter=1984\n",
            "- test_precision: XGB_FULL_SMOTENC | PR-AUC=0.7055 | ROC-AUC=0.8588 | F1=0.6108 | Recall=0.5823 | Precision=0.6423 | thr(val)=0.463 | best_iter=1984\n",
            "=== RESUMEN CV-OOF (por experimento) ===\n",
            "                          tag   pr_auc  roc_auc       f1   recall  bal_acc   thr\n",
            "XGB_FULL_SMOTENC_BASELINE_CV5 0.692154 0.860440 0.619421 0.646770 0.766864 0.441\n",
            "   XGB_FULL_SMOTENC_TUNED_CV5 0.688023 0.854281 0.613581 0.620605 0.758806 0.504\n",
            "=== COMPARACIÓN SOTA XGBOOST vs. MEJOR TEST ===\n",
            "Paper XGBoost: AUC=0.8512 | Recall=N/R | Precision=N/R\n",
            "Tu mejor   : AUC=0.8588 | Recall=0.5823 | Precision=0.6423\n",
            "Deltas     : ΔAUC=+0.0076\n",
            "Fuente SOTA: Shukla (2021), ICSCC — Kaggle Bank Churn (10k)\n",
            "[OK] Normalizado. Backup: baselines_legacy_backup.csv\n"
          ]
        }
      ],
      "source": [
        "def _get_params_for(model_key, tuned=True):\n",
        "    if model_key == \"xgb\":\n",
        "        if tuned and (locals().get(\"xgb_best_params\") is not None):\n",
        "            p = dict(xgb_best_params)\n",
        "            p.update({\"n_jobs\": -1, \"eval_metric\": \"aucpr\", \"tree_method\": \"hist\", \"verbosity\": 0, \"random_state\": RANDOM_STATE})\n",
        "            return p\n",
        "        else:\n",
        "            p = dict(xgb_params_seed)\n",
        "            p.update({\"n_jobs\": -1, \"eval_metric\": \"aucpr\", \"tree_method\": \"hist\", \"verbosity\": 0, \"random_state\": RANDOM_STATE})\n",
        "            return p\n",
        "    elif model_key == \"lgbm\":\n",
        "        if tuned and (locals().get(\"lgbm_best_params\") is not None):\n",
        "            p = dict(lgbm_best_params)\n",
        "            p.update({\"random_state\": RANDOM_STATE})\n",
        "            return p\n",
        "        else:\n",
        "            p = dict(lgbm_params_seed)\n",
        "            p.update({\"random_state\": RANDOM_STATE})\n",
        "            return p\n",
        "    else:\n",
        "        raise ValueError(\"model_key debe ser 'xgb' o 'lgbm'.\")\n",
        "\n",
        "def _fit_adapter(model_key, params, X_tr, y_tr, X_va, y_va):\n",
        "    if model_key == \"xgb\":\n",
        "        mdl = XGBClassifier(**params)\n",
        "        return xgb_fit_with_es(mdl, X_tr, y_tr, X_va, y_va, feature_names=feature_names, rounds=200, verbose=False)\n",
        "    else:\n",
        "        mdl = LGBMClassifier(**params)\n",
        "        return lgbm_fit_with_es(mdl, X_tr, y_tr, X_va, y_va, feature_names=feature_names, rounds=200, verbose=False)\n",
        "\n",
        "def _cv_oof(model_key, tuned=True, n_splits=CV_FOLDS):\n",
        "    X_dev = np.vstack([X_train, X_val])\n",
        "    y_dev = np.concatenate([y_train, y_val])\n",
        "\n",
        "    oof = np.zeros_like(y_dev, dtype=float)\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "    params = _get_params_for(model_key, tuned=tuned)\n",
        "\n",
        "    for fold, (tr_idx, va_idx) in enumerate(skf.split(X_dev, y_dev), 1):\n",
        "        X_tr, y_tr = X_dev[tr_idx], y_dev[tr_idx]\n",
        "        X_va, y_va = X_dev[va_idx], y_dev[va_idx]\n",
        "\n",
        "        if BALANCE_IN_CV and USE_BALANCED_TRAIN:\n",
        "            X_tr, y_tr = maybe_smote(X_tr, y_tr, keep_idx=None)\n",
        "\n",
        "        adapter = _fit_adapter(model_key, params, X_tr, y_tr, X_va, y_va)\n",
        "        oof[va_idx] = adapter.predict_proba(X_va)[:,1]\n",
        "        print(f\"[CV][{model_key.upper()}][{'TUNED' if tuned else 'BASE'}] fold {fold}/{n_splits} listo.\")\n",
        "\n",
        "    ap_oof = average_precision_score(y_dev, oof)\n",
        "    thr_oof, _ = find_best_threshold(y_dev, oof, metric=\"f1\")\n",
        "\n",
        "    X_tr_final, y_tr_final = X_train, y_train\n",
        "    if USE_BALANCED_TRAIN:\n",
        "        X_tr_final, y_tr_final = maybe_smote(X_tr_final, y_tr_final, keep_idx=None)\n",
        "\n",
        "    adapter_final = _fit_adapter(model_key, params, X_tr_final, y_tr_final, X_val, y_val)\n",
        "    p_test = adapter_final.predict_proba(X_test)[:,1]\n",
        "    y_pred_test = (p_test >= thr_oof).astype(int)\n",
        "\n",
        "    val_metrics = compute_all_metrics(y_val, adapter_final.predict_proba(X_val)[:,1], thr_oof)\n",
        "    test_metrics = compute_all_metrics(y_test, p_test, thr_oof)\n",
        "\n",
        "    # Guardados\n",
        "    subdir = DIRS[model_key]\n",
        "    OUT_PREDS = subdir / \"preds\"\n",
        "    OUT_RESULTS = subdir / \"results\"\n",
        "\n",
        "    # OOF\n",
        "    pd.DataFrame({\"proba_oof\": oof, \"y_true\": y_dev}).to_parquet(\n",
        "        OUT_PREDS / f\"oof_preds_{model_key}_{'TUNED' if tuned else 'BASE'}.parquet\", index=False\n",
        "    )\n",
        "\n",
        "    row = {\n",
        "        \"model\": f\"{model_key.upper()}_{VIEW_TAG}_{BAL_TAG}_{'TUNED' if tuned else 'BASE'}_CV\",\n",
        "        \"thr_val\": float(\"nan\"),\n",
        "        \"thr_oof\": float(thr_oof),\n",
        "        \"thr_used\": float(thr_oof),\n",
        "        \"oof_pr_auc\": float(ap_oof),\n",
        "        \"val_pr_auc\": val_metrics[\"pr_auc\"],\n",
        "        \"val_f1\": val_metrics[\"f1\"],\n",
        "        \"test_pr_auc\": test_metrics[\"pr_auc\"],\n",
        "        \"test_f1\": test_metrics[\"f1\"]\n",
        "    }\n",
        "    csv_cv = OUT_RESULTS / \"cv_summary.csv\"\n",
        "    pd.DataFrame([row]).to_csv(csv_cv, mode=(\"a\" if csv_cv.exists() else \"w\"),\n",
        "                               index=False, header=not csv_cv.exists())\n",
        "\n",
        "    print(f\"[CV][{model_key.upper()}][{'TUNED' if tuned else 'BASE'}] OOF AP={ap_oof:.6f} | thr_oof={thr_oof:.3f}\")\n",
        "    return oof, thr_oof, test_metrics\n",
        "\n",
        "# Ejecutar según toggles\n",
        "if DO_CV_BASELINE:\n",
        "    _ = _cv_oof(\"xgb\", tuned=False)\n",
        "    _ = _cv_oof(\"lgbm\", tuned=False)\n",
        "\n",
        "if DO_CV_TUNED:\n",
        "    if xgb_best_params is not None:\n",
        "        _ = _cv_oof(\"xgb\", tuned=True)\n",
        "    if lgbm_best_params is not None:\n",
        "        _ = _cv_oof(\"lgbm\", tuned=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784bbb56",
      "metadata": {},
      "source": [
        "11 — Resumen consolidado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41bffd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "\n",
        "def _read_csv_safepath(path):\n",
        "    return pd.read_csv(path) if Path(path).exists() else pd.DataFrame()\n",
        "\n",
        "frames = []\n",
        "\n",
        "# baselines\n",
        "for key in [\"xgb\",\"lgbm\",\"ens\"]:\n",
        "    csv_path = DIRS[key] / \"results\" / \"baselines.csv\"\n",
        "    if csv_path.exists():\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df[\"where\"] = key\n",
        "        frames.append(df)\n",
        "\n",
        "# CV summaries\n",
        "for key in [\"xgb\",\"lgbm\"]:\n",
        "    cv_path = DIRS[key] / \"results\" / \"cv_summary.csv\"\n",
        "    if cv_path.exists():\n",
        "        df = pd.read_csv(cv_path)\n",
        "        df[\"where\"] = f\"{key}_cv\"\n",
        "        frames.append(df)\n",
        "\n",
        "summary = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
        "summary_cols = [c for c in [\n",
        "    \"model\",\"where\",\n",
        "    \"thr_val\",\"thr_oof\",\"thr_used\",\n",
        "    \"val_pr_auc\",\"val_roc_auc\",\"val_f1\",\"val_precision\",\"val_recall\",\"val_bal_acc\",\n",
        "    \"test_pr_auc\",\"test_roc_auc\",\"test_f1\",\"test_precision\",\"test_recall\",\"test_bal_acc\",\n",
        "    \"oof_pr_auc\"\n",
        "] if c in summary.columns]\n",
        "\n",
        "summary = summary[summary_cols].sort_values(\n",
        "    [\"test_pr_auc\",\"val_pr_auc\",\"oof_pr_auc\"], ascending=[False, False, False]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "SUM_DIR = ARTIF_ROOT / \"summary\"\n",
        "SUM_DIR.mkdir(parents=True, exist_ok=True)\n",
        "summary_fp = SUM_DIR / f\"summary_{VIEW_TAG}_{BAL_TAG}.csv\"\n",
        "summary.to_csv(summary_fp, index=False)\n",
        "\n",
        "display(summary.head(20))\n",
        "print(f\"[OK][SUMMARY] Guardado en: {summary_fp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e3ec4f",
      "metadata": {},
      "source": [
        "12 — Export para producción + loader utilitario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3e0853",
      "metadata": {},
      "outputs": [],
      "source": [
        "EXPORT_DIR = ARTIF_ROOT / \"export\"\n",
        "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Elegimos TUNED si están disponibles\n",
        "_final_xgb = locals().get(\"xgb_tuned\",  locals().get(\"xgb_baseline_model\", None))\n",
        "_final_lgb = locals().get(\"lgbm_tuned\", locals().get(\"lgbm_adapter\", None))\n",
        "\n",
        "# XGB export\n",
        "try:\n",
        "    if _final_xgb is not None:\n",
        "        booster = _final_xgb.get_booster()\n",
        "        booster.save_model(str(EXPORT_DIR / f\"xgb_{VIEW_TAG}_{BAL_TAG}.json\"))\n",
        "        print(\"[EXPORT] XGB guardado como JSON.\")\n",
        "except Exception as e:\n",
        "    print(\"[EXPORT][XGB] Aviso:\", e)\n",
        "\n",
        "# LGBM export\n",
        "try:\n",
        "    if _final_lgb is not None and hasattr(_final_lgb, \"booster_\"):\n",
        "        _final_lgb.booster_.save_model(str(EXPORT_DIR / f\"lgbm_{VIEW_TAG}_{BAL_TAG}.txt\"))\n",
        "        print(\"[EXPORT] LGBM guardado como TXT.\")\n",
        "except Exception as e:\n",
        "    print(\"[EXPORT][LGBM] Aviso:\", e)\n",
        "\n",
        "# Guardar pesos del ensamble\n",
        "ens_params = OUT_PARAMS_E / f\"{EXP_NAME_ENS}_weights.json\"\n",
        "if ens_params.exists():\n",
        "    print(\"[EXPORT] Ensamble (pesos) disponible en:\", ens_params)\n",
        "\n",
        "# === Loader utilitario ===\n",
        "def load_xgb_lgbm(export_dir: Path = EXPORT_DIR):\n",
        "    \"\"\"\n",
        "    Devuelve (xgb_booster, lgbm_booster, ens_cfg_dict or None)\n",
        "    \"\"\"\n",
        "    xgb_path = export_dir / f\"xgb_{VIEW_TAG}_{BAL_TAG}.json\"\n",
        "    lgb_path = export_dir / f\"lgbm_{VIEW_TAG}_{BAL_TAG}.txt\"\n",
        "    ens_path = OUT_PARAMS_E / f\"{EXP_NAME_ENS}_weights.json\"\n",
        "\n",
        "    booster_xgb = None\n",
        "    booster_lgb = None\n",
        "    ens_cfg = None\n",
        "\n",
        "    if xgb_path.exists():\n",
        "        booster_xgb = xgb.Booster()\n",
        "        booster_xgb.load_model(str(xgb_path))\n",
        "\n",
        "    if lgb_path.exists():\n",
        "        booster_lgb = lgb.Booster(model_file=str(lgb_path))\n",
        "\n",
        "    if ens_path.exists():\n",
        "        ens_cfg = json.loads(ens_path.read_text(encoding=\"utf-8\"))\n",
        "\n",
        "    return booster_xgb, booster_lgb, ens_cfg\n",
        "\n",
        "def predict_with_loaded(booster_xgb, booster_lgb, X, ens_cfg=None):\n",
        "\n",
        "    out = {}\n",
        "    if booster_xgb is not None:\n",
        "        px = booster_xgb.predict(xgb.DMatrix(X, feature_names=feature_names))\n",
        "        out[\"xgb\"] = px\n",
        "    if booster_lgb is not None:\n",
        "        pl = booster_lgb.predict(X, num_iteration=booster_lgb.best_iteration)\n",
        "        out[\"lgbm\"] = pl\n",
        "    if ens_cfg is not None and (\"xgb\" in out) and (\"lgbm\" in out):\n",
        "        w = float(ens_cfg.get(\"w_xgb\", 0.5))\n",
        "        method = ens_cfg.get(\"method\", \"logit\")\n",
        "        p1, p2 = out[\"xgb\"], out[\"lgbm\"]\n",
        "        out[\"ens\"] = ens_probs(p1, p2, w=w, method=method)\n",
        "    return out\n",
        "\n",
        "print(\"[OK] Export y loader utilitario listos.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
