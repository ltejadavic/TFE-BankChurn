{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d61c7a4",
   "metadata": {},
   "source": [
    "1 — Imports, config y rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11fd130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT:      /Users/luistejada/Downloads/TFE Churn Bancario\n",
      "DATA_DIR:  /Users/luistejada/Downloads/TFE Churn Bancario/preproc_datasets/full\n",
      "ARTIF_DIR: /Users/luistejada/Downloads/TFE Churn Bancario/artifacts\n",
      "{'xgb': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_REDUCED_SMOTENC', 'lgbm': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_REDUCED_SMOTENC', 'dl': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/DL_REDUCED_SMOTENC', 'ens': '/Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_REDUCED_SMOTENC'}\n"
     ]
    }
   ],
   "source": [
    "import json, os, warnings, time, re\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Métricas / CV / Modelos\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, precision_recall_curve, roc_auc_score, roc_curve,\n",
    "    f1_score, recall_score, balanced_accuracy_score, confusion_matrix, precision_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Balanceo\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTENC, SMOTE\n",
    "    _HAS_IMBLEARN = True\n",
    "except Exception:\n",
    "    _HAS_IMBLEARN = False\n",
    "\n",
    "# Modelos base\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Optuna\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "USE_REDUCED = True\n",
    "USE_BALANCED_TRAIN = True\n",
    "BALANCE_IN_CV = True\n",
    "RANDOM_STATE = 42\n",
    "DO_TUNE_XGB = True\n",
    "DO_TUNE_LGBM = True\n",
    "DO_CV_BASELINE = True\n",
    "DO_CV_TUNED = True\n",
    "CV_FOLDS = 5\n",
    "MI_TOPK = 30\n",
    "\n",
    "# --- Localizador ---\n",
    "def _auto_project_root():\n",
    "    env_root = os.environ.get(\"PROJECT_ROOT\")\n",
    "    if env_root and (Path(env_root)/\"preproc_datasets\"/\"full\").exists():\n",
    "        return Path(env_root)\n",
    "    \n",
    "    here = Path.cwd().resolve()\n",
    "    candidates = [here, here.parent, here.parent.parent, here.parent.parent.parent]\n",
    "    for base in candidates:\n",
    "        if (base/\"preproc_datasets\"/\"full\"/\"X_train_full.npy\").exists():\n",
    "            return base\n",
    "        if (base/\"preproc_datasets\"/\"full\").exists():\n",
    "            return base\n",
    "\n",
    "    home_fallback = Path.home() / \"Downloads\" / \"TFE Churn Bancario\"\n",
    "    if (home_fallback/\"preproc_datasets\"/\"full\").exists():\n",
    "        return home_fallback\n",
    "\n",
    "    return here\n",
    "\n",
    "ROOT = _auto_project_root()\n",
    "\n",
    "# === Rutas ===\n",
    "DATA_DIR = ROOT / \"preproc_datasets\" / \"full\"\n",
    "ARTIF_ROOT = ROOT / \"artifacts\"\n",
    "\n",
    "# Tags\n",
    "VIEW_TAG = \"REDUCED\" if USE_REDUCED else \"FULL\"\n",
    "BAL_TAG  = \"SMOTENC\" if USE_BALANCED_TRAIN else \"IMB\"\n",
    "\n",
    "# Carpetas por modelo y ensamble\n",
    "DIRS = {\n",
    "    \"xgb\":   ARTIF_ROOT / f\"XGB_{VIEW_TAG}_{BAL_TAG}\",\n",
    "    \"lgbm\":  ARTIF_ROOT / f\"LGBM_{VIEW_TAG}_{BAL_TAG}\",\n",
    "    \"dl\":    ARTIF_ROOT / f\"DL_{VIEW_TAG}_{BAL_TAG}\",\n",
    "    \"ens\":   ARTIF_ROOT / f\"ENS_{VIEW_TAG}_{BAL_TAG}\",\n",
    "}\n",
    "for k, base in DIRS.items():\n",
    "    for sub in [\"results\", \"figs\", \"preds\", \"best_params\", \"export\"]:\n",
    "        (base / sub).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:     \", ROOT)\n",
    "print(\"DATA_DIR: \", DATA_DIR)\n",
    "print(\"ARTIF_DIR:\", ARTIF_ROOT)\n",
    "print({k: str(v) for k, v in DIRS.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c7b3d6",
   "metadata": {},
   "source": [
    "2 — Carga de datos y metadatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b45c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (6000, 15) (2000, 15) (2000, 15)\n",
      "y: (6000,) (2000,) (2000,)\n",
      "n features: 15\n",
      "[META] Archivo detectado: N/D | columnas categóricas=0\n",
      "[META] No hay categóricas => caerá en SMOTE estándar.\n",
      "[L1] Máscara L1 cargada con 12 features.\n",
      "[L1] Shapes reducidos: train (6000, 12) | val (2000, 12) | test (2000, 12)\n",
      "[L1] Ejemplo de features usados: ['num__CreditScore', 'num__Age', 'num__Tenure', 'num__Balance', 'num__EstimatedSalary', 'Geography_1', 'Gender_1', 'HasCrCard_1', 'IsActiveMember_1', 'NumOfProducts_1', 'NumOfProducts_2', 'NumOfProducts_3']\n"
     ]
    }
   ],
   "source": [
    "def load_xy_full(dir_full: Path):\n",
    "    expected = [\n",
    "        dir_full / \"X_train_full.npy\",\n",
    "        dir_full / \"X_val_full.npy\",\n",
    "        dir_full / \"X_test_full.npy\",\n",
    "        dir_full / \"y_train.parquet\",\n",
    "        dir_full / \"y_val.parquet\",\n",
    "        dir_full / \"y_test.parquet\",\n",
    "        dir_full / \"feature_names_full.parquet\",\n",
    "    ]\n",
    "    missing = [p.name for p in expected if not p.exists()]\n",
    "    if missing:\n",
    "        listing = [p.name for p in dir_full.glob(\"*\")]\n",
    "        raise FileNotFoundError(\n",
    "            f\"No se encuentran estos archivos en {dir_full} -> {missing}\\n\"\n",
    "            f\"Contenido detectado: {listing}\\n\"\n",
    "            f\"Sugerencia: verifica que ROOT (impreso arriba) apunte a la carpeta del proyecto.\"\n",
    "        )\n",
    "\n",
    "    X_train = np.load(dir_full / \"X_train_full.npy\")\n",
    "    X_val   = np.load(dir_full / \"X_val_full.npy\")\n",
    "    X_test  = np.load(dir_full / \"X_test_full.npy\")\n",
    "\n",
    "    y_train = pd.read_parquet(dir_full / \"y_train.parquet\")[\"Exited\"].to_numpy()\n",
    "    y_val   = pd.read_parquet(dir_full / \"y_val.parquet\")[\"Exited\"].to_numpy()\n",
    "    y_test  = pd.read_parquet(dir_full / \"y_test.parquet\")[\"Exited\"].to_numpy()\n",
    "\n",
    "    feature_names = pd.read_parquet(dir_full / \"feature_names_full.parquet\")[\"feature\"].tolist()\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, feature_names\n",
    "\n",
    "# Cargar datos\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, feature_names = load_xy_full(DATA_DIR)\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"y:\", y_train.shape, y_val.shape, y_test.shape)\n",
    "print(\"n features:\", len(feature_names))\n",
    "\n",
    "# --- Metadatos para SMOTENC ---\n",
    "def _read_feature_roles(dir_full: Path):\n",
    "    candidates = [\n",
    "        dir_full / \"feature_roles_full.parquet\",\n",
    "        dir_full / \"feature_roles.parquet\",\n",
    "        dir_full / \"feature_meta_full.parquet\",\n",
    "        dir_full / \"feature_meta.parquet\",\n",
    "        dir_full / \"feature_types_full.parquet\",\n",
    "        dir_full / \"feature_types.parquet\",\n",
    "        dir_full / \"feature_meta.json\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            if p.suffix == \".parquet\":\n",
    "                df = pd.read_parquet(p)\n",
    "            elif p.suffix == \".json\":\n",
    "                obj = json.loads(p.read_text())\n",
    "                if isinstance(obj, dict) and \"features\" in obj:\n",
    "                    df = pd.DataFrame(obj[\"features\"])\n",
    "                else:\n",
    "                    df = pd.DataFrame(obj)\n",
    "            else:\n",
    "                continue\n",
    "            return df, p.name\n",
    "    return None, None\n",
    "\n",
    "def _build_cat_idx(feature_names, roles_df):\n",
    "    if roles_df is None or len(roles_df) == 0:\n",
    "        return []\n",
    "    df = roles_df.copy()\n",
    "    df.columns = [str(c).lower() for c in df.columns]\n",
    "    if \"feature\" not in df.columns:\n",
    "        if \"name\" in df.columns:\n",
    "            df[\"feature\"] = df[\"name\"]\n",
    "        else:\n",
    "            return []\n",
    "    cat_names = set()\n",
    "    if \"role\" in df.columns:\n",
    "        cat_names = set(df.loc[df[\"role\"].astype(str).str.lower().isin(\n",
    "            [\"cat\",\"categorical\",\"bin\",\"binary\",\"ordinal\"]), \"feature\"])\n",
    "    elif \"dtype\" in df.columns:\n",
    "        cat_names = set(df.loc[df[\"dtype\"].astype(str).str.lower().isin(\n",
    "            [\"category\",\"categorical\",\"object\",\"bool\"]), \"feature\"])\n",
    "    elif \"is_cat\" in df.columns:\n",
    "        cat_names = set(df.loc[df[\"is_cat\"].astype(bool), \"feature\"])\n",
    "    else:\n",
    "        return []\n",
    "    idx = [i for i, f in enumerate(feature_names) if f in cat_names]\n",
    "    return sorted(idx)\n",
    "\n",
    "roles_df, meta_file = _read_feature_roles(DATA_DIR)\n",
    "CAT_IDX_FULL = _build_cat_idx(feature_names, roles_df)\n",
    "print(f\"[META] Archivo detectado: {meta_file or 'N/D'} | columnas categóricas={len(CAT_IDX_FULL)}\")\n",
    "if CAT_IDX_FULL:\n",
    "    print(\"[META] Ejemplo de índices categóricos:\", CAT_IDX_FULL[:10], \"...\")\n",
    "else:\n",
    "    print(\"[META] No hay categóricas => caerá en SMOTE estándar.\")\n",
    "\n",
    "# === L1: cargar máscara desde XGB_REDUCED_SMOTENC ===\n",
    "XGB_L1_FS_DIR = ARTIF_ROOT / \"XGB_REDUCED_SMOTENC\" / \"best_params\" / \"feature_selection\"\n",
    "keep_idx_path = XGB_L1_FS_DIR / \"keep_idx_L1.npy\"\n",
    "\n",
    "if not keep_idx_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"[L1] No se encontró keep_idx_L1.npy en {keep_idx_path}\\n\"\n",
    "        f\"Asegúrate de haber corrido antes el experimento XGB_REDUCED_SMOTENC con selección L1.\"\n",
    "    )\n",
    "\n",
    "# Índices de columnas a conservar según L1 (en el espacio FULL)\n",
    "keep_idx_global = np.load(keep_idx_path).astype(int).tolist()\n",
    "print(f\"[L1] Máscara L1 cargada con {len(keep_idx_global)} features.\")\n",
    "\n",
    "# Helper para aplicar máscara\n",
    "def apply_keep_idx(X, keep_idx):\n",
    "    return X[:, keep_idx]\n",
    "\n",
    "# Aplicar la máscara a los datasets FULL\n",
    "X_train_fit = apply_keep_idx(X_train, keep_idx_global)\n",
    "X_val_fit   = apply_keep_idx(X_val,   keep_idx_global)\n",
    "X_test_fit  = apply_keep_idx(X_test,  keep_idx_global)\n",
    "\n",
    "feature_names_used = [feature_names[i] for i in keep_idx_global]\n",
    "\n",
    "print(\"[L1] Shapes reducidos:\",\n",
    "      \"train\", X_train_fit.shape,\n",
    "      \"| val\", X_val_fit.shape,\n",
    "      \"| test\", X_test_fit.shape)\n",
    "print(\"[L1] Ejemplo de features usados:\", feature_names_used[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b76afe4",
   "metadata": {},
   "source": [
    "3 — Métricas, umbral, y plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789d35e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "def pr_auc(y_true, y_proba): \n",
    "    return float(average_precision_score(y_true, y_proba))\n",
    "\n",
    "def roc_auc(y_true, y_proba): \n",
    "    return float(roc_auc_score(y_true, y_proba))\n",
    "\n",
    "def find_best_threshold(y_true, y_proba, metric=\"f1\"):\n",
    "    thr_grid = np.linspace(0.0, 1.0, 1001)\n",
    "    best_thr, best_score = 0.5, -1.0\n",
    "    for thr in thr_grid:\n",
    "        y_pred = (y_proba >= thr).astype(int)\n",
    "        if metric == \"f1\":\n",
    "            score = f1_score(y_true, y_pred, zero_division=0)\n",
    "        elif metric == \"recall\":\n",
    "            score = recall_score(y_true, y_pred, zero_division=0)\n",
    "        else:\n",
    "            raise ValueError(\"metric no soportada\")\n",
    "        if score > best_score:\n",
    "            best_score, best_thr = score, thr\n",
    "    return float(best_thr), float(best_score)\n",
    "\n",
    "def compute_all_metrics(y_true, y_proba, thr):\n",
    "    y_pred = (y_proba >= thr).astype(int)\n",
    "    return {\n",
    "        \"pr_auc\": pr_auc(y_true, y_proba),\n",
    "        \"roc_auc\": roc_auc(y_true, y_proba),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"bal_acc\": balanced_accuracy_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def plot_pr_curve(y_true, y_proba, title, out_path):\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_proba)\n",
    "    ap = average_precision_score(y_true, y_proba)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.step(rec, prec, where='post')\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.title(f'{title} (AP={ap:.4f})')\n",
    "    plt.grid(True, linestyle='--', alpha=.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "def plot_roc_curve(y_true, y_proba, title, out_path):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, lw=2)\n",
    "    plt.plot([0,1],[0,1], 'k--', lw=1)\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title} (AUC={auc:.4f})')\n",
    "    plt.grid(True, linestyle='--', alpha=.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "def plot_confusion(y_true, y_pred, title, out_path, normalize=False):\n",
    "    norm = 'true' if normalize else None\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize=norm)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    ticks = np.arange(2)\n",
    "    plt.xticks(ticks, ['0','1']); plt.yticks(ticks, ['0','1'])\n",
    "    thresh = cm.max()/2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            txt = f'{cm[i,j]:.2f}' if normalize else str(cm[i,j])\n",
    "            plt.text(j, i, txt, ha='center', va='center',\n",
    "                     color='white' if cm[i,j] > thresh else 'black')\n",
    "    plt.ylabel('True label'); plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5651c79",
   "metadata": {},
   "source": [
    "4 — Helpers de balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be6c0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _map_cat_idx_for_keep(keep_idx, cat_idx_full):\n",
    "    if not cat_idx_full:\n",
    "        return []\n",
    "    if keep_idx is None:\n",
    "        return sorted(cat_idx_full)\n",
    "    pos = {old_i: j for j, old_i in enumerate(keep_idx)}\n",
    "    return sorted([pos[i] for i in cat_idx_full if i in pos])\n",
    "\n",
    "def apply_keep_idx(X, keep_idx):\n",
    "    return X[:, keep_idx]\n",
    "\n",
    "def maybe_smote(X, y, keep_idx=None, random_state=RANDOM_STATE, k_neighbors=5):\n",
    "    if not _HAS_IMBLEARN:\n",
    "        print(\"[BAL] imbalanced-learn no disponible. Se omite balanceo.\")\n",
    "        return X, y\n",
    "    y_int = y.astype(int)\n",
    "    if y_int.max() == 0:\n",
    "        print(\"[BAL] Solo 1 clase en y. Se omite balanceo.\")\n",
    "        return X, y\n",
    "    counts = np.bincount(y_int)\n",
    "    if len(counts) < 2 or counts.min() < 2:\n",
    "        print(\"[BAL] Minoría < 2 muestras. Se omite balanceo.\")\n",
    "        return X, y\n",
    "    k = int(max(1, min(5, counts.min() - 1)))\n",
    "    cat_idx = _map_cat_idx_for_keep(keep_idx, CAT_IDX_FULL)\n",
    "    if cat_idx:\n",
    "        sm = SMOTENC(categorical_features=cat_idx, k_neighbors=k, random_state=random_state)\n",
    "        kind = \"SMOTENC\"\n",
    "    else:\n",
    "        sm = SMOTE(k_neighbors=k, random_state=random_state)\n",
    "        kind = \"SMOTE\"\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    try:\n",
    "        X_res = X_res.astype(X.dtype, copy=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(f\"[BAL] {kind} aplicado | k_neighbors={k} | cat_cols={len(cat_idx)}\")\n",
    "    return X_res, y_res\n",
    "\n",
    "\n",
    "def _json_dump(path, obj):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def _floatify_metrics(d):\n",
    "    return {k: float(v) for k, v in d.items()}\n",
    "\n",
    "def write_minimal_loader(out_dir: Path):\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    code = r'''# -*- coding: utf-8 -*-\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def load_xgb_from_manifest(manifest_path):\n",
    "    import xgboost as xgb\n",
    "    mp = Path(manifest_path)\n",
    "    m = json.loads(mp.read_text(encoding=\"utf-8\"))\n",
    "    mdl_file = mp.parent / m[\"files\"][\"model_json\"]\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(str(mdl_file))\n",
    "\n",
    "    best_it = m.get(\"training\", {}).get(\"best_iteration\", None)\n",
    "    feat_names = m[\"features\"][\"names\"]\n",
    "\n",
    "    def predict_proba(X):\n",
    "        d = xgb.DMatrix(X, feature_names=feat_names)\n",
    "        if best_it is not None and isinstance(best_it, int):\n",
    "            p = booster.predict(d, iteration_range=(0, best_it+1))\n",
    "        else:\n",
    "            p = booster.predict(d)\n",
    "        return np.column_stack([1.0 - p, p])\n",
    "\n",
    "    return m, predict_proba\n",
    "\n",
    "def load_lgbm_from_manifest(manifest_path):\n",
    "    import lightgbm as lgb\n",
    "    mp = Path(manifest_path)\n",
    "    m = json.loads(mp.read_text(encoding=\"utf-8\"))\n",
    "    mdl_file = mp.parent / m[\"files\"][\"model_txt\"]\n",
    "    booster = lgb.Booster(model_file=str(mdl_file))\n",
    "\n",
    "    feat_names = m[\"features\"][\"names\"]\n",
    "    best_it = m.get(\"training\", {}).get(\"best_iteration\", None)\n",
    "\n",
    "    def predict_proba(X):\n",
    "        p1 = booster.predict(X, num_iteration=best_it)\n",
    "        return np.column_stack([1.0 - p1, p1])\n",
    "\n",
    "    return m, predict_proba\n",
    "'''\n",
    "    (out_dir / \"loader_example.py\").write_text(code, encoding=\"utf-8\")\n",
    "\n",
    "def export_xgb(adapter, out_dir, exp_name, feature_names, threshold, val_metrics, test_metrics):\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    booster = adapter.get_booster()\n",
    "    json_path = out_dir / f\"{exp_name}.xgb.json\"\n",
    "    ubj_path  = out_dir / f\"{exp_name}.xgb.ubj\"\n",
    "\n",
    "    booster.save_model(str(json_path))\n",
    "\n",
    "    files = {\"model_json\": json_path.name}\n",
    "    try:\n",
    "        booster.save_model(str(ubj_path))\n",
    "        files[\"model_ubj\"] = ubj_path.name\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import joblib\n",
    "        joblib.dump(\n",
    "            {\"params\": adapter.get_params(), \"best_iteration\": adapter.best_iteration},\n",
    "            out_dir / f\"{exp_name}.sk_params.joblib\"\n",
    "        )\n",
    "        files[\"sk_params_joblib\"] = f\"{exp_name}.sk_params.joblib\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    manifest = {\n",
    "        \"export_schema\": \"v1\",\n",
    "        \"model_key\": exp_name,\n",
    "        \"kind\": \"xgboost_binary_classifier\",\n",
    "        \"framework\": \"xgboost\",\n",
    "        \"framework_version\": getattr(xgb, \"__version__\", \"unknown\"),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"files\": files,\n",
    "        \"features\": {\n",
    "            \"names\": list(map(str, feature_names)),\n",
    "            \"dtype\": \"float32\"\n",
    "        },\n",
    "        \"inference\": {\n",
    "            \"class_labels\": [0, 1],\n",
    "            \"threshold\": float(threshold)\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"best_iteration\": int(adapter.best_iteration) if adapter.best_iteration is not None else None\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"validation\": _floatify_metrics(val_metrics),\n",
    "            \"test\": _floatify_metrics(test_metrics)\n",
    "        }\n",
    "    }\n",
    "    _json_dump(out_dir / f\"{exp_name}_MANIFEST.json\", manifest)\n",
    "    write_minimal_loader(out_dir)\n",
    "    print(f\"[EXPORT][XGB] {exp_name} -> {out_dir}\")\n",
    "    return manifest\n",
    "\n",
    "def export_lgbm(adapter, out_dir, exp_name, feature_names, threshold, val_metrics, test_metrics):\n",
    "    \"\"\"\n",
    "    Exporta:\n",
    "      - Modelo LightGBM en TXT (formato de texto oficial).\n",
    "      - Manifiesto JSON con nombres de features, umbral y métricas.\n",
    "      - Loader de ejemplo.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    booster = adapter.booster_\n",
    "    txt_path = out_dir / f\"{exp_name}.lgbm.txt\"\n",
    "    booster.save_model(str(txt_path))\n",
    "\n",
    "    manifest = {\n",
    "        \"export_schema\": \"v1\",\n",
    "        \"model_key\": exp_name,\n",
    "        \"kind\": \"lightgbm_binary_classifier\",\n",
    "        \"framework\": \"lightgbm\",\n",
    "        \"framework_version\": getattr(lgb, \"__version__\", \"unknown\"),\n",
    "        \"python_version\": platform.python_version(),\n",
    "        \"files\": {\n",
    "            \"model_txt\": txt_path.name\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"names\": list(map(str, feature_names)),\n",
    "            \"dtype\": \"float32\"\n",
    "        },\n",
    "        \"inference\": {\n",
    "            \"class_labels\": [0, 1],\n",
    "            \"threshold\": float(threshold)\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"best_iteration\": int(adapter.best_iteration) if getattr(adapter, \"best_iteration\", None) is not None else None\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"validation\": _floatify_metrics(val_metrics),\n",
    "            \"test\": _floatify_metrics(test_metrics)\n",
    "        }\n",
    "    }\n",
    "    _json_dump(out_dir / f\"{exp_name}_MANIFEST.json\", manifest)\n",
    "    write_minimal_loader(out_dir)\n",
    "    print(f\"[EXPORT][LGBM] {exp_name} -> {out_dir}\")\n",
    "    return manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95e23c",
   "metadata": {},
   "source": [
    "5 — Adaptadores de entrenamiento con Early-Stopping (XGB & LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "828777f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- XGBoost ---\n",
    "class XGBAdapter:\n",
    "    def __init__(self, booster, params, best_iteration, feature_names=None):\n",
    "        self._booster = booster\n",
    "        self._params = dict(params)\n",
    "        self.best_iteration = best_iteration\n",
    "        self._feature_names = feature_names\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        d = xgb.DMatrix(X, feature_names=self._feature_names)\n",
    "        if self.best_iteration is not None:\n",
    "            pred = self._booster.predict(d, iteration_range=(0, int(self.best_iteration)+1))\n",
    "        else:\n",
    "            pred = self._booster.predict(d)\n",
    "        return np.column_stack([1.0 - pred, pred])\n",
    "\n",
    "    def get_booster(self):\n",
    "        return self._booster\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict(self._params)\n",
    "\n",
    "\n",
    "def xgb_fit_with_es(sk_model, X_tr, y_tr, X_va, y_va, feature_names=None, rounds=200, verbose=False):\n",
    "    p = sk_model.get_params()\n",
    "    n_estimators = int(p.pop(\"n_estimators\", 1000))\n",
    "    seed = p.pop(\"random_state\", p.pop(\"seed\", RANDOM_STATE))\n",
    "    nthread = p.pop(\"n_jobs\", None)\n",
    "    if nthread is not None:\n",
    "        p[\"nthread\"] = nthread\n",
    "    p.setdefault(\"seed\", seed)\n",
    "    p.setdefault(\"objective\", \"binary:logistic\")\n",
    "    p.setdefault(\"eval_metric\", \"aucpr\")\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr, feature_names=feature_names)\n",
    "    dvalid = xgb.DMatrix(X_va, label=y_va, feature_names=feature_names)\n",
    "    evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "    booster = xgb.train(\n",
    "        params=p, dtrain=dtrain, num_boost_round=n_estimators,\n",
    "        evals=evals, early_stopping_rounds=rounds, verbose_eval=verbose\n",
    "    )\n",
    "    best_iter = getattr(booster, \"best_iteration\", None)\n",
    "    return XGBAdapter(booster, {**sk_model.get_params(), \"best_iteration\": best_iter}, best_iter, feature_names)\n",
    "\n",
    "\n",
    "def xgb_gain_importances(booster, feature_names):\n",
    "    gain_dict = booster.get_score(importance_type=\"gain\")\n",
    "    name_to_idx = {n: i for i, n in enumerate(feature_names)}\n",
    "    imp_gain = np.zeros(len(feature_names), dtype=float)\n",
    "    for k, v in gain_dict.items():\n",
    "        if k.startswith(\"f\") and k[1:].isdigit():\n",
    "            idx = int(k[1:])\n",
    "        else:\n",
    "            idx = name_to_idx.get(k, None)\n",
    "        if idx is not None and 0 <= idx < len(imp_gain):\n",
    "            imp_gain[idx] = v\n",
    "    return imp_gain\n",
    "\n",
    "\n",
    "# --- LightGBM ---\n",
    "class LGBMAdapter:\n",
    "    \"\"\"Adapter consistente con .booster_ como atributo y .best_iteration.\"\"\"\n",
    "    def __init__(self, booster, params, best_iteration, feature_names=None):\n",
    "        self.booster_ = booster\n",
    "        self._params = dict(params)\n",
    "        self.best_iteration = best_iteration\n",
    "        self._feature_names = feature_names\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        p1 = self.booster_.predict(X, num_iteration=self.best_iteration)\n",
    "        return np.column_stack([1.0 - p1, p1])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return dict(self._params)\n",
    "\n",
    "\n",
    "def lgbm_fit_with_es(sk_model, X_tr, y_tr, X_va, y_va, feature_names=None, rounds=200, verbose=False):\n",
    "    p = dict(sk_model.get_params())\n",
    "    num_boost_round = int(p.pop(\"n_estimators\", 1000))\n",
    "\n",
    "    # Defaults coherentes con AP\n",
    "    lgb_params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": p.pop(\"metric\", \"average_precision\"),\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": p.pop(\"random_state\", RANDOM_STATE),\n",
    "    }\n",
    "\n",
    "    # Mapear nombres scikit->lightgbm\n",
    "    if \"colsample_bytree\" in p:\n",
    "        lgb_params[\"feature_fraction\"] = p.pop(\"colsample_bytree\")\n",
    "    if \"subsample\" in p:\n",
    "        lgb_params[\"bagging_fraction\"] = p.pop(\"subsample\")\n",
    "        lgb_params[\"bagging_freq\"] = 1\n",
    "\n",
    "    # Resto de hiperparámetros\n",
    "    lgb_params.update(p)\n",
    "\n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr, feature_name=feature_names, free_raw_data=True)\n",
    "    dvalid = lgb.Dataset(X_va, label=y_va, feature_name=feature_names, reference=dtrain, free_raw_data=True)\n",
    "\n",
    "    booster = lgb.train(\n",
    "        params=lgb_params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        valid_names=[\"train\", \"valid\"],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=rounds, verbose=verbose)]\n",
    "    )\n",
    "    best_iter = booster.best_iteration\n",
    "    return LGBMAdapter(booster, {**sk_model.get_params(), \"best_iteration\": best_iter}, best_iter, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be5066",
   "metadata": {},
   "source": [
    "6 — Carga/seed de hiperparámetros y defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2c238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HP][xgb] Cargando BEST previo: BEST_XGB_REDUCED_SMOTENC.json\n",
      "[HP][lgbm] Usando defaults.\n"
     ]
    }
   ],
   "source": [
    "def get_xgb_defaults(seed=RANDOM_STATE):\n",
    "    mdl = XGBClassifier(\n",
    "        random_state=seed, n_jobs=-1, eval_metric=\"aucpr\",\n",
    "        tree_method=\"hist\", verbosity=0\n",
    "    )\n",
    "    p = mdl.get_params()\n",
    "    p.pop(\"verbose\", None)\n",
    "    p.setdefault(\"verbosity\", 0)\n",
    "    p.setdefault(\"n_estimators\", 1000)\n",
    "    return p\n",
    "\n",
    "def get_lgbm_defaults(seed=RANDOM_STATE):\n",
    "    mdl = LGBMClassifier(\n",
    "        random_state=seed, n_estimators=1000, metric=\"average_precision\"\n",
    "    )\n",
    "    return mdl.get_params()\n",
    "\n",
    "def _best_file(model_key):\n",
    "    return DIRS[model_key] / \"best_params\" / f\"BEST_{model_key.upper()}_{VIEW_TAG}_{BAL_TAG}.json\"\n",
    "\n",
    "def load_best_or_default(model_key):\n",
    "    best_fp = _best_file(model_key)\n",
    "    if best_fp.exists():\n",
    "        try:\n",
    "            best = json.loads(best_fp.read_text())\n",
    "            print(f\"[HP][{model_key}] Cargando BEST previo:\", best_fp.name)\n",
    "            base = get_xgb_defaults() if model_key==\"xgb\" else get_lgbm_defaults()\n",
    "            base.update(best)\n",
    "            return base, True\n",
    "        except Exception as e:\n",
    "            print(f\"[HP][{model_key}] No se pudo leer BEST. Uso defaults. {e}\")\n",
    "    print(f\"[HP][{model_key}] Usando defaults.\")\n",
    "    return (get_xgb_defaults() if model_key==\"xgb\" else get_lgbm_defaults()), False\n",
    "\n",
    "xgb_params_seed, _ = load_best_or_default(\"xgb\")\n",
    "lgbm_params_seed, _ = load_best_or_default(\"lgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3747f83",
   "metadata": {},
   "source": [
    "7 — Entrenamiento BASELINE (XGB y LGBM) + umbral + guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f8257e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[EXPORT][XGB] XGB_REDUCED_SMOTENC_BASE -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_REDUCED_SMOTENC/export\n",
      "[OK][XGB BASE] Guardados en /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_REDUCED_SMOTENC\n",
      "[EXPORT][LGBM] LGBM_REDUCED_SMOTENC_BASE -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_REDUCED_SMOTENC/export\n",
      "[OK][LGBM BASE] Guardados en /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_REDUCED_SMOTENC\n"
     ]
    }
   ],
   "source": [
    "if \"X_train_fit\" not in globals():\n",
    "    X_train_fit, X_val_fit, X_test_fit = X_train, X_val, X_test\n",
    "\n",
    "if \"feature_names_used\" not in globals():\n",
    "    feature_names_used = feature_names\n",
    "\n",
    "if \"keep_idx_global\" not in globals():\n",
    "    keep_idx_global = None  # sin L1\n",
    "\n",
    "# Balanceo global para baseline\n",
    "X_train_final, y_train_final = X_train_fit, y_train\n",
    "if USE_BALANCED_TRAIN:\n",
    "    X_train_final, y_train_final = maybe_smote(\n",
    "        X_train_fit,\n",
    "        y_train,\n",
    "        keep_idx=keep_idx_global\n",
    "    )\n",
    "\n",
    "# ---- XGB BASELINE ----\n",
    "xgb_seed = dict(xgb_params_seed)\n",
    "xgb_seed.setdefault(\"n_estimators\", xgb_seed.get(\"n_estimators\", 1000))\n",
    "xgb_seed.setdefault(\"random_state\", RANDOM_STATE)\n",
    "xgb_seed.setdefault(\"n_jobs\", -1)\n",
    "xgb_seed.setdefault(\"eval_metric\", \"aucpr\")\n",
    "xgb_seed.setdefault(\"tree_method\", \"hist\")\n",
    "xgb_seed.setdefault(\"verbosity\", 0)\n",
    "xgb_seed.pop(\"verbose\", None)\n",
    "\n",
    "xgb_baseline_model = XGBClassifier(**xgb_seed)\n",
    "xgb_baseline_model = xgb_fit_with_es(\n",
    "    xgb_baseline_model, X_train_final, y_train_final,\n",
    "    X_val_fit, y_val, feature_names=feature_names_used,\n",
    "    rounds=200, verbose=False\n",
    ")\n",
    "\n",
    "# Validación\n",
    "proba_val_xgb = xgb_baseline_model.predict_proba(X_val_fit)[:,1]\n",
    "thr_val_xgb, best_f1_val_xgb = find_best_threshold(y_val, proba_val_xgb, metric=\"f1\")\n",
    "val_metrics_xgb = compute_all_metrics(y_val, proba_val_xgb, thr_val_xgb)\n",
    "\n",
    "# Test\n",
    "proba_test_xgb = xgb_baseline_model.predict_proba(X_test_fit)[:,1]\n",
    "y_pred_test_xgb = (proba_test_xgb >= thr_val_xgb).astype(int)\n",
    "test_metrics_xgb = compute_all_metrics(y_test, proba_test_xgb, thr_val_xgb)\n",
    "\n",
    "# === Guardados ===\n",
    "EXP_NAME_XGB = f\"XGB_{VIEW_TAG}_{BAL_TAG}\"\n",
    "bx = DIRS[\"xgb\"]\n",
    "OUT_RESULTS_X = bx / \"results\"\n",
    "OUT_FIGS_X    = bx / \"figs\"\n",
    "OUT_PREDS_X   = bx / \"preds\"\n",
    "OUT_PARAMS_X  = bx / \"best_params\"\n",
    "OUT_EXPORT_X  = bx / \"export\"\n",
    "\n",
    "# HP seed y \"fitted\"\n",
    "with open(OUT_PARAMS_X / f\"{EXP_NAME_XGB}_BASE_seed_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(xgb_seed, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(OUT_PARAMS_X / f\"{EXP_NAME_XGB}_BASE_fitted_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(xgb_baseline_model.get_params(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Plots\n",
    "plot_pr_curve(y_val,  proba_val_xgb,  f\"{EXP_NAME_XGB} — PR (val)\",  OUT_FIGS_X / f\"{EXP_NAME_XGB}_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_xgb, f\"{EXP_NAME_XGB} — PR (test)\", OUT_FIGS_X / f\"{EXP_NAME_XGB}_pr_test.png\")\n",
    "plot_roc_curve(y_val,  proba_val_xgb,  f\"{EXP_NAME_XGB} — ROC (val)\",  OUT_FIGS_X / f\"{EXP_NAME_XGB}_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_xgb, f\"{EXP_NAME_XGB} — ROC (test)\", OUT_FIGS_X / f\"{EXP_NAME_XGB}_roc_test.png\")\n",
    "plot_confusion(y_test, y_pred_test_xgb,\n",
    "               f\"{EXP_NAME_XGB} — Confusion (test @thr={thr_val_xgb:.3f})\",\n",
    "               OUT_FIGS_X / f\"{EXP_NAME_XGB}_cm_test.png\")\n",
    "\n",
    "# Importancias en el espacio L1-reducido\n",
    "try:\n",
    "    booster = xgb_baseline_model.get_booster()\n",
    "    imp_gain_x = xgb_gain_importances(booster, feature_names_used)\n",
    "except Exception:\n",
    "    imp_gain_x = getattr(xgb_baseline_model, \"feature_importances_\", np.zeros(len(feature_names_used)))\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\"feature\": feature_names_used[:len(imp_gain_x)], \"importance_gain\": imp_gain_x}\n",
    ").sort_values(\"importance_gain\", ascending=False)\\\n",
    " .to_csv(OUT_RESULTS_X / f\"{EXP_NAME_XGB}_feature_importances.csv\", index=False)\n",
    "\n",
    "# Preds test\n",
    "pd.DataFrame({\"proba\": proba_test_xgb, \"y_true\": y_test}).to_parquet(\n",
    "    OUT_PREDS_X / f\"preds_test_{EXP_NAME_XGB}.parquet\", index=False\n",
    ")\n",
    "\n",
    "# Registro baselines.csv\n",
    "best_iter_base_xgb = getattr(xgb_baseline_model, \"best_iteration\", None)\n",
    "row_base_xgb = {\n",
    "    \"model\": EXP_NAME_XGB,\n",
    "    \"thr_val\": thr_val_xgb,\n",
    "    \"val_pr_auc\": val_metrics_xgb[\"pr_auc\"],\n",
    "    \"val_roc_auc\": val_metrics_xgb[\"roc_auc\"],\n",
    "    \"val_precision\": val_metrics_xgb[\"precision\"],\n",
    "    \"val_f1\": val_metrics_xgb[\"f1\"],\n",
    "    \"val_recall\": val_metrics_xgb[\"recall\"],\n",
    "    \"val_bal_acc\": val_metrics_xgb[\"bal_acc\"],\n",
    "    \"test_pr_auc\": test_metrics_xgb[\"pr_auc\"],\n",
    "    \"test_roc_auc\": test_metrics_xgb[\"roc_auc\"],\n",
    "    \"test_precision\": test_metrics_xgb[\"precision\"],\n",
    "    \"test_f1\": test_metrics_xgb[\"f1\"],\n",
    "    \"test_recall\": test_metrics_xgb[\"recall\"],\n",
    "    \"test_bal_acc\": test_metrics_xgb[\"bal_acc\"],\n",
    "    \"best_iteration\": best_iter_base_xgb if best_iter_base_xgb is not None else np.nan\n",
    "}\n",
    "csv_x = OUT_RESULTS_X / \"baselines.csv\"\n",
    "pd.DataFrame([row_base_xgb]).to_csv(csv_x, mode=(\"a\" if csv_x.exists() else \"w\"),\n",
    "                                    index=False, header=not csv_x.exists())\n",
    "\n",
    "# === EXPORT: XGB BASE ===\n",
    "export_xgb(\n",
    "    adapter=xgb_baseline_model,\n",
    "    out_dir=OUT_EXPORT_X,\n",
    "    exp_name=f\"{EXP_NAME_XGB}_BASE\",\n",
    "    feature_names=feature_names_used,\n",
    "    threshold=thr_val_xgb,\n",
    "    val_metrics=val_metrics_xgb,\n",
    "    test_metrics=test_metrics_xgb\n",
    ")\n",
    "print(\"[OK][XGB BASE] Guardados en\", bx)\n",
    "\n",
    "# ---- LGBM BASELINE ----\n",
    "lgbm_seed = dict(lgbm_params_seed)\n",
    "lgbm_seed.setdefault(\"n_estimators\", lgbm_seed.get(\"n_estimators\", 1000))\n",
    "lgbm_seed.setdefault(\"random_state\", RANDOM_STATE)\n",
    "lgbm_seed.setdefault(\"objective\", \"binary\")\n",
    "if (\"metric\" not in lgbm_seed) or (lgbm_seed[\"metric\"] in (None, \"\", [], \"None\")):\n",
    "    lgbm_seed[\"metric\"] = \"average_precision\"\n",
    "lgbm_seed.setdefault(\"n_jobs\", -1)\n",
    "lgbm_seed.setdefault(\"verbosity\", -1)\n",
    "\n",
    "lgbm_baseline_model = LGBMClassifier(**lgbm_seed)\n",
    "lgbm_adapter = lgbm_fit_with_es(\n",
    "    lgbm_baseline_model, X_train_final, y_train_final,\n",
    "    X_val_fit, y_val, feature_names=feature_names_used,\n",
    "    rounds=200, verbose=False\n",
    ")\n",
    "\n",
    "# Validación\n",
    "proba_val_lgb = lgbm_adapter.predict_proba(X_val_fit)[:,1]\n",
    "thr_val_lgb, best_f1_val_lgb = find_best_threshold(y_val, proba_val_lgb, metric=\"f1\")\n",
    "val_metrics_lgb = compute_all_metrics(y_val, proba_val_lgb, thr_val_lgb)\n",
    "\n",
    "# Test\n",
    "proba_test_lgb = lgbm_adapter.predict_proba(X_test_fit)[:,1]\n",
    "y_pred_test_lgb = (proba_test_lgb >= thr_val_lgb).astype(int)\n",
    "test_metrics_lgb = compute_all_metrics(y_test, proba_test_lgb, thr_val_lgb)\n",
    "\n",
    "# === Guardados ===\n",
    "EXP_NAME_LGB = f\"LGBM_{VIEW_TAG}_{BAL_TAG}\"\n",
    "bl = DIRS[\"lgbm\"]\n",
    "OUT_RESULTS_L = bl / \"results\"\n",
    "OUT_FIGS_L    = bl / \"figs\"\n",
    "OUT_PREDS_L   = bl / \"preds\"\n",
    "OUT_PARAMS_L  = bl / \"best_params\"\n",
    "OUT_EXPORT_L  = bl / \"export\"\n",
    "\n",
    "with open(OUT_PARAMS_L / f\"{EXP_NAME_LGB}_BASE_seed_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(lgbm_seed, f, indent=2, ensure_ascii=False)\n",
    "with open(OUT_PARAMS_L / f\"{EXP_NAME_LGB}_BASE_fitted_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(lgbm_baseline_model.get_params(), f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Plots\n",
    "plot_pr_curve(y_val,  proba_val_lgb,  f\"{EXP_NAME_LGB} — PR (val)\",  OUT_FIGS_L / f\"{EXP_NAME_LGB}_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_lgb, f\"{EXP_NAME_LGB} — PR (test)\", OUT_FIGS_L / f\"{EXP_NAME_LGB}_pr_test.png\")\n",
    "plot_roc_curve(y_val,  proba_val_lgb,  f\"{EXP_NAME_LGB} — ROC (val)\",  OUT_FIGS_L / f\"{EXP_NAME_LGB}_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_lgb, f\"{EXP_NAME_LGB} — ROC (test)\", OUT_FIGS_L / f\"{EXP_NAME_LGB}_roc_test.png\")\n",
    "plot_confusion(y_test, y_pred_test_lgb,\n",
    "               f\"{EXP_NAME_LGB} — Confusion (test @thr_used={thr_val_lgb:.3f})\",\n",
    "               OUT_FIGS_L / f\"{EXP_NAME_LGB}_cm_test.png\")\n",
    "\n",
    "# Importancias LGBM (gain) en el espacio L1-reducido\n",
    "try:\n",
    "    imp_gain_l = lgbm_adapter.booster_.feature_importance(importance_type=\"gain\")\n",
    "except Exception:\n",
    "    imp_gain_l = np.zeros(len(feature_names_used))\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\"feature\": feature_names_used[:len(imp_gain_l)], \"importance_gain\": imp_gain_l}\n",
    ").sort_values(\"importance_gain\", ascending=False)\\\n",
    " .to_csv(OUT_RESULTS_L / f\"{EXP_NAME_LGB}_feature_importances.csv\", index=False)\n",
    "\n",
    "# Preds test\n",
    "pd.DataFrame({\"proba\": proba_test_lgb, \"y_true\": y_test})\\\n",
    "  .to_parquet(OUT_PREDS_L / f\"preds_test_{EXP_NAME_LGB}.parquet\", index=False)\n",
    "\n",
    "# Registro baselines.csv (LGBM)\n",
    "row_base_lgb = {\n",
    "    \"model\": EXP_NAME_LGB,\n",
    "    \"thr_val\": thr_val_lgb,\n",
    "    \"thr_oof\": np.nan,\n",
    "    \"thr_used\": thr_val_lgb,\n",
    "    \"val_pr_auc\": val_metrics_lgb[\"pr_auc\"],\n",
    "    \"val_roc_auc\": val_metrics_lgb[\"roc_auc\"],\n",
    "    \"val_precision\": val_metrics_lgb[\"precision\"],\n",
    "    \"val_f1\": val_metrics_lgb[\"f1\"],\n",
    "    \"val_recall\": val_metrics_lgb[\"recall\"],\n",
    "    \"val_bal_acc\": val_metrics_lgb[\"bal_acc\"],\n",
    "    \"test_pr_auc\": test_metrics_lgb[\"pr_auc\"],\n",
    "    \"test_roc_auc\": test_metrics_lgb[\"roc_auc\"],\n",
    "    \"test_precision\": test_metrics_lgb[\"precision\"],\n",
    "    \"test_f1\": test_metrics_lgb[\"f1\"],\n",
    "    \"test_recall\": test_metrics_lgb[\"recall\"],\n",
    "    \"test_bal_acc\": test_metrics_lgb[\"bal_acc\"],\n",
    "    \"best_iteration\": lgbm_adapter.best_iteration if hasattr(lgbm_adapter, \"best_iteration\") else np.nan\n",
    "}\n",
    "csv_l = OUT_RESULTS_L / \"baselines.csv\"\n",
    "pd.DataFrame([row_base_lgb]).to_csv(\n",
    "    csv_l,\n",
    "    mode=(\"a\" if csv_l.exists() else \"w\"),\n",
    "    index=False,\n",
    "    header=not csv_l.exists()\n",
    ")\n",
    "\n",
    "# === EXPORT: LGBM BASE ===\n",
    "export_lgbm(\n",
    "    adapter=lgbm_adapter,\n",
    "    out_dir=OUT_EXPORT_L,\n",
    "    exp_name=f\"{EXP_NAME_LGB}_BASE\",\n",
    "    feature_names=feature_names_used,\n",
    "    threshold=thr_val_lgb,\n",
    "    val_metrics=val_metrics_lgb,\n",
    "    test_metrics=test_metrics_lgb\n",
    ")\n",
    "print(\"[OK][LGBM BASE] Guardados en\", bl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ce5f1",
   "metadata": {},
   "source": [
    "8 — Optuna incremental (XGB y LGBM), re-entreno TUNED + guardados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6491c887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:51:34,780] A new study created in memory with name: XGB_REDUCED_SMOTENC_AP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA][XGB] Enqueue BEST anterior.\n",
      "[OPTUNA][XGB] Iniciando 40 pruebas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:51:37,865] Trial 0 finished with value: 0.6935185898120686 and parameters: {'learning_rate': 0.03306272205342358, 'n_estimators': 1750, 'max_depth': 3, 'min_child_weight': 0.8056681762980753, 'subsample': 0.8493794437375066, 'colsample_bytree': 0.8266138222376377, 'gamma': 1.1473724321013295e-06, 'reg_alpha': 5.6264972100784e-06, 'reg_lambda': 5.631463295646268e-06}. Best is trial 0 with value: 0.6935185898120686.\n",
      "[I 2025-12-12 00:51:42,162] Trial 1 finished with value: 0.6870151419929501 and parameters: {'learning_rate': 0.008468008575248327, 'n_estimators': 2900, 'max_depth': 8, 'min_child_weight': 4.550475813202184, 'subsample': 0.6624074561769746, 'colsample_bytree': 0.662397808134481, 'gamma': 3.200866785899844e-08, 'reg_alpha': 1.156732719914599, 'reg_lambda': 0.016136341713591334}. Best is trial 0 with value: 0.6935185898120686.\n",
      "[I 2025-12-12 00:51:43,462] Trial 2 finished with value: 0.6815970793795767 and parameters: {'learning_rate': 0.05675206026988748, 'n_estimators': 800, 'max_depth': 10, 'min_child_weight': 10.779361932748845, 'subsample': 0.6849356442713105, 'colsample_bytree': 0.6727299868828402, 'gamma': 3.939402261362697e-07, 'reg_alpha': 0.000134801802908908, 'reg_lambda': 0.004712973756110786}. Best is trial 0 with value: 0.6935185898120686.\n",
      "[I 2025-12-12 00:51:47,325] Trial 3 finished with value: 0.6874037558252651 and parameters: {'learning_rate': 0.01174843954800703, 'n_estimators': 1450, 'max_depth': 7, 'min_child_weight': 0.8364645453054501, 'subsample': 0.7168578594140873, 'colsample_bytree': 0.7465447373174767, 'gamma': 9.275538076980542e-05, 'reg_alpha': 0.3134958021096912, 'reg_lambda': 2.498713568466947e-05}. Best is trial 0 with value: 0.6935185898120686.\n",
      "[I 2025-12-12 00:51:50,638] Trial 4 finished with value: 0.6767858481231521 and parameters: {'learning_rate': 0.018785426399210624, 'n_estimators': 2100, 'max_depth': 3, 'min_child_weight': 4.702115628087815, 'subsample': 0.6682096494749166, 'colsample_bytree': 0.6260206371941118, 'gamma': 1.7960847528705854, 'reg_alpha': 5.746775499181867, 'reg_lambda': 0.45580746840273345}. Best is trial 0 with value: 0.6935185898120686.\n",
      "[I 2025-12-12 00:51:53,997] Trial 5 finished with value: 0.6803345681182145 and parameters: {'learning_rate': 0.0056828375585122656, 'n_estimators': 1000, 'max_depth': 8, 'min_child_weight': 2.5358333235759627, 'subsample': 0.6488152939379115, 'colsample_bytree': 0.798070764044508, 'gamma': 1.9913367728263115e-08, 'reg_alpha': 2.318690670290199, 'reg_lambda': 6.478282331897332e-05}. Best is trial 0 with value: 0.6935185898120686.\n",
      "[I 2025-12-12 00:51:55,657] Trial 6 finished with value: 0.6813013971295447 and parameters: {'learning_rate': 0.043767126303409544, 'n_estimators': 1500, 'max_depth': 7, 'min_child_weight': 3.7569262495760847, 'subsample': 0.6739417822102108, 'colsample_bytree': 0.9878338511058234, 'gamma': 0.05531681668096113, 'reg_alpha': 3.7713131110779936, 'reg_lambda': 1.8356566544355097}. Best is trial 0 with value: 0.6935185898120686.\n",
      "[I 2025-12-12 00:51:58,103] Trial 7 finished with value: 0.6930980084551537 and parameters: {'learning_rate': 0.03027406546455652, 'n_estimators': 2850, 'max_depth': 3, 'min_child_weight': 1.0302587393796305, 'subsample': 0.6180909155642152, 'colsample_bytree': 0.7301321323053057, 'gamma': 2.4048726561760165e-05, 'reg_alpha': 7.933105363733024e-05, 'reg_lambda': 0.6326486185661588}. Best is trial 0 with value: 0.6935185898120686.\n",
      "[I 2025-12-12 00:52:01,261] Trial 8 finished with value: 0.6856611302098745 and parameters: {'learning_rate': 0.0076510536667541975, 'n_estimators': 1400, 'max_depth': 7, 'min_child_weight': 0.8408897660399112, 'subsample': 0.9208787923016158, 'colsample_bytree': 0.6298202574719083, 'gamma': 3.845031120156871, 'reg_alpha': 0.2545150013091294, 'reg_lambda': 2.4604229580184192e-05}. Best is trial 0 with value: 0.6935185898120686.\n",
      "[I 2025-12-12 00:52:09,454] Trial 9 finished with value: 0.6713396538935019 and parameters: {'learning_rate': 0.0010319982330247674, 'n_estimators': 2600, 'max_depth': 8, 'min_child_weight': 7.360091638366141, 'subsample': 0.9085081386743783, 'colsample_bytree': 0.6296178606936361, 'gamma': 1.3130541002425655e-05, 'reg_alpha': 6.472669269538641e-06, 'reg_lambda': 1.100839441018132}. Best is trial 0 with value: 0.6935185898120686.\n",
      "[I 2025-12-12 00:52:13,457] Trial 10 finished with value: 0.6935626813475053 and parameters: {'learning_rate': 0.012697810627232126, 'n_estimators': 2250, 'max_depth': 3, 'min_child_weight': 0.9923096205120842, 'subsample': 0.8697981279262107, 'colsample_bytree': 0.6754602651877454, 'gamma': 4.355400178065869e-07, 'reg_alpha': 2.6592044582321092e-06, 'reg_lambda': 1.0843691173090292e-06}. Best is trial 10 with value: 0.6935626813475053.\n",
      "[I 2025-12-12 00:52:17,451] Trial 11 finished with value: 0.6880076231164611 and parameters: {'learning_rate': 0.008070296194473754, 'n_estimators': 2200, 'max_depth': 3, 'min_child_weight': 2.6483219547161814, 'subsample': 0.8643934916485243, 'colsample_bytree': 0.679474132252082, 'gamma': 2.1501760104356407e-06, 'reg_alpha': 1.516991683623085e-05, 'reg_lambda': 0.00014182648178415346}. Best is trial 10 with value: 0.6935626813475053.\n",
      "[I 2025-12-12 00:52:18,429] Trial 12 finished with value: 0.6985261084210692 and parameters: {'learning_rate': 0.11606982623590888, 'n_estimators': 1900, 'max_depth': 3, 'min_child_weight': 0.9277474205244098, 'subsample': 0.7326900127737387, 'colsample_bytree': 0.8024358862026536, 'gamma': 4.187670208393132e-06, 'reg_alpha': 3.373539017033962e-06, 'reg_lambda': 2.261756498442988e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:19,060] Trial 13 finished with value: 0.6726893190232569 and parameters: {'learning_rate': 0.24176279487647723, 'n_estimators': 2450, 'max_depth': 5, 'min_child_weight': 0.6708275376479454, 'subsample': 0.6914461437995818, 'colsample_bytree': 0.9233188920081643, 'gamma': 9.789060213098385e-08, 'reg_alpha': 0.0005256961216110502, 'reg_lambda': 5.692549946318945e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:19,620] Trial 14 finished with value: 0.6919873962237102 and parameters: {'learning_rate': 0.20126698021479306, 'n_estimators': 2550, 'max_depth': 3, 'min_child_weight': 0.7634585350282723, 'subsample': 0.7086317396933615, 'colsample_bytree': 0.7176161704560403, 'gamma': 0.014485581606904335, 'reg_alpha': 5.405314104854348e-06, 'reg_lambda': 1.0388989002868928e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:25,399] Trial 15 finished with value: 0.6842989846419192 and parameters: {'learning_rate': 0.00313575843337583, 'n_estimators': 2350, 'max_depth': 5, 'min_child_weight': 0.5362778885595021, 'subsample': 0.682399278847545, 'colsample_bytree': 0.7925845545622231, 'gamma': 1.4887230035920647e-06, 'reg_alpha': 2.218968094722013e-06, 'reg_lambda': 1.7780062124566815e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:26,044] Trial 16 finished with value: 0.6908352500078669 and parameters: {'learning_rate': 0.13169225981076757, 'n_estimators': 1150, 'max_depth': 4, 'min_child_weight': 0.7790572166057343, 'subsample': 0.6354475137593499, 'colsample_bytree': 0.758793030542456, 'gamma': 2.9234975384001515e-05, 'reg_alpha': 1.8588708840995793e-06, 'reg_lambda': 4.71843545080105e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:30,142] Trial 17 finished with value: 0.6898145209827028 and parameters: {'learning_rate': 0.008660245222379563, 'n_estimators': 2000, 'max_depth': 4, 'min_child_weight': 0.8969664935770135, 'subsample': 0.9980934128093368, 'colsample_bytree': 0.6329466982491444, 'gamma': 7.925601390244858e-07, 'reg_alpha': 0.03539437799338501, 'reg_lambda': 8.584511006046742e-05}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:30,816] Trial 18 finished with value: 0.6912273751355951 and parameters: {'learning_rate': 0.14795645519557524, 'n_estimators': 1850, 'max_depth': 5, 'min_child_weight': 4.610089655217553, 'subsample': 0.8325412578052366, 'colsample_bytree': 0.783146433327712, 'gamma': 0.0004345832150461947, 'reg_alpha': 3.177183956656123e-05, 'reg_lambda': 7.467532355206429e-05}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:31,391] Trial 19 finished with value: 0.6914761678448296 and parameters: {'learning_rate': 0.19126918246186583, 'n_estimators': 1750, 'max_depth': 4, 'min_child_weight': 0.9763696975804808, 'subsample': 0.8188299417367189, 'colsample_bytree': 0.9409649554046838, 'gamma': 0.00453829890314217, 'reg_alpha': 1.3917621279867253e-06, 'reg_lambda': 0.0007837725886307707}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:32,371] Trial 20 finished with value: 0.677490685376699 and parameters: {'learning_rate': 0.08051550603349772, 'n_estimators': 1150, 'max_depth': 8, 'min_child_weight': 0.8763357483398138, 'subsample': 0.8380899323154212, 'colsample_bytree': 0.6908838673029731, 'gamma': 6.308791100178576e-07, 'reg_alpha': 1.3119936743936843e-05, 'reg_lambda': 2.322898833800514e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:36,974] Trial 21 finished with value: 0.671522670376773 and parameters: {'learning_rate': 0.002355824874624858, 'n_estimators': 2250, 'max_depth': 4, 'min_child_weight': 0.5579741197266472, 'subsample': 0.8641682052138353, 'colsample_bytree': 0.6657766707119851, 'gamma': 1.2478257649782393e-08, 'reg_alpha': 8.913516308823712e-06, 'reg_lambda': 2.2153388909436697e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:38,447] Trial 22 finished with value: 0.6920582912792134 and parameters: {'learning_rate': 0.07552914445524782, 'n_estimators': 2050, 'max_depth': 3, 'min_child_weight': 0.7631377278824446, 'subsample': 0.8952716273215934, 'colsample_bytree': 0.7993844121900906, 'gamma': 6.514989912525714e-07, 'reg_alpha': 4.509789127497675e-06, 'reg_lambda': 1.0521680340851999e-05}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:41,111] Trial 23 finished with value: 0.6945830534611728 and parameters: {'learning_rate': 0.02369031043681103, 'n_estimators': 1600, 'max_depth': 3, 'min_child_weight': 1.1197241820221988, 'subsample': 0.7635139050764597, 'colsample_bytree': 0.8285798392898807, 'gamma': 4.7470878950255164e-08, 'reg_alpha': 0.0024234301602095174, 'reg_lambda': 0.00010436116515033634}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:42,449] Trial 24 finished with value: 0.6908362081619246 and parameters: {'learning_rate': 0.03587853630362037, 'n_estimators': 1500, 'max_depth': 3, 'min_child_weight': 2.8695443680129413, 'subsample': 0.6997113223228659, 'colsample_bytree': 0.9054295368272863, 'gamma': 7.535846443131577e-07, 'reg_alpha': 0.03942566204335108, 'reg_lambda': 0.00087636496810176}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:44,829] Trial 25 finished with value: 0.6705727861409159 and parameters: {'learning_rate': 0.004320459993966565, 'n_estimators': 1150, 'max_depth': 4, 'min_child_weight': 1.6780981537673465, 'subsample': 0.8385772511148176, 'colsample_bytree': 0.771028109672448, 'gamma': 4.2404200510239506e-07, 'reg_alpha': 0.03595465269232874, 'reg_lambda': 2.4117197833914423e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:45,644] Trial 26 finished with value: 0.6920217394512688 and parameters: {'learning_rate': 0.13274719577573155, 'n_estimators': 1900, 'max_depth': 3, 'min_child_weight': 1.4726659528685666, 'subsample': 0.8524874342347574, 'colsample_bytree': 0.7477288045768056, 'gamma': 2.702564653801397e-08, 'reg_alpha': 0.11167898057689606, 'reg_lambda': 0.004319477543196731}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:48,316] Trial 27 finished with value: 0.6849227438565642 and parameters: {'learning_rate': 0.011102651245369357, 'n_estimators': 1450, 'max_depth': 3, 'min_child_weight': 0.673744897756296, 'subsample': 0.7521892330946252, 'colsample_bytree': 0.9600657048639601, 'gamma': 6.154092953688871e-08, 'reg_alpha': 0.0009117508915906673, 'reg_lambda': 0.00030792216044173935}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:49,697] Trial 28 finished with value: 0.6889949994736929 and parameters: {'learning_rate': 0.043725498275668316, 'n_estimators': 2100, 'max_depth': 3, 'min_child_weight': 0.9928371046090696, 'subsample': 0.609488841516843, 'colsample_bytree': 0.8104841686725559, 'gamma': 1.471379507732966e-07, 'reg_alpha': 4.033161583387438e-06, 'reg_lambda': 0.00029349620945581063}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:51,546] Trial 29 finished with value: 0.6912563097791827 and parameters: {'learning_rate': 0.03487736327865941, 'n_estimators': 2200, 'max_depth': 5, 'min_child_weight': 0.5125251567189736, 'subsample': 0.8223433424675968, 'colsample_bytree': 0.6433031583517271, 'gamma': 2.2645504239308633e-07, 'reg_alpha': 1.8406402945610175e-06, 'reg_lambda': 0.0037856864282591386}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:52,902] Trial 30 finished with value: 0.6898173794378365 and parameters: {'learning_rate': 0.041420684502773614, 'n_estimators': 2050, 'max_depth': 4, 'min_child_weight': 0.857851780238528, 'subsample': 0.7527089407347713, 'colsample_bytree': 0.827841739890295, 'gamma': 9.804470958841125e-08, 'reg_alpha': 0.013907878462445065, 'reg_lambda': 3.831951802668893e-05}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:54,657] Trial 31 finished with value: 0.6917657998010575 and parameters: {'learning_rate': 0.03710226907320118, 'n_estimators': 2950, 'max_depth': 4, 'min_child_weight': 0.6003699323177996, 'subsample': 0.9189100513577054, 'colsample_bytree': 0.6241035703732626, 'gamma': 3.353548814276307e-08, 'reg_alpha': 1.316428642354495e-06, 'reg_lambda': 6.498617171583451e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:56,796] Trial 32 finished with value: 0.6941843899930263 and parameters: {'learning_rate': 0.03227743357972744, 'n_estimators': 1200, 'max_depth': 3, 'min_child_weight': 0.9129481449330331, 'subsample': 0.8463555663804057, 'colsample_bytree': 0.846791506224903, 'gamma': 0.0004268479943750196, 'reg_alpha': 3.452231264074874e-06, 'reg_lambda': 5.460795406882663e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:52:58,409] Trial 33 finished with value: 0.6908337624035108 and parameters: {'learning_rate': 0.04064694111269033, 'n_estimators': 950, 'max_depth': 3, 'min_child_weight': 0.8689420117470034, 'subsample': 0.8790993401616642, 'colsample_bytree': 0.8895196185751563, 'gamma': 0.00011922234977008154, 'reg_alpha': 5.77335863624196e-06, 'reg_lambda': 3.615263760600911e-05}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:53:01,272] Trial 34 finished with value: 0.6854946078139545 and parameters: {'learning_rate': 0.008290892001283877, 'n_estimators': 1300, 'max_depth': 4, 'min_child_weight': 1.4897979958584786, 'subsample': 0.856800434450058, 'colsample_bytree': 0.845835557666187, 'gamma': 0.07527481032207992, 'reg_alpha': 3.293711231999513e-06, 'reg_lambda': 9.242051977370772e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:53:03,656] Trial 35 finished with value: 0.6924969953160697 and parameters: {'learning_rate': 0.027697000744176303, 'n_estimators': 2100, 'max_depth': 4, 'min_child_weight': 1.8765870719964268, 'subsample': 0.9338768804650979, 'colsample_bytree': 0.6344455522017581, 'gamma': 2.3535239301289404e-07, 'reg_alpha': 3.991579965429e-06, 'reg_lambda': 1.582337251284285e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:53:08,192] Trial 36 finished with value: 0.6935052783938188 and parameters: {'learning_rate': 0.01310315284264603, 'n_estimators': 2450, 'max_depth': 3, 'min_child_weight': 2.0062256427381455, 'subsample': 0.7538947348864831, 'colsample_bytree': 0.696605805178434, 'gamma': 2.6449311452653435e-08, 'reg_alpha': 0.0002840633023098033, 'reg_lambda': 1.081242369972936e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:53:09,248] Trial 37 finished with value: 0.6944795158250094 and parameters: {'learning_rate': 0.10875896877645036, 'n_estimators': 1750, 'max_depth': 3, 'min_child_weight': 1.6426767767398, 'subsample': 0.7435118017883724, 'colsample_bytree': 0.7502664403991676, 'gamma': 1.3233040866074497e-05, 'reg_alpha': 4.308018374084047e-06, 'reg_lambda': 4.002867375746163e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:53:10,684] Trial 38 finished with value: 0.6949146271343892 and parameters: {'learning_rate': 0.0701090265151655, 'n_estimators': 1800, 'max_depth': 3, 'min_child_weight': 4.130100566470891, 'subsample': 0.7124515267681588, 'colsample_bytree': 0.7478448296460654, 'gamma': 0.0009765004989629474, 'reg_alpha': 2.238654126105545e-06, 'reg_lambda': 2.8482159400128845e-06}. Best is trial 12 with value: 0.6985261084210692.\n",
      "[I 2025-12-12 00:53:12,197] Trial 39 finished with value: 0.6823472947626036 and parameters: {'learning_rate': 0.023035797775363152, 'n_estimators': 2350, 'max_depth': 4, 'min_child_weight': 10.27163394850966, 'subsample': 0.697280643278016, 'colsample_bytree': 0.763830880037769, 'gamma': 0.010037304288557588, 'reg_alpha': 1.7666451329806634e-06, 'reg_lambda': 3.5205105775211465e-05}. Best is trial 12 with value: 0.6985261084210692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA][XGB] Mejor AP(val): 0.698526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:53:14,229] A new study created in memory with name: LGBM_REDUCED_SMOTENC_AP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPORT][XGB] XGB_REDUCED_SMOTENC_TUNED -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/XGB_REDUCED_SMOTENC/export\n",
      "[OPTUNA][LGBM] Iniciando 40 pruebas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:53:20,093] Trial 0 finished with value: 0.6754103608368109 and parameters: {'learning_rate': 0.008468008575248327, 'n_estimators': 2900, 'num_leaves': 192, 'max_depth': 7, 'min_child_samples': 35, 'subsample': 0.662397808134481, 'colsample_bytree': 0.6232334448672797, 'reg_alpha': 1.156732719914599, 'reg_lambda': 0.016136341713591334}. Best is trial 0 with value: 0.6754103608368109.\n",
      "[I 2025-12-12 00:53:22,078] Trial 1 finished with value: 0.6682844311238189 and parameters: {'learning_rate': 0.05675206026988748, 'n_estimators': 800, 'num_leaves': 249, 'max_depth': 10, 'min_child_samples': 46, 'subsample': 0.6727299868828402, 'colsample_bytree': 0.6733618039413735, 'reg_alpha': 0.000134801802908908, 'reg_lambda': 0.004712973756110786}. Best is trial 0 with value: 0.6754103608368109.\n",
      "[I 2025-12-12 00:53:29,012] Trial 2 finished with value: 0.6689840193820416 and parameters: {'learning_rate': 0.01174843954800703, 'n_estimators': 1450, 'num_leaves': 163, 'max_depth': 0, 'min_child_samples': 62, 'subsample': 0.7465447373174767, 'colsample_bytree': 0.7824279936868144, 'reg_alpha': 0.3134958021096912, 'reg_lambda': 2.498713568466947e-05}. Best is trial 0 with value: 0.6754103608368109.\n",
      "[I 2025-12-12 00:53:32,446] Trial 3 finished with value: 0.6730637695035263 and parameters: {'learning_rate': 0.018785426399210624, 'n_estimators': 2100, 'num_leaves': 27, 'max_depth': 7, 'min_child_samples': 38, 'subsample': 0.6260206371941118, 'colsample_bytree': 0.9795542149013333, 'reg_alpha': 5.746775499181867, 'reg_lambda': 0.45580746840273345}. Best is trial 0 with value: 0.6754103608368109.\n",
      "[I 2025-12-12 00:53:35,368] Trial 4 finished with value: 0.664256449071986 and parameters: {'learning_rate': 0.0056828375585122656, 'n_estimators': 1000, 'num_leaves': 180, 'max_depth': 5, 'min_child_samples': 28, 'subsample': 0.798070764044508, 'colsample_bytree': 0.6137554084460873, 'reg_alpha': 2.318690670290199, 'reg_lambda': 6.478282331897332e-05}. Best is trial 0 with value: 0.6754103608368109.\n",
      "[I 2025-12-12 00:53:37,727] Trial 5 finished with value: 0.6686299446629156 and parameters: {'learning_rate': 0.043767126303409544, 'n_estimators': 1500, 'num_leaves': 141, 'max_depth': 6, 'min_child_samples': 41, 'subsample': 0.9878338511058234, 'colsample_bytree': 0.9100531293444458, 'reg_alpha': 3.7713131110779936, 'reg_lambda': 1.8356566544355097}. Best is trial 0 with value: 0.6754103608368109.\n",
      "[I 2025-12-12 00:53:38,146] Trial 6 finished with value: 0.6325933248572931 and parameters: {'learning_rate': 0.03027406546455652, 'n_estimators': 2850, 'num_leaves': 37, 'max_depth': 1, 'min_child_samples': 13, 'subsample': 0.7301321323053057, 'colsample_bytree': 0.7554709158757928, 'reg_alpha': 7.933105363733024e-05, 'reg_lambda': 0.6326486185661588}. Best is trial 0 with value: 0.6754103608368109.\n",
      "[I 2025-12-12 00:53:43,239] Trial 7 finished with value: 0.6599918345494928 and parameters: {'learning_rate': 0.0076510536667541975, 'n_estimators': 1400, 'num_leaves': 146, 'max_depth': 0, 'min_child_samples': 162, 'subsample': 0.6298202574719083, 'colsample_bytree': 0.9947547746402069, 'reg_alpha': 0.2545150013091294, 'reg_lambda': 2.4604229580184192e-05}. Best is trial 0 with value: 0.6754103608368109.\n",
      "[I 2025-12-12 00:53:52,528] Trial 8 finished with value: 0.6506027407653329 and parameters: {'learning_rate': 0.0010319982330247674, 'n_estimators': 2600, 'num_leaves': 186, 'max_depth': 9, 'min_child_samples': 156, 'subsample': 0.6296178606936361, 'colsample_bytree': 0.7433862914177091, 'reg_alpha': 6.472669269538641e-06, 'reg_lambda': 1.100839441018132}. Best is trial 0 with value: 0.6754103608368109.\n",
      "[I 2025-12-12 00:53:54,815] Trial 9 finished with value: 0.676634859598434 and parameters: {'learning_rate': 0.03499331111708852, 'n_estimators': 1500, 'num_leaves': 31, 'max_depth': 3, 'min_child_samples': 68, 'subsample': 0.8918424713352255, 'colsample_bytree': 0.8550229885420852, 'reg_alpha': 1.6236379661338332, 'reg_lambda': 0.0020207122587167364}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:53:56,048] Trial 10 finished with value: 0.666543578979341 and parameters: {'learning_rate': 0.11023347223539438, 'n_estimators': 2600, 'num_leaves': 43, 'max_depth': 6, 'min_child_samples': 52, 'subsample': 0.9367697611567503, 'colsample_bytree': 0.8882203653538239, 'reg_alpha': 5.154369658173874, 'reg_lambda': 1.1030840549881682e-05}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:53:58,060] Trial 11 finished with value: 0.6682607110135955 and parameters: {'learning_rate': 0.0329034700028107, 'n_estimators': 3000, 'num_leaves': 214, 'max_depth': 6, 'min_child_samples': 51, 'subsample': 0.6245410001425904, 'colsample_bytree': 0.6540122666432376, 'reg_alpha': 6.735819938021255, 'reg_lambda': 5.057807719218082e-05}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:00,485] Trial 12 finished with value: 0.6698182507917761 and parameters: {'learning_rate': 0.025433800567996462, 'n_estimators': 2250, 'num_leaves': 209, 'max_depth': 9, 'min_child_samples': 51, 'subsample': 0.6656963044667092, 'colsample_bytree': 0.6974957519113263, 'reg_alpha': 0.14632534191111976, 'reg_lambda': 4.18511281170435}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:01,745] Trial 13 finished with value: 0.6752633439512964 and parameters: {'learning_rate': 0.06064989025774603, 'n_estimators': 1200, 'num_leaves': 88, 'max_depth': 3, 'min_child_samples': 103, 'subsample': 0.9599257731138221, 'colsample_bytree': 0.8743188681917087, 'reg_alpha': 0.020996013368230614, 'reg_lambda': 0.0010423350391704123}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:04,749] Trial 14 finished with value: 0.6385875252722928 and parameters: {'learning_rate': 0.003565534344235772, 'n_estimators': 2500, 'num_leaves': 167, 'max_depth': 2, 'min_child_samples': 50, 'subsample': 0.755620880750227, 'colsample_bytree': 0.7130760798498409, 'reg_alpha': 1.2693847305644554, 'reg_lambda': 0.039330810813684204}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:07,661] Trial 15 finished with value: 0.6750524037744442 and parameters: {'learning_rate': 0.02488626934660003, 'n_estimators': 2000, 'num_leaves': 71, 'max_depth': 3, 'min_child_samples': 78, 'subsample': 0.7787838649866394, 'colsample_bytree': 0.776848985264915, 'reg_alpha': 3.935752643863078, 'reg_lambda': 0.017337972439115595}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:12,081] Trial 16 finished with value: 0.6592692615589159 and parameters: {'learning_rate': 0.0017552535659640317, 'n_estimators': 2500, 'num_leaves': 210, 'max_depth': 11, 'min_child_samples': 15, 'subsample': 0.6994423124743923, 'colsample_bytree': 0.736412153938189, 'reg_alpha': 0.057707439970724625, 'reg_lambda': 0.0002514368766235338}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:13,190] Trial 17 finished with value: 0.6754285314350932 and parameters: {'learning_rate': 0.10230117386235915, 'n_estimators': 1250, 'num_leaves': 27, 'max_depth': 0, 'min_child_samples': 19, 'subsample': 0.8342805996109581, 'colsample_bytree': 0.698360956670832, 'reg_alpha': 2.057115282097494, 'reg_lambda': 5.2132191714488565e-05}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:14,078] Trial 18 finished with value: 0.6716491150070005 and parameters: {'learning_rate': 0.17346859156669983, 'n_estimators': 950, 'num_leaves': 38, 'max_depth': 3, 'min_child_samples': 59, 'subsample': 0.9249465625743422, 'colsample_bytree': 0.6418306978075857, 'reg_alpha': 7.788440034447891, 'reg_lambda': 5.62800254036465e-06}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:16,669] Trial 19 finished with value: 0.6524021309414161 and parameters: {'learning_rate': 0.0033984555598279283, 'n_estimators': 1300, 'num_leaves': 16, 'max_depth': 4, 'min_child_samples': 20, 'subsample': 0.9040430070497523, 'colsample_bytree': 0.9377519096139896, 'reg_alpha': 5.30898947027433, 'reg_lambda': 0.00022231981720687187}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:21,418] Trial 20 finished with value: 0.6758452413591997 and parameters: {'learning_rate': 0.007166359946125474, 'n_estimators': 1750, 'num_leaves': 22, 'max_depth': 0, 'min_child_samples': 102, 'subsample': 0.8693519311937868, 'colsample_bytree': 0.806231813489342, 'reg_alpha': 1.1847702386746284, 'reg_lambda': 1.0532072892889075e-06}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:26,498] Trial 21 finished with value: 0.6756291561195464 and parameters: {'learning_rate': 0.008353966090077169, 'n_estimators': 1250, 'num_leaves': 29, 'max_depth': 0, 'min_child_samples': 51, 'subsample': 0.9514087828442228, 'colsample_bytree': 0.7122273472370954, 'reg_alpha': 0.07569455751716024, 'reg_lambda': 1.3009057500658386e-05}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:27,251] Trial 22 finished with value: 0.6356521006415282 and parameters: {'learning_rate': 0.016291049309084232, 'n_estimators': 1950, 'num_leaves': 19, 'max_depth': 2, 'min_child_samples': 140, 'subsample': 0.9856389004087596, 'colsample_bytree': 0.8104151680148726, 'reg_alpha': 0.2116921156603468, 'reg_lambda': 3.712812861289967e-06}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:28,047] Trial 23 finished with value: 0.6327709038357863 and parameters: {'learning_rate': 0.009097519428224338, 'n_estimators': 1550, 'num_leaves': 50, 'max_depth': 1, 'min_child_samples': 34, 'subsample': 0.984644436826069, 'colsample_bytree': 0.7099247637926462, 'reg_alpha': 0.005378123285929923, 'reg_lambda': 2.971942836871297e-06}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:28,356] Trial 24 finished with value: 0.4401811316013492 and parameters: {'learning_rate': 0.0014512406101732096, 'n_estimators': 1850, 'num_leaves': 61, 'max_depth': 1, 'min_child_samples': 121, 'subsample': 0.8622273146683707, 'colsample_bytree': 0.8974879186105357, 'reg_alpha': 0.28188061345589155, 'reg_lambda': 9.224833058265547e-06}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:29,275] Trial 25 finished with value: 0.6097256817692187 and parameters: {'learning_rate': 0.003176617846171557, 'n_estimators': 1200, 'num_leaves': 101, 'max_depth': 1, 'min_child_samples': 142, 'subsample': 0.8300517923984586, 'colsample_bytree': 0.7250442271956347, 'reg_alpha': 6.126079340152215, 'reg_lambda': 1.2862941567436549e-05}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:32,893] Trial 26 finished with value: 0.6706903964422564 and parameters: {'learning_rate': 0.00849795872653911, 'n_estimators': 2600, 'num_leaves': 50, 'max_depth': 3, 'min_child_samples': 118, 'subsample': 0.7752871688159173, 'colsample_bytree': 0.6717289149696688, 'reg_alpha': 0.1001953733411522, 'reg_lambda': 7.625255591079029e-06}. Best is trial 9 with value: 0.676634859598434.\n",
      "[I 2025-12-12 00:54:34,070] Trial 27 finished with value: 0.6796774089263481 and parameters: {'learning_rate': 0.053060418525219775, 'n_estimators': 1050, 'num_leaves': 52, 'max_depth': 3, 'min_child_samples': 61, 'subsample': 0.7758986374640798, 'colsample_bytree': 0.9412046048882953, 'reg_alpha': 0.17031090684833508, 'reg_lambda': 0.0009328142525671588}. Best is trial 27 with value: 0.6796774089263481.\n",
      "[I 2025-12-12 00:54:36,588] Trial 28 finished with value: 0.6720987854371948 and parameters: {'learning_rate': 0.01944690625332452, 'n_estimators': 1000, 'num_leaves': 30, 'max_depth': 8, 'min_child_samples': 50, 'subsample': 0.6706619541347332, 'colsample_bytree': 0.9645706610987205, 'reg_alpha': 0.016461087237330797, 'reg_lambda': 0.0014814568781550302}. Best is trial 27 with value: 0.6796774089263481.\n",
      "[I 2025-12-12 00:54:37,071] Trial 29 finished with value: 0.6329613585610379 and parameters: {'learning_rate': 0.05645312347362452, 'n_estimators': 1550, 'num_leaves': 20, 'max_depth': 1, 'min_child_samples': 29, 'subsample': 0.8431780530022338, 'colsample_bytree': 0.9105387330594645, 'reg_alpha': 0.005412541649476319, 'reg_lambda': 0.0005840533027418544}. Best is trial 27 with value: 0.6796774089263481.\n",
      "[I 2025-12-12 00:54:38,709] Trial 30 finished with value: 0.6713425004391471 and parameters: {'learning_rate': 0.031189237677633614, 'n_estimators': 1000, 'num_leaves': 109, 'max_depth': 5, 'min_child_samples': 63, 'subsample': 0.7642597481733296, 'colsample_bytree': 0.9182293385873297, 'reg_alpha': 0.5149998035616743, 'reg_lambda': 0.00024373593952161298}. Best is trial 27 with value: 0.6796774089263481.\n",
      "[I 2025-12-12 00:54:40,323] Trial 31 finished with value: 0.6820218026680606 and parameters: {'learning_rate': 0.04163472304640577, 'n_estimators': 900, 'num_leaves': 20, 'max_depth': 4, 'min_child_samples': 33, 'subsample': 0.8509340279497106, 'colsample_bytree': 0.802267100716837, 'reg_alpha': 0.5255105942888963, 'reg_lambda': 0.003706120281152843}. Best is trial 31 with value: 0.6820218026680606.\n",
      "[I 2025-12-12 00:54:41,439] Trial 32 finished with value: 0.6671038673198102 and parameters: {'learning_rate': 0.04417865319358998, 'n_estimators': 1000, 'num_leaves': 28, 'max_depth': 2, 'min_child_samples': 49, 'subsample': 0.8874453803120793, 'colsample_bytree': 0.7603446741481734, 'reg_alpha': 0.30990048996750125, 'reg_lambda': 0.18819825873016544}. Best is trial 31 with value: 0.6820218026680606.\n",
      "[I 2025-12-12 00:54:42,205] Trial 33 finished with value: 0.664267329283527 and parameters: {'learning_rate': 0.13996305518863264, 'n_estimators': 1200, 'num_leaves': 26, 'max_depth': 5, 'min_child_samples': 77, 'subsample': 0.9079917658759036, 'colsample_bytree': 0.9515720246324376, 'reg_alpha': 1.429190030748827, 'reg_lambda': 0.0064798933432722305}. Best is trial 31 with value: 0.6820218026680606.\n",
      "[I 2025-12-12 00:54:43,302] Trial 34 finished with value: 0.6785834399115686 and parameters: {'learning_rate': 0.05584401166710457, 'n_estimators': 900, 'num_leaves': 21, 'max_depth': 6, 'min_child_samples': 17, 'subsample': 0.7320507948002924, 'colsample_bytree': 0.6534938608399666, 'reg_alpha': 0.6053727068935864, 'reg_lambda': 0.037597062060635685}. Best is trial 31 with value: 0.6820218026680606.\n",
      "[I 2025-12-12 00:54:44,824] Trial 35 finished with value: 0.6762496379103723 and parameters: {'learning_rate': 0.048848512854734884, 'n_estimators': 850, 'num_leaves': 19, 'max_depth': 6, 'min_child_samples': 12, 'subsample': 0.6528765046987689, 'colsample_bytree': 0.7241626544434137, 'reg_alpha': 0.4799213230017333, 'reg_lambda': 0.1510107900321997}. Best is trial 31 with value: 0.6820218026680606.\n",
      "[I 2025-12-12 00:54:47,430] Trial 36 finished with value: 0.6810717643248159 and parameters: {'learning_rate': 0.03582338934405422, 'n_estimators': 1100, 'num_leaves': 68, 'max_depth': 7, 'min_child_samples': 29, 'subsample': 0.7972280525834896, 'colsample_bytree': 0.6113894529444106, 'reg_alpha': 0.15782821220447932, 'reg_lambda': 0.005918049076923023}. Best is trial 31 with value: 0.6820218026680606.\n",
      "[I 2025-12-12 00:54:50,124] Trial 37 finished with value: 0.6783839872930982 and parameters: {'learning_rate': 0.040135559054015175, 'n_estimators': 1100, 'num_leaves': 109, 'max_depth': 9, 'min_child_samples': 31, 'subsample': 0.8596955014514878, 'colsample_bytree': 0.692848895690173, 'reg_alpha': 0.002799711791496803, 'reg_lambda': 0.0002332469732180985}. Best is trial 31 with value: 0.6820218026680606.\n",
      "[I 2025-12-12 00:54:50,712] Trial 38 finished with value: 0.6684585952909046 and parameters: {'learning_rate': 0.2733124269893132, 'n_estimators': 900, 'num_leaves': 82, 'max_depth': 4, 'min_child_samples': 16, 'subsample': 0.6620374725994462, 'colsample_bytree': 0.6002616031374342, 'reg_alpha': 0.09964966811183917, 'reg_lambda': 0.003444183970132376}. Best is trial 31 with value: 0.6820218026680606.\n",
      "[I 2025-12-12 00:54:52,077] Trial 39 finished with value: 0.6743800976514248 and parameters: {'learning_rate': 0.082060854797907, 'n_estimators': 1200, 'num_leaves': 49, 'max_depth': 9, 'min_child_samples': 82, 'subsample': 0.7975444959401509, 'colsample_bytree': 0.6495221334140056, 'reg_alpha': 0.09179019807503884, 'reg_lambda': 0.08281705013252982}. Best is trial 31 with value: 0.6820218026680606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTUNA][LGBM] Mejor AP(val): 0.682022\n",
      "[EXPORT][LGBM] LGBM_REDUCED_SMOTENC_TUNED -> /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/LGBM_REDUCED_SMOTENC/export\n"
     ]
    }
   ],
   "source": [
    "def tune_xgb_with_optuna(seed_params, X_train_final, y_train_final, X_val_fit, y_val, feature_names_used):\n",
    "    \"\"\"\n",
    "    Tuning de XGBoost maximizando AP(val) con Optuna.\n",
    "    Devuelve:\n",
    "      - adapter entrenado (XGBAdapter)\n",
    "      - best_params (dict con hiperparámetros finales)\n",
    "    \"\"\"\n",
    "    N_TRIALS = 40\n",
    "    STUDY_NAME = f\"XGB_{VIEW_TAG}_{BAL_TAG}_AP\"\n",
    "    sampler = TPESampler(seed=RANDOM_STATE, multivariate=True, group=False)\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                study_name=STUDY_NAME,\n",
    "                                sampler=sampler)\n",
    "\n",
    "    SEARCH_KEYS = [\n",
    "        \"learning_rate\", \"n_estimators\", \"max_depth\", \"min_child_weight\",\n",
    "        \"subsample\", \"colsample_bytree\", \"gamma\", \"reg_alpha\", \"reg_lambda\"\n",
    "    ]\n",
    "\n",
    "    best_fp = _best_file(\"xgb\")\n",
    "    if best_fp.exists():\n",
    "        try:\n",
    "            prev = json.loads(best_fp.read_text())\n",
    "            warm = {k: prev[k] for k in SEARCH_KEYS if k in prev}\n",
    "            if warm:\n",
    "                print(\"[OPTUNA][XGB] Enqueue BEST anterior.\")\n",
    "                study.enqueue_trial(warm)\n",
    "        except Exception as e:\n",
    "            print(\"[OPTUNA][XGB] Aviso warm-start:\", e)\n",
    "\n",
    "    def suggest(trial):\n",
    "        p = {}\n",
    "        p[\"learning_rate\"]    = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "        p[\"n_estimators\"]     = trial.suggest_int(\"n_estimators\", 800, 3000, step=50)\n",
    "        p[\"max_depth\"]        = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "        p[\"min_child_weight\"] = trial.suggest_float(\"min_child_weight\", 0.5, 20.0, log=True)\n",
    "        p[\"subsample\"]        = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        p[\"colsample_bytree\"] = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
    "        p[\"gamma\"]            = trial.suggest_float(\"gamma\", 1e-8, 5.0, log=True)\n",
    "        p[\"reg_alpha\"]        = trial.suggest_float(\"reg_alpha\", 1e-6, 10.0, log=True)\n",
    "        p[\"reg_lambda\"]       = trial.suggest_float(\"reg_lambda\", 1e-6, 10.0, log=True)\n",
    "        # Fijos\n",
    "        p[\"random_state\"]     = RANDOM_STATE\n",
    "        p[\"n_jobs\"]           = -1\n",
    "        p[\"eval_metric\"]      = \"aucpr\"\n",
    "        p[\"tree_method\"]      = \"hist\"\n",
    "        p[\"verbosity\"]        = 0\n",
    "        return p\n",
    "\n",
    "    def objective(trial):\n",
    "        hp = suggest(trial)\n",
    "        mdl = XGBClassifier(**{**seed_params, **hp})\n",
    "        mdl = xgb_fit_with_es(\n",
    "            mdl,\n",
    "            X_train_final, y_train_final,\n",
    "            X_val_fit, y_val,\n",
    "            feature_names=feature_names_used,\n",
    "            rounds=200,\n",
    "            verbose=False\n",
    "        )\n",
    "        proba_val = mdl.predict_proba(X_val_fit)[:, 1]\n",
    "        ap = average_precision_score(y_val, proba_val)\n",
    "        trial.set_user_attr(\"best_iteration\", getattr(mdl, \"best_iteration\", None))\n",
    "        return ap\n",
    "\n",
    "    print(f\"[OPTUNA][XGB] Iniciando {N_TRIALS} pruebas...\")\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "    best = study.best_trial\n",
    "    print(f\"[OPTUNA][XGB] Mejor AP(val): {best.value:.6f}\")\n",
    "\n",
    "    best_params = dict(best.params)\n",
    "    best_params.update({\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"n_jobs\": -1,\n",
    "        \"eval_metric\": \"aucpr\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"verbosity\": 0,\n",
    "    })\n",
    "\n",
    "    # Guardamos BEST global\n",
    "    with open(best_fp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Re-entrenamos con los mejores hiperparámetros\n",
    "    tuned_clf = XGBClassifier(**{**seed_params, **best_params})\n",
    "    tuned_adapter = xgb_fit_with_es(\n",
    "        tuned_clf,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names=feature_names_used,\n",
    "        rounds=200,\n",
    "        verbose=False\n",
    "    )\n",
    "    return tuned_adapter, best_params\n",
    "\n",
    "\n",
    "def tune_lgbm_with_optuna(seed_params, X_train_final, y_train_final, X_val_fit, y_val, feature_names_used):\n",
    "    N_TRIALS = 40\n",
    "    STUDY_NAME = f\"LGBM_{VIEW_TAG}_{BAL_TAG}_AP\"\n",
    "    sampler = TPESampler(seed=RANDOM_STATE, multivariate=True, group=False)\n",
    "    study = optuna.create_study(direction=\"maximize\",\n",
    "                                study_name=STUDY_NAME,\n",
    "                                sampler=sampler)\n",
    "\n",
    "    SEARCH_KEYS = [\n",
    "        \"learning_rate\", \"n_estimators\", \"num_leaves\", \"max_depth\",\n",
    "        \"min_child_samples\", \"subsample\", \"colsample_bytree\",\n",
    "        \"reg_alpha\", \"reg_lambda\"\n",
    "    ]\n",
    "\n",
    "    best_fp = _best_file(\"lgbm\")\n",
    "    if best_fp.exists():\n",
    "        try:\n",
    "            prev = json.loads(best_fp.read_text())\n",
    "            warm = {k: prev[k] for k in SEARCH_KEYS if k in prev}\n",
    "            if warm:\n",
    "                print(\"[OPTUNA][LGBM] Enqueue BEST anterior.\")\n",
    "                study.enqueue_trial(warm)\n",
    "        except Exception as e:\n",
    "            print(\"[OPTUNA][LGBM] Aviso warm-start:\", e)\n",
    "\n",
    "    def suggest(trial):\n",
    "        p = {}\n",
    "        p[\"learning_rate\"]     = trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True)\n",
    "        p[\"n_estimators\"]      = trial.suggest_int(\"n_estimators\", 800, 3000, step=50)\n",
    "        p[\"num_leaves\"]        = trial.suggest_int(\"num_leaves\", 16, 256)\n",
    "        p[\"max_depth\"]         = trial.suggest_int(\"max_depth\", -1, 12)\n",
    "        p[\"min_child_samples\"] = trial.suggest_int(\"min_child_samples\", 5, 200)\n",
    "        p[\"subsample\"]         = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "        p[\"colsample_bytree\"]  = trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
    "        p[\"reg_alpha\"]         = trial.suggest_float(\"reg_alpha\", 1e-6, 10.0, log=True)\n",
    "        p[\"reg_lambda\"]        = trial.suggest_float(\"reg_lambda\", 1e-6, 10.0, log=True)\n",
    "        return p\n",
    "\n",
    "    def objective(trial):\n",
    "        hp = suggest(trial)\n",
    "        params = {**seed_params, **hp}\n",
    "        params.setdefault(\"objective\", \"binary\")\n",
    "        params.setdefault(\"metric\", \"average_precision\")\n",
    "        params.setdefault(\"random_state\", RANDOM_STATE)\n",
    "        params.setdefault(\"n_jobs\", -1)\n",
    "\n",
    "        base_clf = LGBMClassifier(**params)\n",
    "        adapter = lgbm_fit_with_es(\n",
    "            base_clf,\n",
    "            X_train_final, y_train_final,\n",
    "            X_val_fit, y_val,\n",
    "            feature_names=feature_names_used,\n",
    "            rounds=200,\n",
    "            verbose=False\n",
    "        )\n",
    "        proba_val = adapter.predict_proba(X_val_fit)[:, 1]\n",
    "        ap = average_precision_score(y_val, proba_val)\n",
    "        trial.set_user_attr(\"best_iteration\", adapter.best_iteration)\n",
    "        return ap\n",
    "\n",
    "    print(f\"[OPTUNA][LGBM] Iniciando {N_TRIALS} pruebas...\")\n",
    "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "    best = study.best_trial\n",
    "    print(f\"[OPTUNA][LGBM] Mejor AP(val): {best.value:.6f}\")\n",
    "\n",
    "    best_params = dict(best.params)\n",
    "    best_params.update({\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"average_precision\",\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"n_jobs\": -1,\n",
    "    })\n",
    "\n",
    "    # Guardamos BEST global\n",
    "    with open(best_fp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    tuned_base = LGBMClassifier(**{**seed_params, **best_params})\n",
    "    tuned_adapter = lgbm_fit_with_es(\n",
    "        tuned_base,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names=feature_names_used,\n",
    "        rounds=200,\n",
    "        verbose=False\n",
    "    )\n",
    "    return tuned_adapter, best_params\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#   EJECUCIÓN DEL TUNING + RE-ENTRENO + GUARDADO DE preds_val_*\n",
    "# ============================================================\n",
    "\n",
    "# Por si acaso, definimos nombres base\n",
    "EXP_NAME_XGB = f\"XGB_{VIEW_TAG}_{BAL_TAG}\"\n",
    "EXP_NAME_LGB = f\"LGBM_{VIEW_TAG}_{BAL_TAG}\"\n",
    "\n",
    "# Directorios para XGB\n",
    "bx = DIRS[\"xgb\"]\n",
    "OUT_RESULTS_X = bx / \"results\"\n",
    "OUT_FIGS_X    = bx / \"figs\"\n",
    "OUT_PREDS_X   = bx / \"preds\"\n",
    "OUT_PARAMS_X  = bx / \"best_params\"\n",
    "OUT_EXPORT_X  = bx / \"export\"\n",
    "\n",
    "# Directorios para LGBM\n",
    "bl = DIRS[\"lgbm\"]\n",
    "OUT_RESULTS_L = bl / \"results\"\n",
    "OUT_FIGS_L    = bl / \"figs\"\n",
    "OUT_PREDS_L   = bl / \"preds\"\n",
    "OUT_PARAMS_L  = bl / \"best_params\"\n",
    "OUT_EXPORT_L  = bl / \"export\"\n",
    "\n",
    "# === XGB TUNED ===\n",
    "if DO_TUNE_XGB:\n",
    "    xgb_tuned_adapter, xgb_best_params = tune_xgb_with_optuna(\n",
    "        xgb_params_seed,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names_used\n",
    "    )\n",
    "    xgb_tuned_model = xgb_tuned_adapter  \n",
    "\n",
    "    # Predicciones y métricas en VAL\n",
    "    proba_val_xgb_tuned = xgb_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "    thr_val_xgb_tuned, _ = find_best_threshold(y_val, proba_val_xgb_tuned, metric=\"f1\")\n",
    "    val_metrics_xgb_tuned = compute_all_metrics(y_val, proba_val_xgb_tuned, thr_val_xgb_tuned)\n",
    "\n",
    "    # Predicciones y métricas en TEST\n",
    "    proba_test_xgb_tuned = xgb_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "    test_metrics_xgb_tuned = compute_all_metrics(y_test, proba_test_xgb_tuned, thr_val_xgb_tuned)\n",
    "    y_pred_test_xgb_tuned = (proba_test_xgb_tuned >= thr_val_xgb_tuned).astype(int)\n",
    "\n",
    "    # --- GUARDAMOS preds_val_* y preds_test_* ---\n",
    "    EXP_NAME_XGB_TUNED = f\"{EXP_NAME_XGB}_TUNED\"\n",
    "\n",
    "    pd.DataFrame({\"proba\": proba_val_xgb_tuned, \"y_true\": y_val}).to_parquet(\n",
    "        OUT_PREDS_X / f\"preds_val_{EXP_NAME_XGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame({\"proba\": proba_test_xgb_tuned, \"y_true\": y_test}).to_parquet(\n",
    "        OUT_PREDS_X / f\"preds_test_{EXP_NAME_XGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # Guardamos best_params específicos de TUNED\n",
    "    with open(OUT_PARAMS_X / f\"{EXP_NAME_XGB_TUNED}_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(xgb_best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Plots TUNED\n",
    "    plot_pr_curve(y_val,  proba_val_xgb_tuned,\n",
    "                  f\"{EXP_NAME_XGB_TUNED} — PR (val)\",\n",
    "                  OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_pr_val.png\")\n",
    "    plot_pr_curve(y_test, proba_test_xgb_tuned,\n",
    "                  f\"{EXP_NAME_XGB_TUNED} — PR (test)\",\n",
    "                  OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_pr_test.png\")\n",
    "    plot_roc_curve(y_val,  proba_val_xgb_tuned,\n",
    "                   f\"{EXP_NAME_XGB_TUNED} — ROC (val)\",\n",
    "                   OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_roc_val.png\")\n",
    "    plot_roc_curve(y_test, proba_test_xgb_tuned,\n",
    "                   f\"{EXP_NAME_XGB_TUNED} — ROC (test)\",\n",
    "                   OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_roc_test.png\")\n",
    "    plot_confusion(\n",
    "        y_test,\n",
    "        y_pred_test_xgb_tuned,\n",
    "        f\"{EXP_NAME_XGB_TUNED} — Confusion (test @thr={thr_val_xgb_tuned:.3f})\",\n",
    "        OUT_FIGS_X / f\"{EXP_NAME_XGB_TUNED}_cm_test.png\"\n",
    "    )\n",
    "\n",
    "    # Registro tuned.csv\n",
    "    best_iter_tuned_xgb = getattr(xgb_tuned_adapter, \"best_iteration\", None)\n",
    "    row_tuned_xgb = {\n",
    "        \"model\": EXP_NAME_XGB_TUNED,\n",
    "        \"thr_val\": thr_val_xgb_tuned,\n",
    "        \"val_pr_auc\": val_metrics_xgb_tuned[\"pr_auc\"],\n",
    "        \"val_roc_auc\": val_metrics_xgb_tuned[\"roc_auc\"],\n",
    "        \"val_precision\": val_metrics_xgb_tuned[\"precision\"],\n",
    "        \"val_f1\": val_metrics_xgb_tuned[\"f1\"],\n",
    "        \"val_recall\": val_metrics_xgb_tuned[\"recall\"],\n",
    "        \"val_bal_acc\": val_metrics_xgb_tuned[\"bal_acc\"],\n",
    "        \"test_pr_auc\": test_metrics_xgb_tuned[\"pr_auc\"],\n",
    "        \"test_roc_auc\": test_metrics_xgb_tuned[\"roc_auc\"],\n",
    "        \"test_precision\": test_metrics_xgb_tuned[\"precision\"],\n",
    "        \"test_f1\": test_metrics_xgb_tuned[\"f1\"],\n",
    "        \"test_recall\": test_metrics_xgb_tuned[\"recall\"],\n",
    "        \"test_bal_acc\": test_metrics_xgb_tuned[\"bal_acc\"],\n",
    "        \"best_iteration\": best_iter_tuned_xgb if best_iter_tuned_xgb is not None else np.nan,\n",
    "    }\n",
    "    csv_x_tuned = OUT_RESULTS_X / \"tuned.csv\"\n",
    "    pd.DataFrame([row_tuned_xgb]).to_csv(\n",
    "        csv_x_tuned,\n",
    "        mode=(\"a\" if csv_x_tuned.exists() else \"w\"),\n",
    "        index=False,\n",
    "        header=not csv_x_tuned.exists()\n",
    "    )\n",
    "\n",
    "    # Export XGB TUNED\n",
    "    export_xgb(\n",
    "        adapter=xgb_tuned_adapter,\n",
    "        out_dir=OUT_EXPORT_X,\n",
    "        exp_name=EXP_NAME_XGB_TUNED,\n",
    "        feature_names=feature_names_used,\n",
    "        threshold=thr_val_xgb_tuned,\n",
    "        val_metrics=val_metrics_xgb_tuned,\n",
    "        test_metrics=test_metrics_xgb_tuned,\n",
    "    )\n",
    "\n",
    "\n",
    "# === LGBM TUNED ===\n",
    "if DO_TUNE_LGBM:\n",
    "    lgbm_tuned_adapter, lgbm_best_params = tune_lgbm_with_optuna(\n",
    "        lgbm_params_seed,\n",
    "        X_train_final, y_train_final,\n",
    "        X_val_fit, y_val,\n",
    "        feature_names_used\n",
    "    )\n",
    "    # Alias de compatibilidad\n",
    "    lgbm_tuned_model = lgbm_tuned_adapter  \n",
    "\n",
    "    # Predicciones y métricas en VAL\n",
    "    proba_val_lgb_tuned = lgbm_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "    thr_val_lgb_tuned, _ = find_best_threshold(y_val, proba_val_lgb_tuned, metric=\"f1\")\n",
    "    val_metrics_lgb_tuned = compute_all_metrics(y_val, proba_val_lgb_tuned, thr_val_lgb_tuned)\n",
    "\n",
    "    # Predicciones y métricas en TEST\n",
    "    proba_test_lgb_tuned = lgbm_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "    test_metrics_lgb_tuned = compute_all_metrics(y_test, proba_test_lgb_tuned, thr_val_lgb_tuned)\n",
    "    y_pred_test_lgb_tuned = (proba_test_lgb_tuned >= thr_val_lgb_tuned).astype(int)\n",
    "\n",
    "    EXP_NAME_LGB_TUNED = f\"{EXP_NAME_LGB}_TUNED\"\n",
    "\n",
    "    pd.DataFrame({\"proba\": proba_val_lgb_tuned, \"y_true\": y_val}).to_parquet(\n",
    "        OUT_PREDS_L / f\"preds_val_{EXP_NAME_LGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "    pd.DataFrame({\"proba\": proba_test_lgb_tuned, \"y_true\": y_test}).to_parquet(\n",
    "        OUT_PREDS_L / f\"preds_test_{EXP_NAME_LGB_TUNED}.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    with open(OUT_PARAMS_L / f\"{EXP_NAME_LGB_TUNED}_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(lgbm_best_params, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Plots TUNED\n",
    "    plot_pr_curve(y_val,  proba_val_lgb_tuned,\n",
    "                  f\"{EXP_NAME_LGB_TUNED} — PR (val)\",\n",
    "                  OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_pr_val.png\")\n",
    "    plot_pr_curve(y_test, proba_test_lgb_tuned,\n",
    "                  f\"{EXP_NAME_LGB_TUNED} — PR (test)\",\n",
    "                  OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_pr_test.png\")\n",
    "    plot_roc_curve(y_val,  proba_val_lgb_tuned,\n",
    "                   f\"{EXP_NAME_LGB_TUNED} — ROC (val)\",\n",
    "                   OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_roc_val.png\")\n",
    "    plot_roc_curve(y_test, proba_test_lgb_tuned,\n",
    "                   f\"{EXP_NAME_LGB_TUNED} — ROC (test)\",\n",
    "                   OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_roc_test.png\")\n",
    "    plot_confusion(\n",
    "        y_test,\n",
    "        y_pred_test_lgb_tuned,\n",
    "        f\"{EXP_NAME_LGB_TUNED} — Confusion (test @thr={thr_val_lgb_tuned:.3f})\",\n",
    "        OUT_FIGS_L / f\"{EXP_NAME_LGB_TUNED}_cm_test.png\"\n",
    "    )\n",
    "\n",
    "    # Registro tuned.csv\n",
    "    row_tuned_lgb = {\n",
    "        \"model\": EXP_NAME_LGB_TUNED,\n",
    "        \"thr_val\": thr_val_lgb_tuned,\n",
    "        \"val_pr_auc\": val_metrics_lgb_tuned[\"pr_auc\"],\n",
    "        \"val_roc_auc\": val_metrics_lgb_tuned[\"roc_auc\"],\n",
    "        \"val_precision\": val_metrics_lgb_tuned[\"precision\"],\n",
    "        \"val_f1\": val_metrics_lgb_tuned[\"f1\"],\n",
    "        \"val_recall\": val_metrics_lgb_tuned[\"recall\"],\n",
    "        \"val_bal_acc\": val_metrics_lgb_tuned[\"bal_acc\"],\n",
    "        \"test_pr_auc\": test_metrics_lgb_tuned[\"pr_auc\"],\n",
    "        \"test_roc_auc\": test_metrics_lgb_tuned[\"roc_auc\"],\n",
    "        \"test_precision\": test_metrics_lgb_tuned[\"precision\"],\n",
    "        \"test_f1\": test_metrics_lgb_tuned[\"f1\"],\n",
    "        \"test_recall\": test_metrics_lgb_tuned[\"recall\"],\n",
    "        \"test_bal_acc\": test_metrics_lgb_tuned[\"bal_acc\"],\n",
    "        \"best_iteration\": lgbm_tuned_adapter.best_iteration if hasattr(lgbm_tuned_adapter, \"best_iteration\") else np.nan,\n",
    "    }\n",
    "    csv_l_tuned = OUT_RESULTS_L / \"tuned.csv\"\n",
    "    pd.DataFrame([row_tuned_lgb]).to_csv(\n",
    "        csv_l_tuned,\n",
    "        mode=(\"a\" if csv_l_tuned.exists() else \"w\"),\n",
    "        index=False,\n",
    "        header=not csv_l_tuned.exists()\n",
    "    )\n",
    "\n",
    "    # Export LGBM TUNED\n",
    "    export_lgbm(\n",
    "        adapter=lgbm_tuned_adapter,\n",
    "        out_dir=OUT_EXPORT_L,\n",
    "        exp_name=EXP_NAME_LGB_TUNED,\n",
    "        feature_names=feature_names_used,\n",
    "        threshold=thr_val_lgb_tuned,\n",
    "        val_metrics=val_metrics_lgb_tuned,\n",
    "        test_metrics=test_metrics_lgb_tuned,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94dfcfe",
   "metadata": {},
   "source": [
    "9 — Ensemble híbrido (stacking + soft voting XGB + LGBM + DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc70a583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SOFT VOTING (val) ===\n",
      "{'pr_auc': 0.6985826556926243, 'roc_auc': 0.8611199797640476, 'precision': 0.6728232189973615, 'f1': 0.648854961832061, 'recall': 0.6265356265356266, 'bal_acc': 0.7743475370594015}\n",
      "\n",
      "=== SOFT VOTING (test) ===\n",
      "{'pr_auc': 0.7159912908961111, 'roc_auc': 0.8666601886940869, 'precision': 0.6384615384615384, 'f1': 0.6248431618569636, 'recall': 0.6117936117936118, 'bal_acc': 0.7616406853694989}\n",
      "\n",
      "=== STACKING (val) ===\n",
      "{'pr_auc': 0.7004850930790464, 'roc_auc': 0.8617724041452856, 'precision': 0.6899441340782123, 'f1': 0.6457516339869281, 'recall': 0.6068796068796068, 'bal_acc': 0.7685998787693703}\n",
      "\n",
      "=== STACKING (test) ===\n",
      "{'pr_auc': 0.7155778532129755, 'roc_auc': 0.8667974600177991, 'precision': 0.6648351648351648, 'f1': 0.6277561608300908, 'recall': 0.5945945945945946, 'bal_acc': 0.7590047674793438}\n",
      "\n",
      "[OK][ENSEMBLE] Métricas, preds y pesos del modelo híbrido guardados en:\n",
      "  RESULTS: /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_REDUCED_SMOTENC/results\n",
      "  PREDS:   /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_REDUCED_SMOTENC/preds\n",
      "  PARAMS:  /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/ENS_REDUCED_SMOTENC/best_params\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "EXP_NAME_ENS = f\"ENS_{VIEW_TAG}_{BAL_TAG}\"\n",
    "be = DIRS[\"ens\"]\n",
    "OUT_RESULTS_E = be / \"results\"\n",
    "OUT_FIGS_E    = be / \"figs\"\n",
    "OUT_PREDS_E   = be / \"preds\"\n",
    "OUT_PARAMS_E  = be / \"best_params\"\n",
    "\n",
    "OUT_RESULTS_E.mkdir(parents=True, exist_ok=True)\n",
    "OUT_FIGS_E.mkdir(parents=True, exist_ok=True)\n",
    "OUT_PREDS_E.mkdir(parents=True, exist_ok=True)\n",
    "OUT_PARAMS_E.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "proba_val_xgb_tuned  = xgb_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "proba_test_xgb_tuned = xgb_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "\n",
    "proba_val_lgb_tuned  = lgbm_tuned_adapter.predict_proba(X_val_fit)[:, 1]\n",
    "proba_test_lgb_tuned = lgbm_tuned_adapter.predict_proba(X_test_fit)[:, 1]\n",
    "\n",
    "dl_preds_val_path  = DIRS[\"dl\"] / \"preds\" / f\"preds_val_DL_{VIEW_TAG}_{BAL_TAG}.parquet\"\n",
    "dl_preds_test_path = DIRS[\"dl\"] / \"preds\" / f\"preds_test_DL_{VIEW_TAG}_{BAL_TAG}.parquet\"\n",
    "\n",
    "df_dl_val  = pd.read_parquet(dl_preds_val_path)\n",
    "df_dl_test = pd.read_parquet(dl_preds_test_path)\n",
    "\n",
    "proba_val_dl  = df_dl_val[\"proba\"].to_numpy()\n",
    "proba_test_dl = df_dl_test[\"proba\"].to_numpy()\n",
    "\n",
    "assert len(proba_val_dl)  == len(y_val),  \"[DL] Longitud de preds val != y_val\"\n",
    "assert len(proba_test_dl) == len(y_test), \"[DL] Longitud de preds test != y_test\"\n",
    "\n",
    "\n",
    "proba_val_soft = (proba_val_xgb_tuned + proba_val_lgb_tuned + proba_val_dl) / 3.0\n",
    "thr_val_soft, f1_val_soft = find_best_threshold(y_val, proba_val_soft, metric=\"f1\")\n",
    "metrics_val_soft  = compute_all_metrics(y_val,  proba_val_soft,  thr_val_soft)\n",
    "\n",
    "proba_test_soft = (proba_test_xgb_tuned + proba_test_lgb_tuned + proba_test_dl) / 3.0\n",
    "metrics_test_soft = compute_all_metrics(y_test, proba_test_soft, thr_val_soft)\n",
    "y_pred_test_soft  = (proba_test_soft >= thr_val_soft).astype(int)\n",
    "\n",
    "Z_val  = np.column_stack([proba_val_xgb_tuned,  proba_val_lgb_tuned,  proba_val_dl])\n",
    "Z_test = np.column_stack([proba_test_xgb_tuned, proba_test_lgb_tuned, proba_test_dl])\n",
    "\n",
    "stack_clf = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=1.0,\n",
    "    solver=\"liblinear\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "stack_clf.fit(Z_val, y_val)\n",
    "\n",
    "proba_val_stack = stack_clf.predict_proba(Z_val)[:, 1]\n",
    "thr_val_stack, f1_val_stack = find_best_threshold(y_val, proba_val_stack, metric=\"f1\")\n",
    "metrics_val_stack  = compute_all_metrics(y_val,  proba_val_stack,  thr_val_stack)\n",
    "\n",
    "proba_test_stack = stack_clf.predict_proba(Z_test)[:, 1]\n",
    "metrics_test_stack = compute_all_metrics(y_test, proba_test_stack, thr_val_stack)\n",
    "y_pred_test_stack  = (proba_test_stack >= thr_val_stack).astype(int)\n",
    "\n",
    "\n",
    "plot_pr_curve(y_val,  proba_val_soft,\n",
    "              f\"{EXP_NAME_ENS}_SOFT — PR (val)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_soft,\n",
    "              f\"{EXP_NAME_ENS}_SOFT — PR (test)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_pr_test.png\")\n",
    "\n",
    "plot_pr_curve(y_val,  proba_val_stack,\n",
    "              f\"{EXP_NAME_ENS}_STACK — PR (val)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_pr_val.png\")\n",
    "plot_pr_curve(y_test, proba_test_stack,\n",
    "              f\"{EXP_NAME_ENS}_STACK — PR (test)\",\n",
    "              OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_pr_test.png\")\n",
    "\n",
    "plot_roc_curve(y_val,  proba_val_soft,\n",
    "               f\"{EXP_NAME_ENS}_SOFT — ROC (val)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_soft,\n",
    "               f\"{EXP_NAME_ENS}_SOFT — ROC (test)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_roc_test.png\")\n",
    "\n",
    "plot_roc_curve(y_val,  proba_val_stack,\n",
    "               f\"{EXP_NAME_ENS}_STACK — ROC (val)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_roc_val.png\")\n",
    "plot_roc_curve(y_test, proba_test_stack,\n",
    "               f\"{EXP_NAME_ENS}_STACK — ROC (test)\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_roc_test.png\")\n",
    "\n",
    "plot_confusion(y_test, y_pred_test_soft,\n",
    "               f\"{EXP_NAME_ENS}_SOFT — Confusion (test @thr={thr_val_soft:.3f})\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_SOFT_cm_test.png\")\n",
    "\n",
    "plot_confusion(y_test, y_pred_test_stack,\n",
    "               f\"{EXP_NAME_ENS}_STACK — Confusion (test @thr={thr_val_stack:.3f})\",\n",
    "               OUT_FIGS_E / f\"{EXP_NAME_ENS}_STACK_cm_test.png\")\n",
    "\n",
    "ens_summary = {\n",
    "    \"model\": EXP_NAME_ENS,\n",
    "    \"stack\": {\n",
    "        \"thr_val\": float(thr_val_stack),\n",
    "        \"metrics_val\":  _floatify_metrics(metrics_val_stack),\n",
    "        \"metrics_test\": _floatify_metrics(metrics_test_stack),\n",
    "    },\n",
    "    \"soft_voting\": {\n",
    "        \"thr_val\": float(thr_val_soft),\n",
    "        \"metrics_val\":  _floatify_metrics(metrics_val_soft),\n",
    "        \"metrics_test\": _floatify_metrics(metrics_test_soft),\n",
    "    }\n",
    "}\n",
    "_json_dump(OUT_RESULTS_E / f\"{EXP_NAME_ENS}_metrics.json\", ens_summary)\n",
    "\n",
    "row_stack = {\n",
    "    \"model\": f\"{EXP_NAME_ENS}_STACK\",\n",
    "    \"thr_val\":        thr_val_stack,\n",
    "    \"val_pr_auc\":     metrics_val_stack[\"pr_auc\"],\n",
    "    \"val_roc_auc\":    metrics_val_stack[\"roc_auc\"],\n",
    "    \"val_precision\":  metrics_val_stack[\"precision\"],\n",
    "    \"val_f1\":         metrics_val_stack[\"f1\"],\n",
    "    \"val_recall\":     metrics_val_stack[\"recall\"],\n",
    "    \"val_bal_acc\":    metrics_val_stack[\"bal_acc\"],\n",
    "    \"test_pr_auc\":    metrics_test_stack[\"pr_auc\"],\n",
    "    \"test_roc_auc\":   metrics_test_stack[\"roc_auc\"],\n",
    "    \"test_precision\": metrics_test_stack[\"precision\"],\n",
    "    \"test_f1\":        metrics_test_stack[\"f1\"],\n",
    "    \"test_recall\":    metrics_test_stack[\"recall\"],\n",
    "    \"test_bal_acc\":   metrics_test_stack[\"bal_acc\"],\n",
    "}\n",
    "\n",
    "row_soft = {\n",
    "    \"model\": f\"{EXP_NAME_ENS}_SOFT\",\n",
    "    \"thr_val\":        thr_val_soft,\n",
    "    \"val_pr_auc\":     metrics_val_soft[\"pr_auc\"],\n",
    "    \"val_roc_auc\":    metrics_val_soft[\"roc_auc\"],\n",
    "    \"val_precision\":  metrics_val_soft[\"precision\"],\n",
    "    \"val_f1\":         metrics_val_soft[\"f1\"],\n",
    "    \"val_recall\":     metrics_val_soft[\"recall\"],\n",
    "    \"val_bal_acc\":    metrics_val_soft[\"bal_acc\"],\n",
    "    \"test_pr_auc\":    metrics_test_soft[\"pr_auc\"],\n",
    "    \"test_roc_auc\":   metrics_test_soft[\"roc_auc\"],\n",
    "    \"test_precision\": metrics_test_soft[\"precision\"],\n",
    "    \"test_f1\":        metrics_test_soft[\"f1\"],\n",
    "    \"test_recall\":    metrics_test_soft[\"recall\"],\n",
    "    \"test_bal_acc\":   metrics_test_soft[\"bal_acc\"],\n",
    "}\n",
    "\n",
    "csv_e = OUT_RESULTS_E / \"ensembles.csv\"\n",
    "pd.DataFrame([row_stack, row_soft]).to_csv(\n",
    "    csv_e,\n",
    "    mode=(\"a\" if csv_e.exists() else \"w\"),\n",
    "    index=False,\n",
    "    header=not csv_e.exists()\n",
    ")\n",
    "\n",
    "stack_params = {\n",
    "    \"type\": \"LogisticRegression\",\n",
    "    \"penalty\": stack_clf.penalty,\n",
    "    \"C\": float(stack_clf.C),\n",
    "    \"solver\": stack_clf.solver,\n",
    "    \"feature_names_level1\": [\"xgb_tuned_proba\", \"lgbm_tuned_proba\", \"dl_proba\"],\n",
    "    \"coef_\": stack_clf.coef_[0].tolist(),\n",
    "    \"intercept_\": float(stack_clf.intercept_[0]),\n",
    "}\n",
    "_json_dump(OUT_PARAMS_E / f\"BEST_{EXP_NAME_ENS}.json\", stack_params)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"p_xgb\":   proba_val_xgb_tuned,\n",
    "    \"p_lgbm\":  proba_val_lgb_tuned,\n",
    "    \"p_dl\":    proba_val_dl,\n",
    "    \"p_soft\":  proba_val_soft,\n",
    "    \"p_stack\": proba_val_stack,\n",
    "    \"y_true\":  y_val,\n",
    "}).to_parquet(OUT_PREDS_E / f\"preds_val_{EXP_NAME_ENS}.parquet\", index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"p_xgb\":   proba_test_xgb_tuned,\n",
    "    \"p_lgbm\":  proba_test_lgb_tuned,\n",
    "    \"p_dl\":    proba_test_dl,\n",
    "    \"p_soft\":  proba_test_soft,\n",
    "    \"p_stack\": proba_test_stack,\n",
    "    \"y_true\":  y_test,\n",
    "}).to_parquet(OUT_PREDS_E / f\"preds_test_{EXP_NAME_ENS}.parquet\", index=False)\n",
    "\n",
    "print(\"\\n=== SOFT VOTING (val) ===\")\n",
    "print(metrics_val_soft)\n",
    "print(\"\\n=== SOFT VOTING (test) ===\")\n",
    "print(metrics_test_soft)\n",
    "\n",
    "print(\"\\n=== STACKING (val) ===\")\n",
    "print(metrics_val_stack)\n",
    "print(\"\\n=== STACKING (test) ===\")\n",
    "print(metrics_test_stack)\n",
    "\n",
    "print(\"\\n[OK][ENSEMBLE] Métricas, preds y pesos del modelo híbrido guardados en:\")\n",
    "print(\"  RESULTS:\", OUT_RESULTS_E)\n",
    "print(\"  PREDS:  \", OUT_PREDS_E)\n",
    "print(\"  PARAMS: \", OUT_PARAMS_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f7b15",
   "metadata": {},
   "source": [
    "10 - Stacking con OOF (CV=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ead0c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[BAL] SMOTE aplicado | k_neighbors=5 | cat_cols=0\n",
      "[WARN][_find_one] múltiples matches para patrón 'preds/oof_DL_*_CV5.parquet':\n",
      "   - /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/DL_REDUCED_SMOTENC/preds/oof_DL_REDUCED_SMOTENC_TUNED_CV5.parquet\n",
      "   - /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/DL_REDUCED_SMOTENC/preds/oof_DL_REDUCED_SMOTENC_BASELINE_CV5.parquet\n",
      "   - /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/DL_FULL_SMOTENC/preds/oof_DL_FULL_SMOTENC_BASELINE_CV5.parquet\n",
      "   - /Users/luistejada/Downloads/TFE Churn Bancario/artifacts/DL_FULL_SMOTENC/preds/oof_DL_FULL_SMOTENC_TUNED_CV5.parquet\n",
      "[DL OOF] Columnas encontradas en el parquet: Index(['oof_proba', 'y_true'], dtype='object')\n",
      "[DL OOF] Usando columna 'oof_proba' como probabilidad.\n",
      "[STACK OOF] Usando SOLO TRAIN para meta-learner (len = 6000 ).\n",
      "[STACK OOF] Mejor C para meta-learner: 0.1000 | AP OOF: 0.70258\n",
      "[STACK OOF] OK — resultados de VAL y TEST guardados.\n"
     ]
    }
   ],
   "source": [
    "DO_FULL_STACKING_OOF = True \n",
    "\n",
    "def _find_one(pattern, base=None):\n",
    "    \n",
    "    base = Path(base or ARTIF_ROOT)\n",
    "    matches = list(base.rglob(pattern))\n",
    "    if not matches:\n",
    "        return None\n",
    "    if len(matches) > 1:\n",
    "        print(f\"[WARN][_find_one] múltiples matches para patrón '{pattern}':\")\n",
    "        for m in matches:\n",
    "            print(\"   -\", m)\n",
    "    return matches[0]\n",
    "\n",
    "if DO_FULL_STACKING_OOF:\n",
    "    skf = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    def oof_preds_tree(model_builder, X, y, name):\n",
    "        oof = np.zeros(len(y), dtype=float)\n",
    "        models = []\n",
    "        for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "            X_tr, y_tr = X[tr], y[tr]\n",
    "            X_va, y_va = X[va], y[va]\n",
    "            if USE_BALANCED_TRAIN:\n",
    "                X_tr, y_tr = maybe_smote(X_tr, y_tr, keep_idx=keep_idx_global)\n",
    "            mdl = model_builder()\n",
    "            if name == \"xgb\":\n",
    "                mdl = xgb_fit_with_es(\n",
    "                    mdl,\n",
    "                    X_tr, y_tr,\n",
    "                    X_va, y_va,\n",
    "                    feature_names=feature_names_used,\n",
    "                    rounds=200,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                mdl = lgbm_fit_with_es(\n",
    "                    mdl,\n",
    "                    X_tr, y_tr,\n",
    "                    X_va, y_va,\n",
    "                    feature_names=feature_names_used,\n",
    "                    rounds=200,\n",
    "                    verbose=False\n",
    "                )\n",
    "            oof[va] = mdl.predict_proba(X_va)[:, 1]\n",
    "            models.append(mdl)\n",
    "        return oof, models\n",
    "\n",
    "    # --- Builders de los modelos base ---\n",
    "    xgb_hp = xgb_best_params if ('xgb_best_params' in locals() and xgb_best_params is not None) else xgb_params_seed\n",
    "    def build_xgb():\n",
    "        return XGBClassifier(\n",
    "            **{\n",
    "                **xgb_hp,\n",
    "                \"n_jobs\": -1,\n",
    "                \"eval_metric\": \"aucpr\",\n",
    "                \"tree_method\": \"hist\",\n",
    "                \"verbosity\": 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    lgbm_hp = lgbm_best_params if ('lgbm_best_params' in locals() and lgbm_best_params is not None) else lgbm_params_seed\n",
    "    def build_lgb():\n",
    "        return LGBMClassifier(\n",
    "            **{\n",
    "                **lgbm_hp,\n",
    "                \"metric\": \"average_precision\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # --- OOF XGB / LGBM sobre TRAIN+VAL (L1) ---\n",
    "    X_all = np.vstack([X_train_fit, X_val_fit])\n",
    "    y_all = np.concatenate([y_train, y_val])\n",
    "\n",
    "    oof_xgb, xgb_models = oof_preds_tree(build_xgb, X_all, y_all, \"xgb\")\n",
    "    oof_lgb, lgb_models = oof_preds_tree(build_lgb, X_all, y_all, \"lgbm\")\n",
    "\n",
    "    # --- OOF DL ---\n",
    "    dl_oof_fp = _find_one(\"preds/oof_DL_*_CV5.parquet\") or _find_one(\"oof_DL_*_CV5.parquet\")\n",
    "    if dl_oof_fp is None:\n",
    "        raise FileNotFoundError(\"[DL] No se encontró oof del DL (oof_DL_*_CV5.parquet).\")\n",
    "\n",
    "    df_oof_dl = pd.read_parquet(dl_oof_fp)\n",
    "    print(\"[DL OOF] Columnas encontradas en el parquet:\", df_oof_dl.columns)\n",
    "\n",
    "    # Elegimos la columna de probas ignorando columnas típicas de label\n",
    "    candidate_cols = [c for c in df_oof_dl.columns if c.lower() not in (\"y_true\", \"y\", \"target\", \"label\")]\n",
    "    if len(candidate_cols) != 1:\n",
    "        raise ValueError(f\"[DL] No puedo identificar de forma única la columna de probas. Columnas: {df_oof_dl.columns}\")\n",
    "    proba_col_dl = candidate_cols[0]\n",
    "    print(f\"[DL OOF] Usando columna '{proba_col_dl}' como probabilidad.\")\n",
    "\n",
    "    oof_dl = df_oof_dl[proba_col_dl].to_numpy()\n",
    "\n",
    "    # --- Alinear longitudes ---\n",
    "    n_dl = len(oof_dl)\n",
    "    n_all = len(y_all)\n",
    "    n_train = len(y_train)\n",
    "\n",
    "    if n_dl == n_all:\n",
    "        oof_xgb_meta = oof_xgb\n",
    "        oof_lgb_meta = oof_lgb\n",
    "        y_meta = y_all\n",
    "        print(\"[STACK OOF] Usando TRAIN+VAL para meta-learner (len =\", n_dl, \").\")\n",
    "    elif n_dl == n_train:\n",
    "        oof_xgb_meta = oof_xgb[:n_dl]\n",
    "        oof_lgb_meta = oof_lgb[:n_dl]\n",
    "        y_meta = y_train\n",
    "        print(\"[STACK OOF] Usando SOLO TRAIN para meta-learner (len =\", n_dl, \").\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"[DL] Longitud OOF DL {n_dl} no coincide ni con TRAIN ({n_train}) ni con TRAIN+VAL ({n_all}).\"\n",
    "        )\n",
    "\n",
    "    X_meta_oof = np.column_stack([oof_xgb_meta, oof_lgb_meta, oof_dl])\n",
    "    meta_oof = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        C=1.0,\n",
    "        solver=\"liblinear\"\n",
    "    )\n",
    "\n",
    "    best_c, best_ap = None, -1.0\n",
    "    for c in [0.01, 0.1, 1.0, 3.0, 10.0]:\n",
    "        tmp = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            class_weight=\"balanced\",\n",
    "            C=c,\n",
    "            solver=\"liblinear\"\n",
    "        )\n",
    "        tmp.fit(X_meta_oof, y_meta)\n",
    "        ap = average_precision_score(y_meta, tmp.predict_proba(X_meta_oof)[:, 1])\n",
    "        if ap > best_ap:\n",
    "            best_ap, best_c = ap, c\n",
    "\n",
    "    meta_oof.set_params(C=best_c)\n",
    "    meta_oof.fit(X_meta_oof, y_meta)\n",
    "    print(f\"[STACK OOF] Mejor C para meta-learner: {best_c:.4f} | AP OOF: {best_ap:.5f}\")\n",
    "\n",
    "    def fit_full_and_pred(models_builder, name):\n",
    "        mdl = models_builder()\n",
    "        if name == \"xgb\":\n",
    "            mdl = xgb_fit_with_es(\n",
    "                mdl,\n",
    "                X_all, y_all,\n",
    "                X_val_fit, y_val,\n",
    "                feature_names=feature_names_used,\n",
    "                rounds=200,\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            mdl = lgbm_fit_with_es(\n",
    "                mdl,\n",
    "                X_all, y_all,\n",
    "                X_val_fit, y_val,\n",
    "                feature_names=feature_names_used,\n",
    "                rounds=200,\n",
    "                verbose=False\n",
    "            )\n",
    "        return mdl.predict_proba(X_test_fit)[:, 1]\n",
    "\n",
    "    ptest_xgb_full = fit_full_and_pred(build_xgb, \"xgb\")\n",
    "    ptest_lgb_full = fit_full_and_pred(build_lgb, \"lgbm\")\n",
    "\n",
    "    # --- DL TEST ---\n",
    "    if proba_test_dl is None:\n",
    "        raise FileNotFoundError(\"[DL] Falta preds TEST del DL para completar stacking canónico.\")\n",
    "\n",
    "    X_meta_test = np.column_stack([ptest_xgb_full, ptest_lgb_full, proba_test_dl])\n",
    "\n",
    "    X_meta_val = np.column_stack([proba_val_xgb_tuned, proba_val_lgb_tuned, proba_val_dl])\n",
    "    p_val_stack_oof = meta_oof.predict_proba(X_meta_val)[:, 1]\n",
    "\n",
    "    thr_stack_oof, _ = find_best_threshold(y_val, p_val_stack_oof, metric=\"f1\")\n",
    "    metrics_val_stack_oof = compute_all_metrics(y_val, p_val_stack_oof, thr_stack_oof)\n",
    "\n",
    "    # --- PREDICCIONES STACK EN TEST ---\n",
    "    p_test_stack_oof = meta_oof.predict_proba(X_meta_test)[:, 1]\n",
    "    metrics_test_stack_oof = compute_all_metrics(y_test, p_test_stack_oof, thr_stack_oof)\n",
    "\n",
    "    # --- GUARDAR PREDS VAL Y TEST ---\n",
    "    pd.DataFrame({\"proba\": p_val_stack_oof, \"y_true\": y_val}).to_parquet(\n",
    "        OUT_PREDS_E / \"preds_val_ENS_STACK_OOF.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    pd.DataFrame({\"proba\": p_test_stack_oof, \"y_true\": y_test}).to_parquet(\n",
    "        OUT_PREDS_E / \"preds_test_ENS_STACK_OOF.parquet\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    with open(OUT_RESULTS_E / \"ensemble_stack_oof_summary.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"thr\": float(thr_stack_oof),\n",
    "                \"val\": {k: float(v) for k, v in metrics_val_stack_oof.items()},\n",
    "                \"test\": {k: float(v) for k, v in metrics_test_stack_oof.items()},\n",
    "            },\n",
    "            f,\n",
    "            indent=2,\n",
    "        )\n",
    "\n",
    "    print(\"[STACK OOF] OK — resultados de VAL y TEST guardados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
